{
  "hash": "7cd9ee67b7d0bd6a22cdaf03576ecd2d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Wrangling Data with dplyr\ndescription: 'Making data tidying and transformation fun!\n\n  '\ncreative_commons: CC BY\n---\n\n\n\n\n\n\n:::obj\n\n**Learning objectives**\n \n - Learn how to approach tidying and transforming datasets\n - Understand how to use the pipe (`%>%`) to chain operations together\n - Learn the core **`dplyr`** tools including functions (`select`, `filter`, `group_by`, `summarize`)\n\n:::\n\n## Data wrangling with **`dplyr`**\n\nData wrangling is usually the part of any data analysis project that takes the most time. Although it may not necessarily be fun, it is foundational to all the work that follows. Often, data wrangling also takes significantly longer than actually performing the data analysis or creating a data visualization, so do not panic if, in the future, you find yourself spending a lot of time on this phase. Once you learn some core approaches and tools, you can deal with nearly any dataset you may face!\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![*Illustration by @allison_horst, from Hadley Wickham's talk \"The Joy of Functional Programming (for Data Science)\"*](images/data_cowboy.png){width=100%}\n:::\n:::\n\n\n\n\nThe data wrangling process includes data import, tidying, and transformation.  The process directly feeds into the *understanding* or modeling side of data exploration, but in a iterative way.  More generally, data wrangling is the manipulation or combination of datasets for the purpose of analysis, and often you have to rinse and repeat this process as your understanding of the data changes, and as your modeling and visualization needs also changes.  \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![*Image from \"R for Data Science\" by Garrett Grolemund and Hadley Wickham.*](images/data-science-wrangle.png){width=100%}\n:::\n:::\n\n\n\n\n### Have an Objective\n\n > **All data wrangling is based on a purpose.**  \n \nNo one wrangles for the sake of wrangling (usually), so the process always begins by answering the following two questions:\n\n 1. What do my **input** data look like?\n 2. What *should* my **output** data look like given what I want to do?\n\n### Learn some Core Tools for Common Operations\n\nAt the most basic level, going from what your data looks like to what you want it look like will require a few key operations. The more we practice these operations with common tools, the easier this will be no matter the size or complexity of data that we face. \n\n<aside> *Tidy data* is an important part of being able to re-use these tools! See this great [blog](https://www.openscapes.org/blog/2020/10/12/tidy-data/) (by Lowndes & Horst) \n</aside>\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst.](images/tidydata_4.jpg){width=80%}\n:::\n:::\n\n\n\n\nCommon operations that occur during data wrangling:\n\n* **`select`** specific variables\n* **`filter`** observations by some criteria\n* Add or modify (**`mutate`**) existing variables \n* **`rename`** variables\n* **`arrange`** rows by a variable\n* **`summarize`** a variable conditional on others\n\nThe **`dplyr`** package provides easy tools for these common data manipulation tasks and is a core package from the [`tidyverse`](https://www.tidyverse.org/) suite of packages. The philosophy of **`dplyr`** is that **one** function does **one** thing and the name of the function says what it does. \n\n### Start from Scratch\n\nAny reproducible analysis should be easily repeated. Using code in a script allows us to rebuild every step of the data analysis from scratch -- from data import, to data wrangling, to visualization and modeling -- sometimes over and over again. For that reason, it's a good habit to **Restart your R Session** before you begin a new coding adventure. Sometimes this solves issues, sometimes it's just good to make sure everything runs up to the point you are working. Let's do that now!\n\n 1. Go to **`Session`** > **`Restart R`**!\n 2. Check your `Environment` tab...it should be empty!\n\n### Import Libraries and Data \n\nFirst we need to load the libraries that we'll be using. Let's the `tidyverse` packages (which includes `dplyr` and `ggplot2`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\nNext we import a `.csv` of groundwater monitoring stations across California^[see [here for more about this data](https://data.cnra.ca.gov/dataset/periodic-groundwater-level-measurements)]. \n\n<aside> We can use either `read.csv` or `read_csv` functions. What's the main difference between the two? Hint, check `class(stations)` for each method.\n</aside>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read the stations.csv file and assign it to an object \"stations\"\nstations <- read.csv(\"data/gwl/stations.csv\")\n```\n:::\n\n\n\n\nBelow your code that imports the dataset, let's make a new section! Go to **`Code`** > **`Insert Section`**, and type your new section header `Wrangling Data`. We can also use keyboard shortcuts for this (`Ctrl` or `⌘` + `Shift` + `R`). You should notice how this now appears in an expandable table of contents on the right hand side of your script pane (look for the tiny button that has little gray horizontal lines on it). This feature can be very helpful in keeping you and your scripts organized.\n\n## `filter` rows\n\nOne of the most common steps to slicing data is to **filter rows** of a dataframe. We can filter by a variable (e.g., column name) or by a condition (i.e., only values greater than 100). Remember, `dplyr` is designed so that *one* function does *one* thing, and the name of the function says what it does. \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Artwork by @allison_horst](images/filter.jpg){width=80%}\n:::\n:::\n\n\n\n\n\nLet's filter this large dataframe of potential groundwater station locations (n=43807 total!) to just stations from Sacramento County with `filter()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_sac <- filter(stations, COUNTY_NAME == \"Sacramento\")\nnrow(stations_sac)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 494\n```\n\n\n:::\n:::\n\n\n\n\nWhat happened? `filter()` requires the data, and then any number of filtering options. We provided just one. Here the `==` means \"equal to\", while a single `=` would mean `is`. \n\nWe can also combine filter conditions in the same call! Let's add another condition that we only want `Residential` wells (a category of the `WELL_USE` column). We can get a count of how many categories of `WELL_USE` have observations (or rows) in the `stations_sac` dataframe using the `table()` function. Then we can combine multiple conditions in our `filter` with a `,` (in `filter`, commas are another way to say `AND`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get what the categories of WELL_USE are in Sacramento County:\ntable(stations_sac$WELL_USE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                 Industrial    Irrigation   Observation         Other \n            8             2           144            90           101 \n  Residential Stockwatering       Unknown \n           76             6            67 \n```\n\n\n:::\n\n```{.r .cell-code}\n# combine conditions with \",\" which is equivalent to AND\nstations_sac <- filter(stations, COUNTY_NAME == \"Sacramento\", WELL_USE == \"Residential\")\nnrow(stations_sac)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 76\n```\n\n\n:::\n:::\n\n\n\n\nWe can mix and match `filter` conditions however we choose. What if we want to select multiple counties? The `%in%` is a great way to provide a list of things you want `filter` to use to see if there's a match within your dataframe. Let's filter to 3 counties: Sacramento, Placer, and El Dorado.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter to multiple counties\nsep_counties <- c(\"Sacramento\", \"Placer\", \"El Dorado\")\nstations_multcounties <- filter(stations, COUNTY_NAME %in% sep_counties)\n```\n:::\n\n\n\n\n<aside> To quote or not to quote? Inside tidyverse functions like `filter` we typically **don't** need to quote variable names (columns) but we **do** need to quote specific values.\n</aside>\n\nWhat happened? We used a list (using the `c()` to combine different possible values of `COUNTY_NAME`) to specify what data we wanted to keep, and we told `filter` to return all rows in the `COUNTY_NAME` column with entries in (`%in%`) that list of county names (`sep_counties`). \n\n\n### Exclude rows\n\nWe can also *exclude* rows based on conditions. To exclude, we can negate a condition with the `!` symbol in front of the condition. Let's exclude all stations from Yolo County. Here we show 3 ways to to do the same thing.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# include everything except Yolo County...3 different ways\nstations_trim <- filter(stations, !COUNTY_NAME %in% c(\"Yolo\"))\nstations_trim <- filter(stations, !COUNTY_NAME == \"Yolo\")\nstations_trim <- filter(stations, COUNTY_NAME != \"Yolo\")\n```\n:::\n\n\n\n\n## `select` columns\n\nWe can also select specific columns or variables of interest with the `select()` function. Similar to `filter`, we pass the data, and the conditions we want to use to subset our data. Let's select only `STN_ID`, `LATITUDE`, `LONGITUDE`, and `COUNTY_NAME`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select specific columns to keep\nstations_sel1 <- select(stations, c(STN_ID, LATITUDE, LONGITUDE, COUNTY_NAME))\nnames(stations_sel1) # names of columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"STN_ID\"      \"LATITUDE\"    \"LONGITUDE\"   \"COUNTY_NAME\"\n```\n\n\n:::\n:::\n\n\n\n\nRemember we don't have to quote the variable names here! What if we want to just remove a column? We can use the same structure, but put a \"**`-`**\" in front of our list or condition.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select columns to drop\nstations_sel2 <- select(stations, -c(LATITUDE:BASIN_NAME, COUNTY_NAME))\nncol(stations_sel2) # how many columns?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n:::\n\n\n\n\nHere we used the `:` to say columns from `LATITUDE` *through* `BASIN_NAME`, then we included `COUNTY_NAME` in the list of columns to drop.\n\nThere are some very powerful options that exist within `select()`, including the ability to rename columns, or select by things that relate to the column names. Let's select columns that start with \"W\" or contain \"NAME\".\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select columns to drop\nstations_sel3 <- select(stations, starts_with(\"W\"), contains(\"NAME\"))\nnames(stations_sel3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"WELL_NAME\"   \"WLM_METHOD\"  \"WLM_ACC\"     \"WELL_DEPTH\"  \"WELL_USE\"   \n[6] \"WELL_TYPE\"   \"WCR_NO\"      \"BASIN_NAME\"  \"COUNTY_NAME\"\n```\n\n\n:::\n:::\n\n\n\n\n## The Pipe ( `%>%` ) \n\nWe now have two ways to slice and dice our data, filter rows and select by columns. A very useful tool you may have seen is something called a \"pipe\". In R, this is represented by the symbol **`%>%`** and can be inserted with the keyboard shortcut `Ctrl` or `⌘` + `Shift` + `M`.\n\nThe pipe is a way to chain multiple commands together, and it passes the result of the first function to the second function This is particularly helpful in R when you don't need to (or want to) assign temporary steps to your code. For example, compare a filter, select, and rename set of operations with and without the `%>%`. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter\nstations_multcounty1 <- filter(stations, COUNTY_NAME %in% \n                                  c(\"Sacramento\", \"Placer\"))\n\n# select\nstations_multcounty2 <- select(stations_multcounty1, starts_with(\"W\"), \n                               contains(\"NAME\"), contains(\"ID\"))\n\n# rename the STN_ID to station_id: rename(new_col_name, old_col_name)\nstations_multcounty3 <- rename(stations_multcounty2, station_id = STN_ID)\n```\n:::\n\n\n\n\nNow let's rewrite the code above with `%>%`. Notice we don't need to explicitly define intermediate variables for each step, because data is \"piped\" between steps.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter, select, and rename\nstations_multcounty <- stations %>%\n  filter(COUNTY_NAME %in% c(\"Sacramento\", \"Placer\")) %>% \n  select(starts_with(\"W\"), contains(\"NAME\"), contains(\"ID\")) %>%\n  rename(station_id = STN_ID)\n```\n:::\n\n\n\n\nWe piped (`%>%`) the data to `filter()`, and the result of the `filter()` to `select()`, and then piped the result of that to `rename()`. Since we assigned all of this at the very beginning to `stations_multcounty`, that's our final output. \n\n\n:::challenge\n\n<font color=\"#009E73\">**Challenge 1: You Try!**</font> \n\n1. Using the `stations` dataset, filter to only **`Residential`** wells that have a `WELL_DEPTH > 1000` feet.  \n2. Select only `STN_ID`, `WELL_DEPTH`, `WELL_NAME`, `BASIN_NAME`, and `COUNTY_NAME` columns in the dataframe. \n3. How many records are in Los Angeles?   \n\n:::\n\n<br>\n\n<details>\n  <summary class=\"challenge-ans-title\"><font color=\"#0072B2\">**Click for Answers!**</font></summary>\n  <div class=\"challenge-ans-body\">\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter and select\nchallenge1 <- stations %>% \n  filter(WELL_USE == \"Residential\", WELL_DEPTH > 1000) %>% \n  select(STN_ID, WELL_DEPTH, ends_with(\"NAME\"))\nnames(challenge1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"STN_ID\"      \"WELL_DEPTH\"  \"WELL_NAME\"   \"BASIN_NAME\"  \"COUNTY_NAME\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# count the number of rows for each unique value of COUNTY_NAME\ntable(challenge1$COUNTY_NAME)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   Los Angeles      Riverside San Bernardino       Siskiyou         Tulare \n             5              3             12              2              1 \n       Ventura \n             3 \n```\n\n\n:::\n:::\n\n\n\n\n  </div>\n</details>\n\n## `mutate` existing data\n\n`mutate` is a function that modifies existing data, either by adding a new column, or modifying existing columns. When using `mutate` we expect R to return some modified version of our input dataframe... but it should still be a version of the original dataframe.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Artwork by @allison_horst](images/mutate.png){width=70%}\n:::\n:::\n\n\n\n\n\n`mutate` allows us to pass other functions or data into our dataframe, and is a powerful way to clean and tidy our data.\n\nLet's create a new `WELL_DEPTH_m` column which converts the existing `WELL_DEPTH` column from feet to meters. We pass the new column name and then the function or operation we want to preform to create the new column (or modify an existing column).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_mutate1 <- stations %>% \n  mutate(WELL_DEPTH_m = WELL_DEPTH * 0.3048)\n```\n:::\n\n\n\n\nLet's use `ggplot2` to check and see if this worked. We can use the `%>%` here too!\n\n\n\n\n::: {.cell preview='true'}\n\n```{.r .cell-code}\nstations_mutate1 %>% \n  # pass stations_mutate1 into the ggplot function\n  ggplot() + \n  # add a point geom in feet\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH), color = \"cyan4\", alpha = 0.5) +\n  # add a point geom in meters\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH_m), color = \"maroon\", pch = 21, alpha = 0.8)\n```\n\n::: {.cell-output-display}\n![](m_dplyr_files/figure-html/mut1ggplot1-1.png){width=672}\n:::\n:::\n\n\n\n\nAs shown above, the reddish circles for observations in units of meters indicate smaller values than the blueish circles for observations in units of feet. This looks about right! It is rarely a bad idea to \"visualize\" your outcomes whenever possible, these are great ways to double check things are happening as you intend when data wrangling. \n\n<details>\n  <summary class=\"extra-practice-title\">Extra Practice</summary>\n  <div class=\"extra-practice-body\">\nLet's pause a moment and do this in base R *without* the tidyverse so we can see both methods. There are many positives about knowing different approaches, and also when troubleshooting, it can be helpful to understand different ways of doing the same thing. Can you spot a few differences?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter and select (subset)\nstations_basefilter <- stations[stations$WELL_USE == \"Residential\" & \n                                  stations$WELL_DEPTH > 1000,\n                                c(1, 4, grep(\"NAME\", colnames(stations)))] \n\n# mutate\nstations_base1 <- stations # make a copy so we don't alter original data\nstations_base1$WELL_DEPTH_m <- stations_base1$WELL_DEPTH * 0.3048\n\n# ggplot\nggplot(data = stations_base1) + \n  geom_point(aes(x = STN_ID, y = WELL_DEPTH), \n             color = \"cyan4\", alpha = 0.5) +\n  # this in meters\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH_m), \n             color = \"maroon\", pch = 21, alpha = 0.8)\n```\n:::\n\n\n\n\n  </div>\n</details>\n\n## `group_by` & `summarize`\n\nThe final core `dplyr` \"verbs\" to be aware of are `group_by` and `summarize`. These are really useful to transform your data into an entirely **new** dataset, and to do many operations at once based on different columns (variables) within your dataset.\n\nFor example let's say we want a count of how many records exist for each county. We can use **`group_by`** and **`count`**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_by_county <- stations %>% \n  group_by(COUNTY_NAME) %>% \n  count()\n\nhead(n_by_county)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"COUNTY_NAME\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Alameda\",\"2\":\"237\"},{\"1\":\"Alpine\",\"2\":\"33\"},{\"1\":\"Amador\",\"2\":\"11\"},{\"1\":\"Butte\",\"2\":\"302\"},{\"1\":\"Calaveras\",\"2\":\"17\"},{\"1\":\"Colusa\",\"2\":\"229\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\nWhat happened? `group_by()` groups the data based on the variable(s) included in the function. Once our `data.frame` is grouped, there are many options and functions we can apply, including things like `count()`, `tally()`, and `summarize()`.\n\nLet's look at this a bit more. What if we really only want the 5 counties with the most observations? We can add two additional options here. The `arrange()` function will sort our data by a variable(s) of our choice, and we can use `desc()` to specify we want that data in descending order. We then pipe that to `head()` and ask for just the first 5 rows.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations %>% \n  group_by(COUNTY_NAME) %>% \n  count() %>% # count by COUNTY_NAME\n  arrange(desc(n)) %>% # sort by column n in descending order\n  head(5) # return the first 5 rows\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"COUNTY_NAME\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Fresno\",\"2\":\"6053\"},{\"1\":\"San Bernardino\",\"2\":\"5837\"},{\"1\":\"Kern\",\"2\":\"5155\"},{\"1\":\"Riverside\",\"2\":\"2636\"},{\"1\":\"Merced\",\"2\":\"2479\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n\n### Summarizing Data\n\nSimilar to `mutate`, `summarize` allows us to create new columns. The difference is `summarize` creates a completely new dataframe based on the `group_by`. Thus, `summarize` always follows a `group_by`. Here we can do a similar analysis to what we did above, but now we can customize our columns and what we want to calculate. Let's calculate the average depth of wells in all counties, then plot the top 10 counties with the deepest average well depth. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations %>% \n  group_by(COUNTY_NAME) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH, na.rm = TRUE)) %>% \n  arrange(desc(mean_well_depth)) %>% \n  head(10) %>% # return the first 5 rows and pass to ggplot\n  ggplot() + \n  geom_col(aes(x = COUNTY_NAME, y = mean_well_depth)) +\n  labs(title = \"Mean well depth\", \n       subtitle = \"Top 10 Counties with deepest wells\")\n```\n\n::: {.cell-output-display}\n![](m_dplyr_files/figure-html/grpby3-1.png){width=672}\n:::\n:::\n\n\n\n\n<aside> This is a plot to consider rotating the axis labels. How do we do that? Practice your web-search skills or visit the [ggplot2 help page](https://ggplot2.tidyverse.org/index.qmd). *Hint: look at `theme(axis.text.x = )`. Another option is `coord_flip()`.*\n</aside>\n\nWe looked at only the top 10 counties before, but remember that `group_by` and `summarize` work on entire dataframes. To illustrate how these operations scale to entire dataframes, if we wanted to visualize the mean well depth for all counties in the data, we could do so with slightly modified code:  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations %>% \n  # group by the county name\n  group_by(COUNTY_NAME) %>% \n  # calculate the mean well depth\n  summarize(mean_well_depth = mean(WELL_DEPTH, na.rm = TRUE)) %>% \n  # remove two counties that have NA values\n  filter(!is.na(mean_well_depth)) %>% \n  # pass to ggplot\n  ggplot() + \n  geom_col(aes(x = fct_reorder(COUNTY_NAME, mean_well_depth), \n               y = mean_well_depth)) +\n  labs(title = \"Mean well depth\", \n       subtitle = \"Periodic groundwater level database\", \n       y = \"Mean well depth (ft)\",\n       x = \"\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](m_dplyr_files/figure-html/well-depth-all-counties-1.png){width=672}\n:::\n:::\n\n\n\n\n\nWe can add additional groups or columns depending on what we are interested in. Let's try the following:\n\n - find the mean well depth by each `WELL_USE` category using **`summarize`**\n - **`filter`** to only groups that have more than 10 stations and no NA's (*for NA's, we can use the `is.na()` function, and the negate or `!` symbol we used previously*)\n - pipe to ggplot and make a boxplot to visualize the result\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth)) %>% \n  ggplot() + \n  geom_boxplot(aes(x = WELL_USE, y = mean_well_depth), \n               fill = \"seagreen\", alpha = 0.5) +\n  labs(title = \"Well Depth for Groundwater Stations in CA\",\n       subtitle = \"For groups with >10 stations\",\n       x = \"Well Use\", \n       y = \"Well Depth (ft)\")\n```\n\n::: {.cell-output-display}\n![](m_dplyr_files/figure-html/grpby4-1.png){width=672}\n:::\n:::\n\n\n\n\n What happened? We grouped by two different variables, and then applied functions (or transformed the data) to those groups using `summarize`, then we filtered, and visualized. The order of these steps can influence your analysis and result, so it's good to consider when you want to filter and when you want to group_by/summarize things. Let's try a challenge!\n \n:::challenge\n\n<font color=\"#009E73\">**Challenge 2: You Try!**</font> \n\n1. Using the code we ran above, figure out how to make this boxplot without the \"`Unknown`\" and blank (`\"\"`) `WELL_USE` categories. \n2. Try adding points as another layer over your boxplot using `geom_jitter`.\n3. For a bonus challenge, try adding a caption with information about this dataset, and plotting these data in meters instead of feet.\n\n:::\n\n<br>\n\n<details>\n  <summary class=\"challenge-ans-title\"><font color=\"#0072B2\">**Click for Answers!**</font></summary>\n  <div class=\"challenge-ans-body\">\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations %>% \n  filter(WELL_USE != \"\", WELL_USE != \"Unknown\") %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            mean_well_depth_m = mean_well_depth * 0.3048,\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth_m)) %>%\n  ggplot() + \n  geom_boxplot(aes(x = WELL_USE, y = mean_well_depth_m), \n               fill = \"seagreen\", alpha = 0.5) +\n  # add jittered points\n  geom_jitter(aes(x = WELL_USE, y = mean_well_depth_m), \n              color = \"forestgreen\", alpha = 0.5) +\n  # add labels and caption\n  labs(title = \"Well Depth for Groundwater Stations in CA\",\n       subtitle = \"For groups with >10 stations\",\n       caption  = \"Data from the DWR Periodic Groundwater Level Database.\",\n       x = \"Well Use\", \n       y = \"Well Depth (ft)\")\n```\n\n::: {.cell-output-display}\n![](m_dplyr_files/figure-html/challenge-2-1.png){width=672}\n:::\n:::\n\n\n\n\n  </div>\n</details>\n\n### Remember to `ungroup`\n\nA common blunder that can impact downstream analyses, or even the current analysis you may want to do is forgetting to ungroup. `dplyr` is smart, and will do exactly as you program it to do, but when you use `group_by`, your output dataframe **will retain those groups** for future use. This can be helpful in some cases, but it's important to keep an eye on and be aware of (for when it's *not* helpful). We can always use `class()` to better check this. Let's try some code from above and check with `class()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_by_cnty_use <- stations %>% \n  filter(WELL_USE != \"\", WELL_USE != \"Unknown\") %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth)) \n\nclass(stations_by_cnty_use)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n:::\n\n\n\n\nHere we can see the `grouped_df` title, which tells us this is a **grouped** dataset. We can easily *ungroup* with the concisely named `ungroup()` function. This isn't required, but it's a good habit to get into (or be aware of).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations_by_cnty_use <- ungroup(stations_by_cnty_use)\nclass(stations_by_cnty_use)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n:::\n\n\n\n\nCongratulations! You've made it through `dplyr`. There are many more great options and tricks within `dplyr`, but hopefully this will get you going.\n\n<br>  \n\n\n",
    "supporting": [
      "m_dplyr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "articles": [
    {
      "path": "index.html",
      "title": "R4WRDS Introductory Course",
      "description": "Unleashing the power of reproducible workflows with \"R for Water Resources Data Science\" (R 4 WRDS).\n",
      "author": [],
      "contents": "\n\nWho is this course for?\nThis course is most relevant and targeted at folks who work with data, from analysts and program staff to engineers and scientists. This course provides an introduction to the power and possibility of a reproducible programming language (R) by demonstrating how to import, explore, visualize, analyze, and communicate different types of data. Using water resources based examples, this course guides participants through basic data science skills and strategies for continued learning and use of R.\n\nWhy R?\nR is a language for statistical computing and a general purpose programming language. It is one of the primary languages used for data science, modeling, and visualization.\nThis workshop will provide attendees with a starting point for continued learning and use of R. We will cover a variety of commonly used file types (i.e., .csv, .xlsx, .shp) used in analysis, and provide resources for additional learning.\n\nWhat will you learn?\nIn this course, we start from first principles and assume no prior experience with R. Although each module in this course can serve as a “stand-alone” lesson, we recommend completing modules in order from start to finish.\nIn this course you will gain practice in: \nData and file management: understanding RProjects and file paths\nUnderstand and identifying different data formats (i.e., wide, long, tidy)\nWorking with different data structures (i.e., vectors, dataframes, lists)\nImporting and exporting various water resources data\nStrategies for Exploratory Data Analysis (EDA)\nStrategies for troubleshooting (reading documentation, intro to reprex)\nTransforming data with {dplyr}\nData visualization with {ggplot2}\nData presentation and communication with RMarkdown\n\n\n\nFigure 1: Artwork by @allison_horst\n\n\n\n\nCourse Modules\nInstall R and RStudio\nGet oriented in RStudio\nPractice data and file management: understand RProjects and file paths\nImport and export various water resources data\nVisualize data with {ggplot2}\nUnderstand and identify different data structures (i.e., vectors, dataframes, lists)\nTransform data with {dplyr}\nDiscuss spreadsheets and pivots\nLearn how to write custom functions\nJoin different datasets together\nUse spatial data to create static and interactive maps\nExplore strategies for Exploratory Data Analysis (EDA)\nPractice data presentation and communication with {RMarkdown}\nExplore strategies for troubleshooting (reading documentation, intro to reprex)\n\n\nData\nAll data used in this course is expected to live in a /data subfolder in the project directory. It can be downloaded in 1 of 2 ways:\nDownload combined Rproj and dataset from the r4wrds-data Github repository\nDownload data-only from OSF\nYour project directory structure should look like this (note the position of the /data subfolder):\n.\n├── scripts\n│   ├── module_01.R\n│   └── module_02.R\n│   └── ...\n├── data\n│   ├── gwl.csv\n│   └── polygon.shp\n│   └── ...\n└── intro_proj.Rproj\nTo complete code exercises and follow along in the course, we will create these folders and download the data in the introductory project management module.\n\nWorkshop Overview\nWe will follow the SFS Code of Conduct throughout our workshop.\n\nSource content\nAll source materials for this website can be accessed at the r4wrds Github repository.\n\nAttribution\nContent in these lessons has been modified and/or adapted from Data Carpentry: R for data analysis and visualization of Ecological Data, the USGS-R training curriculum here, the NCEAS Open Science for Synthesis workshop here, Mapping in R, and the wonderful text R for data science.\n\n\nNext module:1. Install R/RStudio\nsite last updated: 2024-01-20 20:55\n\n\n\n",
      "last_modified": "2024-01-21T12:17:11-08:00"
    },
    {
      "path": "m_data_structures.html",
      "title": "6. Variable Storage & Data Structures",
      "description": "The building blocks of data science in R\n",
      "author": [],
      "contents": "\n\nContents\nAssignment\nBasic\nobject classes\nlogical\n(boolean)\nnumeric\nfactor\ncharacter\n(string)\n\nBasic data structures\nvector\nlist\ndata.frame and tibble\n\nA note on\nNA\n\n\nLearning objectives\nAssigning objects\nKnow basic object classes (logical, numeric, factor,\ncharacter)\nUnderstand basic R data structures (vectors, lists,\ndata.frames)\nKnow properties of R data structures and how to interact with\nthem\nKnow how to spot and deal with missing values (NA)\n\nAssignment\nR is an object oriented programming language, which means that it is\noriented around objects which can be data (e.g., data.frames, vectors,\nlists) or code (e.g., functions). We’ve already been using assignment to\ncreate variables, or objects. In the previous lesson, we read in csv\nfiles and assigned them to a variable using the\nassignment operator, <- (RStudio shortcut:\nAlt + -). Avoid using = for\nassignment which will also work but cause confusion later (here’s a blog post on the\nhistory behind <-).\nWhen assigning an object, avoid overly simplistic names (e.g.,\nx, y), and rather, pick concise names that\ndescribes the object and improve code interpretability. Let’s read in\nour groundwater level station data and assign it to the\nvariable name stations.\n\nIn R the only true rule about naming variables is we can’t start a\nvariable with a number or use special characters (i.e., +-!@#$%^*)\n\n\n# read the stations.csv file and assign it to an object \"stations\"\nstations <- read.csv(\"data/gwl/stations.csv\")\n\n\n\nNotice that in the Global Environment pane, we have now have a “Data”\nobject stations.\n\n\n\nLet’s look at what types of data (classes) these columns\ncurrently have. We can use some useful functions that help us explore\ndata a bit more, as well as use RStudio to figure these things out.\nWe’ll talk more about functions later – there’s an entire module on functions coming up.\nLet’s look at str() or structure first.\nThe same information can be displayed in RStudio by clicking the blue\narrow in the Environment tab.\n\n\nstr(stations)\n\n\n'data.frame':   43807 obs. of  15 variables:\n $ STN_ID     : int  51445 25067 25068 39833 25069 38479 35592 48699 20460 35590 ...\n $ SITE_CODE  : chr  \"320000N1140000W001\" \"325450N1171061W001\" \"325450N1171061W002\" \"325450N1171061W003\" ...\n $ SWN        : chr  \"\" \"19S02W05K003S\" \"19S02W05K004S\" \"19S02W05K005S\" ...\n $ WELL_NAME  : chr  \"Bay Ridge\" \"\" \"\" \"\" ...\n $ LATITUDE   : num  35.6 32.5 32.5 32.5 32.5 ...\n $ LONGITUDE  : num  -122 -117 -117 -117 -117 ...\n $ WLM_METHOD : chr  \"USGS quad\" \"Unknown\" \"Unknown\" \"Unknown\" ...\n $ WLM_ACC    : chr  \"Unknown\" \"Unknown\" \"Unknown\" \"Unknown\" ...\n $ BASIN_CODE : chr  \"\" \"9-033\" \"9-033\" \"9-033\" ...\n $ BASIN_NAME : chr  \"\" \"Coastal Plain Of San Diego\" \"Coastal Plain Of San Diego\" \"Coastal Plain Of San Diego\" ...\n $ COUNTY_NAME: chr  \"Monterey\" \"San Diego\" \"San Diego\" \"San Diego\" ...\n $ WELL_DEPTH : int  NA NA NA NA NA NA NA 280 NA NA ...\n $ WELL_USE   : chr  \"Residential\" \"Unknown\" \"Unknown\" \"Unknown\" ...\n $ WELL_TYPE  : chr  \"Part of a nested/multi-completion well\" \"Unknown\" \"Unknown\" \"Unknown\" ...\n $ WCR_NO     : chr  \"\" \"\" \"\" \"\" ...\n\n\n\n\nThis function str() tells us the structure of the data.\nIt gives us:\nthe type of Data (data.frame)\nthe column names (the part after $)\nthe data class (int=integer,\nnum=numeric, chr=character)\nthe number of rows in our dataset (or the length of the vector)\nfinally, the the first 5 values for that vector\nBasic object classes\nEvery object in R has a class property, and each\nproperty defines what functions will work on it. Many bugs result from\nfunctions applied to the wrong object class, so it’s important to know\nhow to check the class of an object and figure out what functions can be\napplied to it.\nThere are more base object classes in R, like matrices and arrays,\nbut in this course we will focus on vectors, and devote most of our\nattention to a special type of list called the data.frame\nor tibble.\nLet’s check the class of the objects we created above with the\nclass() function:\n\n\nclass(stations)\n\n\n[1] \"data.frame\"\n\nWhat about for a single column? Or a value we assign to some\ntext?\n\n\nclass(stations$SITE_CODE)\n\n\n[1] \"character\"\n\nriver_name <- \"Sacramento River\"\nclass(river_name)\n\n\n[1] \"character\"\n\nAs expected, stations is a data.frame,\nstations$SITE_CODE is a vector of character, and\nriver_name is a single value of class character.\nVectors are objects where every entry in that object is\nthe same type of data. Sometimes, these are called\natomic vectors because each part of the vector is the\nsame.\nThere are 4 major classes of atomic vectors, arranged below in order\nof complexity.\nlogical (TRUE,\nFALSE)\nnumeric (contains both integer and double, but we\nwill only cover double)\nfactor (categorical and ordinal variables)\ncharacter (strings)\nUse the c() (concatenate or combine)\nfunction to create vectors. Let’s use c() to create each of\nthese 4 vector classes for an imaginary data set of river reaches.\n\n\n# logical: is the river dry at the time of measurement\ndry <- c(TRUE, FALSE, FALSE)\n\n# flow measured at each reach in cfs\nflow <- c(0, 57, 128)\n\n# month the measurement was taken\ndate <- factor(c(\"July\", \"January\", \"February\"), levels = month.name)\n\n# reach name\nreach <- c(\"Dry Creek\", \"Raging Waters\", \"Wild Rapids\")\n\n\n\nEach vector above has 3 entries, also called\nelements. We can check the class of each of these\nvectors:\n\n\nclass(dry)\n\n\n[1] \"logical\"\n\nclass(flow)\n\n\n[1] \"numeric\"\n\nclass(date)\n\n\n[1] \"factor\"\n\nclass(reach)\n\n\n[1] \"character\"\n\nlogical (boolean)\nLogical vectors (also called booleans) are the most simple type of\natomic vectors, and can take one of three values: TRUE,\nFALSE, or NA. Logical vectors are output as\nthe result of logical tests.\n\nThere are many ways to query or use logical tests for data. In other\nlanguages like SQL, these are words like AND,\nOR IN. In R, we use\n& for AND,\n| for OR, and\n%in% for IN. For more on this see Fig 5.1\nin R4DS.\n\n\n# Is the character string \"Merced River\" in the character vector \"reach\"?\n\"Merced River\" %in% reach\n\n\n[1] FALSE\n\n# Is the character string \"Raging Waters\" in the character vector \"reach\"?\n\"Raging Waters\" %in% reach\n\n\n[1] TRUE\n\nnumeric\nImagine you wanted to transform the numeric flow data you have from\ncubic feet per second (cfs) to gallons per minute (gpm). R is a\n“vectorized” language and allows transformations over an entire vector\nwith relative ease.\n\n\n# convert each element of \"flow\" from cfs to gpm by multiplying by 448.83\nflow_gpm <- flow * 448.83\n\n# print the result\nflow_gpm\n\n\n[1]     0.00 25583.31 57450.24\n\nfactor\nIf our factor variable month was an ordinary character\nvector, it would not sort meaningfully.\n\n\ndate_character <- c(\"July\", \"January\", \"February\")\nsort( date_character )\n\n\n[1] \"February\" \"January\"  \"July\"    \n\nThe above is out of order, but if we define the\nlevels that these ordinal variables should follow, we\ncan store the vector as a factor and get meaningful sorting behavior. R\ndefaults to alphabetic order with character vectors.\n\n\n# create a factor by specifying the levels (order) of the variable\ndate <- factor(c(\"July\", \"January\", \"February\"), levels = month.name)\nsort(date)\n\n\n[1] January  February July    \n12 Levels: January February March April May June July ... December\n\ncharacter (string)\nCharacter vectors can store arbitrary strings. There are many ways to\nwork with strings from basic string\nmanipulation, all the way to natural language processing\nthat we don’t have time to cover in this course, but you should know\nthat they exist.\nCreate strings by enclosing them with quotation marks. It doesn’t\nmatter if you use single (’) or double quotes (“), just be sure to use\nthe same quote style for a single character string!\n\n\n# create a character vector of length 1 using single quotes\nhello <- 'Why helloooo'\n\n# create a character vector of length 2 using double quotes\ninstructors <- c(\"Rich\", \"Ryan\")\n\n# paste the vectors together\npaste(hello, instructors)\n\n\n[1] \"Why helloooo Rich\" \"Why helloooo Ryan\"\n\nNotice that when we pasted together a vector of length 1 with a\nvector of length 2, we got an output character vector of length 2. This\nconcept is called “recycling” (because the shorter length vector was\nused twice, or recycled) and will come back later in this module.\n\nChallenge 1\nCreate an character vector called meals and assign it a\nstring with what you plan to have for for breakfast, lunch, and dinner\ntoday.\nCreate a numeric vector called cost with the\napproximate cost in dollars of each meal.\nCalculate the cost each meal if you ate that and only that for 365\ndays a year (Hint: multiply cost by 365, then take the\nsum()).\nBonus: Paste together a string that announces this\ncost.\n\n\nClick for Answers!\n\n\n\n# create a string of three meals\nmeals <- c(\"eggs, toast and coffee\", \"pizza\", \"tacos and salad\")\n\n# cost of each meal in dollars\ncost <- c(2.25, 5.50, 8.95)\n\n# annual cost\nannual_cost <- cost * 365\nsum(annual_cost)\n\n\n[1] 6095.5\n\n# bonus\npaste(\"Three meals a day costs\", sum(annual_cost), \"per year.\")\n\n\n[1] \"Three meals a day costs 6095.5 per year.\"\n\n\nBasic data structures\nVectors are the building blocks of more useful data structures,\nespecially the data.frame and tibble that will\nbe the focus of subsequent modules.\nvector\nAbove, we covered atomic vectors, which have one and only one class\n(logical, factor, numeric, character). All vectors have a property of\nlength greater than 1.\n\n\nlength(reach)\n\n\n[1] 3\n\nlength(flow)\n\n\n[1] 3\n\nlist\nA special type of non-atomic vector called the list can\ncontain many different types of data. Lists can contain any types of\ndata structures, even other lists!\n\n\nl <- list(dry, reach, flow, \"a random string\")\nlength(l)\n\n\n[1] 4\n\nWe can access list elements with double bracket notation\n[[ and the index (think row number) of the element we\nwant.\n\n\n# access first element of the list\nl[[1]]\n\n\n[1]  TRUE FALSE FALSE\n\n# access second element\nl[[2]]\n\n\n[1] \"Dry Creek\"     \"Raging Waters\" \"Wild Rapids\"  \n\n# access third element\nl[[3]]\n\n\n[1]   0  57 128\n\nWe can also name a list, and then access list elements with\ndouble-bracket notation and name instead of index, like\nso:\n\n\nnames(l) <- c(\"dry\", \"reach\", \"flow\", \"string\")\nl[[\"dry\"]]\n\n\n[1]  TRUE FALSE FALSE\n\ndata.frame and tibble\nThe data.frame is perhaps the most common form of data\nyou will encounter in R, and the focus of most of the rest of the\nmodules in the course. The data frame is a set of named vectors arranged\nas columns all of a common length, typically atomic vectors, but they\ncan host general vectors or lists as well1.\nLet’s use the vectors we created earlier to make a\ndata.frame called riv.\n\n\nriv <- data.frame(reach, date, dry, flow)\n\nriv\n\n\n          reach     date   dry flow\n1     Dry Creek     July  TRUE    0\n2 Raging Waters  January FALSE   57\n3   Wild Rapids February FALSE  128\n\nWe can access any column from the data frame as a vector using the\n$ notation. In RStudio, typing $ also brings\nup an auto-complete, and we can see all of the columns in the\ndata.frame.\n\n\n\nFigure 1: Use $ to access columns\n\n\n\n\n\nriv$reach\n\n\n[1] \"Dry Creek\"     \"Raging Waters\" \"Wild Rapids\"  \n\nriv$flow\n\n\n[1]   0  57 128\n\nWe can also use $ to remove a column if we assign an\nexisting column to a value of NULL, or add a new\ncolumn by entering a new column name not already present, and\nassigning it a value.\n\n\n# remove the \"Reach\" column\nriv$reach <- NULL\nriv\n\n\n      date   dry flow\n1     July  TRUE    0\n2  January FALSE   57\n3 February FALSE  128\n\n\n\n# Add the reach column back, but called \"reach_name\" this time\nriv$reach_name <- reach\nriv\n\n\n      date   dry flow    reach_name\n1     July  TRUE    0     Dry Creek\n2  January FALSE   57 Raging Waters\n3 February FALSE  128   Wild Rapids\n\nWhen we assign a vector to a data.frame with length less\nthan the total number of rows of the data.frame, R will try\nto “recycle” the vector.\n\n\n# add a vector \"tech\" for the \"field technician\" to be recycled\nriv$tech <- c(\"Rich\", \"Ryan\")\n\n\n\nUh oh! This won’t work because the length of the vector we attempted\nto add (2) to the data.frame isn’t equal to, or a multiple\nof, the number of rows (3).\n\n\nnrow(riv) # total rows is 3\n\n\n[1] 3\n\nnrow(riv) == length(c(\"Rich\", \"Ryan\"))\n\n\n[1] FALSE\n\nHowever, if we add a vector to the data.frame with\nlength 1 (which is a multiple of 3):\n\n\nriv$tech <- \"Rich\"\nriv\n\n\n      date   dry flow    reach_name tech\n1     July  TRUE    0     Dry Creek Rich\n2  January FALSE   57 Raging Waters Rich\n3 February FALSE  128   Wild Rapids Rich\n\nR recycles the string “Rich”, repeating it 3 times.\nThis is convenient, but can also lead to silent bugs, so we advise using\nthe tibble instead of the data.frame.\nThe tibble is a modern data.frame with\nstricter recycling rules, R-friendly print behavior (prints only the\nfirst 10 rows and shows column types), and a few\nmore features that make them more predictable and less likely to\nlead to bugs.\nTo use tibble data types, we need to read our data with\none of the {tidyverse}\npackages,{readr}. When we\nread stations with read_csv() it reads it in\nas a tibble, which we can verify with class(stations). We\ncan always convert stations back to just a data.frame and\nprint it to console by running data.frame(stations).\n\n\nlibrary(readr)\nstations <- read_csv(\"data/gwl/stations.csv\")\nstations_df <- data.frame(stations)\nclass(stations_df)\n\n\n[1] \"data.frame\"\n\ntibbles are essentially spreadsheets in R, flat, 2D\nrectangular data made of rows and columns. We can check the number of\nrows and columns in a tibble.\n\n\ndim(stations)\n\n\n[1] 43807    15\n\nnrow(stations)\n\n\n[1] 43807\n\nncol(stations)\n\n\n[1] 15\n\nA tibble combines lists into columns, so its length is\nthe same as its number of columns.\n\n\nlength(stations) \n\n\n[1] 15\n\n\nChallenge 2\nExtract the “LATITUDE” column from stations, assign it\nto a variable called lat, and calculate the mean\nlatitude.\nAssign a column to riv called “depth” with values 1, 2,\n3.\nAssign another column called “width” with values 10, 10, 10. Take\nadvantage of recycling when creating the “width” column.\nAssign a new variable called “area” to riv which is the\nproduct of “depth” and “width” (Hint:\nriv$depth * riv$width).\n\n\nClick for Answers!\n\n\n\n# extract LATITUDE and calculate the mean value\nlat <- stations$LATITUDE\nmean(lat)\n\n\n[1] 35.95708\n\n# add depth and width to riv\nriv$depth <- c(1, 2, 3)\nriv$width <- 10 # this vector of length 1 is recycled \n\n# calculate area and add it to riv\nriv$area <- riv$depth * riv$width\n\n\n\n\nA note on NA\nNA has a special meaning in R and designates a missing\nvalue. Operations on a vector with missing values cannot return a value\nunless we explicitly tell R to ignore these missing values.\n\n\nz <- c(2, NA, 4)\nmean(z)\n\n\n[1] NA\n\nsum(z)\n\n\n[1] NA\n\n\n\nmean(z, na.rm = TRUE)\n\n\n[1] 3\n\nsum(z, na.rm = TRUE)\n\n\n[1] 6\n\nLesson adapted from R for Data\nScience.\n\n\nPrevious\nmodule: 5. Data Visualization\nNext\nmodule: 7. Data Wrangling\n\nList-columns are a special type of data that we will\ncover elsewhere.↩︎\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_dplyr.html",
      "title": "7. Wrangling Data with dplyr",
      "description": "Making data tidying and transformation fun!\n",
      "author": [],
      "contents": "\n\nContents\nData wrangling with\n{dplyr}\nHave an\nObjective\nLearn some Core\nTools for Common Operations\nStart from\nScratch\nImport Libraries and Data\n\nfilter\nrows\nExclude rows\n\nselect columns\nThe Pipe ( %>%\n)\nmutate existing data\ngroup_by &\nsummarize\nSummarizing\nData\nRemember\nto ungroup\n\n\n\nLearning objectives\nLearn how to approach tidying and transforming datasets\nUnderstand how to use the pipe (%>%) to chain\noperations together\nLearn the core {dplyr} tools including\nfunctions (select, filter,\ngroup_by, summarize)\n\nData wrangling with\n{dplyr}\nData wrangling is usually the part of any data analysis project that\ntakes the most time. Although it may not necessarily be fun, it is\nfoundational to all the work that follows. Often, data wrangling also\ntakes significantly longer than actually performing the data analysis or\ncreating a data visualization, so do not panic if, in the future, you\nfind yourself spending a lot of time on this phase. Once you learn some\ncore approaches and tools, you can deal with nearly any dataset you may\nface!\n(ref:AHtidyplot) Illustration by @allison_horst, from Hadley Wickham’s\ntalk “The Joy of Functional Programming (for Data Science)”\n\n\n\nFigure 1: (ref:AHtidyplot)\n\n\n\nThe data wrangling process includes data import, tidying, and\ntransformation. The process directly feeds into the\nunderstanding or modeling side of data exploration, but in a\niterative way. More generally, data wrangling is the manipulation or\ncombination of datasets for the purpose of analysis, and often you have\nto rinse and repeat this process as your understanding of the data\nchanges, and as your modeling and visualization needs also changes.\n\n\n\nFigure 2: Image from “R for Data Science” by Garrett\nGrolemund and Hadley Wickham.\n\n\n\nHave an Objective\n\nAll data wrangling is based on a purpose.\n\nNo one wrangles for the sake of wrangling (usually), so the process\nalways begins by answering the following two questions:\nWhat do my input data look like?\nWhat should my output data look like given\nwhat I want to do?\nLearn some Core\nTools for Common Operations\nAt the most basic level, going from what your data looks like to what\nyou want it look like will require a few key operations. The more we\npractice these operations with common tools, the easier this will be no\nmatter the size or complexity of data that we face.\nTidy data is an important part of being able to re-use these\ntools! See this great blog\n(by Lowndes & Horst)\n\n\n\nFigure 3: Illustrations from the Openscapes blog Tidy Data\nfor reproducibility, efficiency, and collaboration by Julia Lowndes and\nAllison Horst.\n\n\n\nCommon operations that occur during data wrangling:\nselect specific variables\nfilter observations by some\ncriteria\nAdd or modify (mutate) existing\nvariables\nrename variables\narrange rows by a variable\nsummarize a variable conditional on\nothers\nThe {dplyr} package provides easy tools\nfor these common data manipulation tasks and is a core package from the\n{tidyverse} suite of packages.\nThe philosophy of {dplyr} is that\none function does one thing and the\nname of the function says what it does.\nStart from Scratch\nAny reproducible analysis should be easily repeated. Using code in a\nscript allows us to rebuild every step of the data analysis from scratch\n– from data import, to data wrangling, to visualization and modeling –\nsometimes over and over again. For that reason, it’s a good habit to\nRestart your R Session before you begin a new coding\nadventure. Sometimes this solves issues, sometimes it’s just good to\nmake sure everything runs up to the point you are working. Let’s do that\nnow!\nGo to Session >\nRestart R!\nCheck your Environment tab…it should be empty!\nImport Libraries and Data\nFirst we need to load the libraries that we’ll be using. Let’s the\n{tidyverse} packages (which includes {dplyr}\nand {ggplot2}).\n\n\nlibrary(tidyverse)\n\n\n\nNext we import a .csv of groundwater monitoring stations\nacross California1.\n\nWe can use either read.csv or read_csv\nfunctions. What’s the main difference between the two? Hint, check\nclass(stations) for each method.\n\n\n# read the stations.csv file and assign it to an object \"stations\"\nstations <- read.csv(\"data/gwl/stations.csv\")\n\n\n\nBelow your code that imports the dataset, let’s make a new section!\nGo to Code >\nInsert Section, and type your new section\nheader Wrangling Data. We can also use keyboard shortcuts\nfor this (Ctrl or ⌘ + Shift +\nR). You should notice how this now appears in an expandable\ntable of contents on the right hand side of your script pane (look for\nthe tiny button that has little gray horizontal lines on it). This\nfeature can be very helpful in keeping you and your scripts\norganized.\nfilter rows\nOne of the most common steps to slicing data is to filter\nrows of a dataframe. We can filter by a variable (e.g., column\nname) or by a condition (i.e., only values greater than 100). Remember,\n{dplyr} is designed so that one function does\none thing, and the name of the function says what it does.\n\n\n\nFigure 4: Artwork by @allison_horst\n\n\n\nLet’s filter this large dataframe of potential groundwater station\nlocations (n=43807 total!) to just stations from Sacramento County with\nfilter().\n\n\nstations_sac <- filter(stations, COUNTY_NAME == \"Sacramento\")\nnrow(stations_sac)\n\n\n[1] 494\n\nWhat happened? filter() requires the data, and then any\nnumber of filtering options. We provided just one. Here the\n== means “equal to”, while a single = would\nmean is.\nWe can also combine filter conditions in the same call! Let’s add\nanother condition that we only want Residential wells (a\ncategory of the WELL_USE column). We can get a count of how\nmany categories of WELL_USE have observations (or rows) in\nthe stations_sac dataframe using the table()\nfunction. Then we can combine multiple conditions in our\nfilter with a , (in filter,\ncommas are another way to say AND).\n\n\n# get what the categories of WELL_USE are in Sacramento County:\ntable(stations_sac$WELL_USE)\n\n\n\n                 Industrial    Irrigation   Observation         Other \n            8             2           144            90           101 \n  Residential Stockwatering       Unknown \n           76             6            67 \n\n# combine conditions with \",\" which is equivalent to AND\nstations_sac <- filter(stations, COUNTY_NAME == \"Sacramento\", WELL_USE == \"Residential\")\nnrow(stations_sac)\n\n\n[1] 76\n\nWe can mix and match filter conditions however we\nchoose. What if we want to select multiple counties? The\n%in% is a great way to provide a list of things you want\nfilter to use to see if there’s a match within your\ndataframe. Let’s filter to 3 counties: Sacramento, Placer, and El\nDorado.\n\n\n# filter to multiple counties\nsep_counties <- c(\"Sacramento\", \"Placer\", \"El Dorado\")\nstations_multcounties <- filter(stations, COUNTY_NAME %in% sep_counties)\n\n\n\n\nTo quote or not to quote? Inside tidyverse functions like\nfilter we typically don’t need to quote\nvariable names (columns) but we do need to quote\nspecific values.\nWhat happened? We used a list (using the c() to combine\ndifferent possible values of COUNTY_NAME) to specify what\ndata we wanted to keep, and we told filter to return all\nrows in the COUNTY_NAME column with entries in\n(%in%) that list of county names\n(sep_counties).\nExclude rows\nWe can also exclude rows based on conditions. To exclude, we\ncan negate a condition with the ! symbol in front of the\ncondition. Let’s exclude all stations from Yolo County. Here we show 3\nways to to do the same thing.\n\n\n# include everything except Yolo County...3 different ways\nstations_trim <- filter(stations, !COUNTY_NAME %in% c(\"Yolo\"))\nstations_trim <- filter(stations, !COUNTY_NAME == \"Yolo\")\nstations_trim <- filter(stations, COUNTY_NAME != \"Yolo\")\n\n\n\nselect columns\nWe can also select specific columns or variables of interest with the\nselect() function. Similar to filter, we pass\nthe data, and the conditions we want to use to subset our data. Let’s\nselect only STN_ID, LATITUDE,\nLONGITUDE, and COUNTY_NAME.\n\n\n# select specific columns to keep\nstations_sel1 <- select(stations, c(STN_ID, LATITUDE, LONGITUDE, COUNTY_NAME))\nnames(stations_sel1) # names of columns\n\n\n[1] \"STN_ID\"      \"LATITUDE\"    \"LONGITUDE\"   \"COUNTY_NAME\"\n\nRemember we don’t have to quote the variable names here! What if we\nwant to just remove a column? We can use the same structure, but put a\n“-” in front of our list or condition.\n\n\n# select columns to drop\nstations_sel2 <- select(stations, -c(LATITUDE:BASIN_NAME, COUNTY_NAME))\nncol(stations_sel2) # how many columns?\n\n\n[1] 8\n\nHere we used the : to say columns from\nLATITUDE through BASIN_NAME, then we\nincluded COUNTY_NAME in the list of columns to drop.\nThere are some very powerful options that exist within\nselect(), including the ability to rename columns, or\nselect by things that relate to the column names. Let’s select columns\nthat start with “W” or contain “NAME”.\n\n\n# select columns to drop\nstations_sel3 <- select(stations, starts_with(\"W\"), contains(\"NAME\"))\nnames(stations_sel3)\n\n\n[1] \"WELL_NAME\"   \"WLM_METHOD\"  \"WLM_ACC\"     \"WELL_DEPTH\" \n[5] \"WELL_USE\"    \"WELL_TYPE\"   \"WCR_NO\"      \"BASIN_NAME\" \n[9] \"COUNTY_NAME\"\n\nThe Pipe ( %>% )\nWe now have two ways to slice and dice our data, filter rows and\nselect by columns. A very useful tool you may have seen is something\ncalled a “pipe”. In R, this is represented by the symbol\n%>% and can be inserted with the\nkeyboard shortcut Ctrl or ⌘ +\nShift + M.\nThe pipe is a way to chain multiple commands together, and it passes\nthe result of the first function to the second function This is\nparticularly helpful in R when you don’t need to (or want to) assign\ntemporary steps to your code. For example, compare a filter, select, and\nrename set of operations with and without the %>%.\n\n\n# filter\nstations_multcounty1 <- filter(stations, COUNTY_NAME %in% \n                                  c(\"Sacramento\", \"Placer\"))\n\n# select\nstations_multcounty2 <- select(stations_multcounty1, starts_with(\"W\"), \n                               contains(\"NAME\"), contains(\"ID\"))\n\n# rename the STN_ID to station_id: rename(new_col_name, old_col_name)\nstations_multcounty3 <- rename(stations_multcounty2, station_id = STN_ID)\n\n\n\nNow let’s rewrite the code above with %>%. Notice we\ndon’t need to explicitly define intermediate variables for each step,\nbecause data is “piped” between steps.\n\n\n# filter, select, and rename\nstations_multcounty <- stations %>%\n  filter(COUNTY_NAME %in% c(\"Sacramento\", \"Placer\")) %>% \n  select(starts_with(\"W\"), contains(\"NAME\"), contains(\"ID\")) %>%\n  rename(station_id = STN_ID)\n\n\n\nWe piped (%>%) the data to filter(), and\nthe result of the filter() to select(), and\nthen piped the result of that to rename(). Since we\nassigned all of this at the very beginning to\nstations_multcounty, that’s our final output.\n\nChallenge 1: You\nTry!\nUsing the stations dataset, filter to only\nResidential wells that have a\nWELL_DEPTH > 1000 feet.\nSelect only STN_ID, WELL_DEPTH,\nWELL_NAME, BASIN_NAME, and\nCOUNTY_NAME columns in the dataframe.\nHow many records are in Los Angeles?\n\n\nClick for Answers!\n\n\n\n# filter and select\nchallenge1 <- stations %>% \n  filter(WELL_USE == \"Residential\", WELL_DEPTH > 1000) %>% \n  select(STN_ID, WELL_DEPTH, ends_with(\"NAME\"))\nnames(challenge1)\n\n\n[1] \"STN_ID\"      \"WELL_DEPTH\"  \"WELL_NAME\"   \"BASIN_NAME\" \n[5] \"COUNTY_NAME\"\n\n# count the number of rows for each unique value of COUNTY_NAME\ntable(challenge1$COUNTY_NAME)\n\n\n\n   Los Angeles      Riverside San Bernardino       Siskiyou \n             5              3             12              2 \n        Tulare        Ventura \n             1              3 \n\n\nmutate existing data\nmutate is a function that modifies existing data, either\nby adding a new column, or modifying existing columns. When using\nmutate we expect R to return some modified version of our\ninput dataframe… but it should still be a version of the original\ndataframe.\n\n\n\nFigure 5: Artwork by @allison_horst\n\n\n\nmutate allows us to pass other functions or data into\nour dataframe, and is a powerful way to clean and tidy our data.\nLet’s create a new WELL_DEPTH_m column which converts\nthe existing WELL_DEPTH column from feet to meters. We pass\nthe new column name and then the function or operation we want to\npreform to create the new column (or modify an existing column).\n\n\nstations_mutate1 <- stations %>% \n  mutate(WELL_DEPTH_m = WELL_DEPTH * 0.3048)\n\n\n\nLet’s use ggplot2 to check and see if this worked. We\ncan use the %>% here too!\n\n\nstations_mutate1 %>% \n  # pass stations_mutate1 into the ggplot function\n  ggplot() + \n  # add a point geom in feet\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH), color = \"cyan4\", alpha = 0.5) +\n  # add a point geom in meters\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH_m), color = \"maroon\", pch = 21, alpha = 0.8)\n\n\n\n\nAs shown above, the reddish circles for observations in units of\nmeters indicate smaller values than the blueish circles for observations\nin units of feet. This looks about right! It is rarely a bad idea to\n“visualize” your outcomes whenever possible, these are great ways to\ndouble check things are happening as you intend when data wrangling.\n\nExtra Practice\n\nLet’s pause a moment and do this in base R without the\ntidyverse so we can see both methods. There are many positives about\nknowing different approaches, and also when troubleshooting, it can be\nhelpful to understand different ways of doing the same thing. Can you\nspot a few differences?\n\n\n# filter and select (subset)\nstations_basefilter <- stations[stations$WELL_USE == \"Residential\" & \n                                  stations$WELL_DEPTH > 1000,\n                                c(1, 4, grep(\"NAME\", colnames(stations)))] \n\n# mutate\nstations_base1 <- stations # make a copy so we don't alter original data\nstations_base1$WELL_DEPTH_m <- stations_base1$WELL_DEPTH * 0.3048\n\n# ggplot\nggplot(data = stations_base1) + \n  geom_point(aes(x = STN_ID, y = WELL_DEPTH), \n             color = \"cyan4\", alpha = 0.5) +\n  # this in meters\n  geom_point(aes(x = STN_ID, y = WELL_DEPTH_m), \n             color = \"maroon\", pch = 21, alpha = 0.8)\n\n\n\n\ngroup_by &\nsummarize\nThe final core {dplyr} “verbs” to be aware of are\ngroup_by and summarize. These are really\nuseful to transform your data into an entirely new\ndataset, and to do many operations at once based on different columns\n(variables) within your dataset.\nFor example let’s say we want a count of how many records exist for\neach county. We can use group_by and\ncount.\n\n\nn_by_county <- stations %>% \n  group_by(COUNTY_NAME) %>% \n  count()\n\nhead(n_by_county)\n\n\n# A tibble: 6 × 2\n# Groups:   COUNTY_NAME [6]\n  COUNTY_NAME     n\n  <chr>       <int>\n1 Alameda       237\n2 Alpine         33\n3 Amador         11\n4 Butte         302\n5 Calaveras      17\n6 Colusa        229\n\nWhat happened? group_by() groups the data based on the\nvariable(s) included in the function. Once our data.frame\nis grouped, there are many options and functions we can apply, including\nthings like count(), tally(), and\nsummarize().\nLet’s look at this a bit more. What if we really only want the 5\ncounties with the most observations? We can add two additional options\nhere. The arrange() function will sort our data by a\nvariable(s) of our choice, and we can use desc() to specify\nwe want that data in descending order. We then pipe that to\nhead() and ask for just the first 5 rows.\n\n\nstations %>% \n  group_by(COUNTY_NAME) %>% \n  count() %>% # count by COUNTY_NAME\n  arrange(desc(n)) %>% # sort by column n in descending order\n  head(5) # return the first 5 rows\n\n\n# A tibble: 5 × 2\n# Groups:   COUNTY_NAME [5]\n  COUNTY_NAME        n\n  <chr>          <int>\n1 Fresno          6053\n2 San Bernardino  5837\n3 Kern            5155\n4 Riverside       2636\n5 Merced          2479\n\nSummarizing Data\nSimilar to mutate, summarize allows us to\ncreate new columns. The difference is summarize creates a\ncompletely new dataframe based on the group_by. Thus,\nsummarize always follows a group_by. Here we\ncan do a similar analysis to what we did above, but now we can customize\nour columns and what we want to calculate. Let’s calculate the average\ndepth of wells in all counties, then plot the top 10 counties with the\ndeepest average well depth.\n\n\nstations %>% \n  group_by(COUNTY_NAME) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH, na.rm = TRUE)) %>% \n  arrange(desc(mean_well_depth)) %>% \n  head(10) %>% # return the first 5 rows and pass to ggplot\n  ggplot() + \n  geom_col(aes(x = COUNTY_NAME, y = mean_well_depth)) +\n  labs(title = \"Mean well depth\", \n       subtitle = \"Top 10 Counties with deepest wells\")\n\n\n\n\n\nThis is a plot to consider rotating the axis labels. How do we do that?\nPractice your web-search skills or visit the ggplot2 help page.\nHint: look at theme(axis.text.x = ). Another option is\ncoord_flip().\nWe looked at only the top 10 counties before, but remember that\ngroup_by and summarize work on entire\ndataframes. To illustrate how these operations scale to entire\ndataframes, if we wanted to visualize the mean well depth for all\ncounties in the data, we could do so with slightly modified code:\n\n\nstations %>% \n  # group by the county name\n  group_by(COUNTY_NAME) %>% \n  # calculate the mean well depth\n  summarize(mean_well_depth = mean(WELL_DEPTH, na.rm = TRUE)) %>% \n  # remove two counties that have NA values\n  filter(!is.na(mean_well_depth)) %>% \n  # pass to ggplot\n  ggplot() + \n  geom_col(aes(x = fct_reorder(COUNTY_NAME, mean_well_depth), \n               y = mean_well_depth)) +\n  labs(title = \"Mean well depth\", \n       subtitle = \"Periodic groundwater level database\", \n       y = \"Mean well depth (ft)\",\n       x = \"\") +\n  coord_flip()\n\n\n\n\nWe can add additional groups or columns depending on what we are\ninterested in. Let’s try the following:\nfind the mean well depth by each WELL_USE category\nusing summarize\nfilter to only groups that have more\nthan 10 stations and no NA’s (for NA’s, we can use the\nis.na() function, and the negate or ! symbol\nwe used previously)\npipe to ggplot and make a boxplot to visualize the result\n\n\nstations %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth)) %>% \n  ggplot() + \n  geom_boxplot(aes(x = WELL_USE, y = mean_well_depth), \n               fill = \"seagreen\", alpha = 0.5) +\n  labs(title = \"Well Depth for Groundwater Stations in CA\",\n       subtitle = \"For groups with >10 stations\",\n       x = \"Well Use\", \n       y = \"Well Depth (ft)\")\n\n\n\n\nWhat happened? We grouped by two different variables, and then\napplied functions (or transformed the data) to those groups using\nsummarize, then we filtered, and visualized. The order of\nthese steps can influence your analysis and result, so it’s good to\nconsider when you want to filter and when you want to group_by/summarize\nthings. Let’s try a challenge!\n\nChallenge 2: You\nTry!\nUsing the code we ran above, figure out how to make this boxplot\nwithout the “Unknown” and blank (\"\")\nWELL_USE categories.\nTry adding points as another layer over your boxplot using\ngeom_jitter.\nFor a bonus challenge, try adding a caption with information about\nthis dataset, and plotting these data in meters instead of feet.\n\n\nClick for Answers!\n\n\n\nstations %>% \n  filter(WELL_USE != \"\", WELL_USE != \"Unknown\") %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            mean_well_depth_m = mean_well_depth * 0.3048,\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth_m)) %>%\n  ggplot() + \n  geom_boxplot(aes(x = WELL_USE, y = mean_well_depth_m), \n               fill = \"seagreen\", alpha = 0.5) +\n  # add jittered points\n  geom_jitter(aes(x = WELL_USE, y = mean_well_depth_m), \n              color = \"forestgreen\", alpha = 0.5) +\n  # add labels and caption\n  labs(title = \"Well Depth for Groundwater Stations in CA\",\n       subtitle = \"For groups with >10 stations\",\n       caption  = \"Data from the DWR Periodic Groundwater Level Database.\",\n       x = \"Well Use\", \n       y = \"Well Depth (ft)\")\n\n\n\n\n\nRemember to ungroup\nA common blunder that can impact downstream analyses, or even the\ncurrent analysis you may want to do is forgetting to ungroup.\n{dplyr} is smart, and will do exactly as you program it to\ndo, but when you use group_by, your output dataframe\nwill retain those groups for future use. This can be\nhelpful in some cases, but it’s important to keep an eye on and be aware\nof (for when it’s not helpful). We can always use\nclass() to better check this. Let’s try some code from\nabove and check with class().\n\n\nstations_by_cnty_use <- stations %>% \n  filter(WELL_USE != \"\", WELL_USE != \"Unknown\") %>% \n  group_by(COUNTY_NAME, WELL_USE) %>% \n  summarize(mean_well_depth = mean(WELL_DEPTH),\n            total_records = n()) %>% \n  filter(total_records > 10, !is.na(mean_well_depth)) \n\nclass(stations_by_cnty_use)\n\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nHere we can see the grouped_df title, which tells us\nthis is a grouped dataset. We can easily\nungroup with the concisely named ungroup()\nfunction. This isn’t required, but it’s a good habit to get into (or be\naware of).\n\n\nstations_by_cnty_use <- ungroup(stations_by_cnty_use)\nclass(stations_by_cnty_use)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nCongratulations! You’ve made it through {dplyr}. There\nare many more great options and tricks within {dplyr}, but\nhopefully this will get you going.\n\n\nPrevious\nmodule: 6. Data Structures\nNext\nmodule: 8. Spreadsheets & Pivots\n\nsee here\nfor more about this data↩︎\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_exploratory_DA.html",
      "title": "12. Exploratory Data Analysis",
      "description": "Learn to enjoy EDA adventures\n",
      "author": [],
      "contents": "\n\nContents\nWhat’s EDA?\nGenerate\nquestions\nSearch for\nAnswers\nQuestion 1: Well use and\nlocation\nQuestion 2: Well depths\nQuestion 3:\nGroundwater level change through time\nQuestion 4:\nRelating groundwater to CES scores\n\nLearn more\nCommunicate results\n\n\nLearning objectives\nUnderstand when and how to carry out Exploratory Data Analysis\n(EDA)\nPractice EDA with tools and data from previous modules\n\nWhat’s EDA?\nExploratory\nData Analysis, or EDA, is an approach to data analysis that allows\nthe data analyst to explore data and identify hypotheses or additional\nquestions to test. In the book, R for\nData Science, EDA is described as an iterative cycle where\nyou:\n\nGenerate questions about your data.\nSearch for answers by visualizing, transforming, and modeling your\ndata.\nUse what you learn to refine your questions and/or generate new\nquestions for communication.\n\nThis process can be applied to any data, and is foundational to data\nscience. Ultimately it is how we understand and then communicate our\ndata.\n(ref:ah-openscapes) Illustration by @allison_horst for Dr. Julia Lowndes\nuseR!2019 keynote.\n\n\n\nFigure 1: (ref:ah-openscapes)\n\n\n\nIn previous modules, we’ve covered the building blocks to perform EDA\nin R, and in this module, we’re going to bring it all together and\nperform EDA on the groundwater measurements dataset1 and\nCalEnviroscreen 3.0 data.2\nWe will focus on generating questions, and answering them through\nvisualization.3\nGenerate questions\nOur dataset contains observations of groundwater level measurements\nat monitoring stations throughout Sacramento County, and these\nobservations have been spatially joined by census tract to\nCalEnviroScreen 3.0 (CES) scores.\nTo provide context for our analysis, the groundwater elevation\ndataset is based on measurements of the depth to groundwater in an\naquifer system. These measurements are taken at individual wells (Figure\n2).\n(ref:aquifer) Wells at different locations and depths in an\naquifer, from Wikimedia\nCommons.\n\n\n\nFigure 2: (ref:aquifer)\n\n\n\nTo begin, let’s ask a few general questions:\nWhat well uses (e.g., domestic, public, agricultural) are most\ncommon, and where are they located?\nHow do well depths compare between well uses?\nWhat are the historical trends in groundwater elevation?\nHow do CES scores relate to groundwater level trends?\nWe will use the following packages in our EDA, so we load them\nnow.\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(mapview)\n\n\n\nSearch for Answers\nFirst we need to load our data, which we created in the module on joins and binds, and then inspect what’s\nthere.\n\n\n# groundwater level measurements joined to stations, perforations, and CES data\n\n# load imports an object named \"gwl\"\nload(\"data/sacramento_gw_data_w_calenviro.rda\")\n\n# alternatively we can specify an object name when using an .rds file\n# gwl <- read_rds(\"data/sacramento_gw_data_w_calenviro.rds\")\n\n# check class, dim, names to refresh memory\nclass(gwl)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(gwl)\n\n\n[1] 31735    91\n\nnames(gwl)\n\n\n [1] \"STN_ID\"                                           \n [2] \"SITE_CODE\"                                        \n [3] \"SWN\"                                              \n [4] \"WELL_NAME\"                                        \n [5] \"LATITUDE\"                                         \n [6] \"LONGITUDE\"                                        \n [7] \"WLM_METHOD\"                                       \n [8] \"WLM_ACC\"                                          \n [9] \"BASIN_CODE\"                                       \n[10] \"BASIN_NAME\"                                       \n[11] \"COUNTY_NAME\"                                      \n[12] \"WELL_DEPTH\"                                       \n[13] \"WELL_USE\"                                         \n[14] \"WELL_TYPE\"                                        \n[15] \"WCR_NO\"                                           \n[16] \"TOP_PRF\"                                          \n[17] \"BOT_PRF\"                                          \n[18] \"WLM_ID\"                                           \n[19] \"MSMT_DATE\"                                        \n[20] \"WLM_RPE\"                                          \n[21] \"WLM_GSE\"                                          \n[22] \"RDNG_WS\"                                          \n[23] \"RDNG_RP\"                                          \n[24] \"WSE\"                                              \n[25] \"RPE_WSE\"                                          \n[26] \"GSE_WSE\"                                          \n[27] \"WLM_QA_DESC\"                                      \n[28] \"WLM_DESC\"                                         \n[29] \"WLM_ACC_DESC\"                                     \n[30] \"WLM_ORG_ID\"                                       \n[31] \"WLM_ORG_NAME\"                                     \n[32] \"MSMT_CMT\"                                         \n[33] \"COOP_AGENCY_ORG_ID\"                               \n[34] \"COOP_ORG_NAME\"                                    \n[35] \"tract\"                                            \n[36] \"Total Population\"                                 \n[37] \"California County\"                                \n[38] \"ZIP\"                                              \n[39] \"Nearby City \\n(to help approximate location only)\"\n[40] \"Longitude\"                                        \n[41] \"Latitude\"                                         \n[42] \"CES 3.0 Score\"                                    \n[43] \"CES 3.0 Percentile\"                               \n[44] \"CES 3.0 \\nPercentile Range\"                       \n[45] \"SB 535 Disadvantaged Community\"                   \n[46] \"Ozone\"                                            \n[47] \"Ozone Pctl\"                                       \n[48] \"PM2.5\"                                            \n[49] \"PM2.5 Pctl\"                                       \n[50] \"Diesel PM\"                                        \n[51] \"Diesel PM Pctl\"                                   \n[52] \"Drinking Water\"                                   \n[53] \"Drinking Water Pctl\"                              \n[54] \"Pesticides\"                                       \n[55] \"Pesticides Pctl\"                                  \n[56] \"Tox. Release\"                                     \n[57] \"Tox. Release Pctl\"                                \n[58] \"Traffic\"                                          \n[59] \"Traffic Pctl\"                                     \n[60] \"Cleanup Sites\"                                    \n[61] \"Cleanup Sites Pctl\"                               \n[62] \"Groundwater Threats\"                              \n[63] \"Groundwater Threats Pctl\"                         \n[64] \"Haz. Waste\"                                       \n[65] \"Haz. Waste Pctl\"                                  \n[66] \"Imp. Water Bodies\"                                \n[67] \"Imp. Water Bodies Pctl\"                           \n[68] \"Solid Waste\"                                      \n[69] \"Solid Waste Pctl\"                                 \n[70] \"Pollution Burden\"                                 \n[71] \"Pollution Burden Score\"                           \n[72] \"Pollution Burden Pctl\"                            \n[73] \"Asthma\"                                           \n[74] \"Asthma Pctl\"                                      \n[75] \"Low Birth Weight\"                                 \n[76] \"Low Birth Weight Pctl\"                            \n[77] \"Cardiovascular Disease\"                           \n[78] \"Cardiovascular Disease Pctl\"                      \n[79] \"Education\"                                        \n[80] \"Education Pctl\"                                   \n[81] \"Linguistic Isolation\"                             \n[82] \"Linguistic Isolation Pctl\"                        \n[83] \"Poverty\"                                          \n[84] \"Poverty Pctl\"                                     \n[85] \"Unemployment\"                                     \n[86] \"Unemployment Pctl\"                                \n[87] \"Housing Burden\"                                   \n[88] \"Housing Burden Pctl\"                              \n[89] \"Pop. Char.\"                                       \n[90] \"Pop. Char. Score\"                                 \n[91] \"Pop. Char. Pctl\"                                  \n\nWe might want to use View(gwl) these data to refresh our\nmemory. Remember when View()ing data in RStudio, that only\n50 columns are shown at a time, so you need to click the arrows in the\ntop navigation bar to move between sets of 50 columns.\n\n\n\nIt appears that we have two columns with “County” information, so\nlet’s drop the column from CES and keep the one from the groundwater\nlevel dataset.\n\n\ngwl <- select(gwl, -`California County`)\n\n\n\nFinally, let’s keep in mind that the dataframe structure of this\ndataset is many samples per well through time (see the\nSITE_CODE is repeated for multiple MSMT_DATE\nobservations through time).\nQuestion 1: Well use and\nlocation\nWhat well uses are most common? We can answer this with\ntable(gwl$WELL_USE), but let’s do the same thing with\n{dplyr} functions.\n\n\ngwl %>% \n  count(WELL_USE) %>% # group by well use and summarize the count\n  arrange(desc(n))    # sort in decreasing order \n\n\n# A tibble: 8 × 2\n  WELL_USE          n\n  <chr>         <int>\n1 Irrigation    11539\n2 Unknown        6813\n3 Residential    5863\n4 Observation    4170\n5 Other          2435\n6 Stockwatering   497\n7 <NA>            248\n8 Industrial      170\n\nWe could have come to this same table by way of a plot.\n\n\ngwl %>% \n  ggplot(aes(WELL_USE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nLet’s clean things up a bit and make this plot more visually\nappealing.\n\n\np1 <- gwl %>% \n  count(WELL_USE) %>%   # group by well use and summarise the count\n  arrange(desc(n)) %>%  # sort in decreasing order \n  filter(!is.na(WELL_USE)) %>% # remove NA well uses\n  ggplot(aes(fct_reorder(WELL_USE, n), n)) + # reorder well use by n\n  geom_col(aes(fill = WELL_USE)) +    # use column geometry\n  coord_flip() +  # flip x and y axes\n  theme_classic() + # use a theme\n  labs(title = \"Monitoring well use\", \n       subtitle = \"Sacramento County\",\n       x = \"\", y = \"Count\") +\n  guides(fill = \"none\") # remove colorbar\n\np1\n\n\n\n\nUnderstanding where wells are located is a spatial question, luckily\nthese data contain spatial information we can use to make a map (see our\nlast module how to\nconvert a dataframe to an sf object). We know the well coordinates\nare using a projection and coordinate reference system in NAD83 (EPSG\n4269), so we can convert this gw object to\nan {sf} object class. We will also import a Sacramento county polygon\nshapefile for plotting.\n\n\n# convert gwl from dataframe to sf\ngwl <- st_as_sf(gwl, \n                coords = c(\"LONGITUDE\", \"LATITUDE\"), # note x goes first\n                crs    = 4269,                       # projection, NAD83\n                remove = FALSE) %>%                  # don't remove lat/lon\n  st_transform(3310) # convert to geographic CRS\n\n# verify transformation worked\nclass(gwl)\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# also read in the Sacramento county shapefile for plotting\n# and transform it to the same crs as gwl\nsac <- st_read(\"data/shp/sac/sac_county.shp\") %>% \n  st_transform(st_crs(gwl))\n\n\nReading layer `sac_county' from data source \n  `/Users/richpauloo/Documents/GitHub/r4wrds/intro/data/shp/sac/sac_county.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -13565710 ymin: 4582007 xmax: -13472670 ymax: 4683976\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# verify gwl points and sac polygon are in the same crs\nst_crs(gwl)$epsg == st_crs(sac)$epsg\n\n\n[1] TRUE\n\nLet’s plot the well use in Sacramento County, similar to what we did\nin the previous mapmaking module.\nNote, because we have many measurements per well we need to reduce these\ndata to a distinct() list of SITE_CODEs. There\nare several ways to do this, but one option is to leverage how\ngroup_by() works, and slice() just\n1 observation from each group, giving us a unique list\nof sites.\n\n\n# because we're only plotting location, which doesn't change between \n# measurements, we slice the first observation per SITE_CODE\ngwl_minimal <- gwl %>% \n  filter(!is.na(WELL_USE)) %>% # remove wells without a well use\n  group_by(SITE_CODE) %>%      # take first well per site code\n  slice(1) \n\n# check number of stations, should be n=486\nlength(unique(gwl_minimal$SITE_CODE))\n\n\n[1] 486\n\n# map\np2 <- ggplot() +\n  geom_sf(data = sac) + \n  geom_sf(data = gwl_minimal, aes(color = WELL_USE), alpha = 0.4) +\n  facet_wrap(~WELL_USE) +\n  guides(color = \"none\") +\n  theme_void()\n\np2\n\n\n\n\nCombining Plots\nThe {patchwork} R package is great for combining plots. We can\ncombine the previous 2 plots with a simple “+” once\npatchwork is loaded.\n\n\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\nThese plots tell us a lot about the distribution of monitoring wells\nin Sacramento County. For instance, they show that irrigation and\nresidential wells are among the most common known well uses.\nInterestingly, a substantial number of wells have an unknown use.\nIrrigation and residential wells appear collocated.\n\n\nChallenge 1: Grouped\nsummary\nWhat is the average number of samples at each SITE_CODE\nper WELL_USE?\nCan you express the distribution of samples at each\nSITE_CODE per WELL_USE as a boxplot?\n\n\nClick for Answers!\n\n\n\n# group by the site code and well use and count\ncount(gwl, SITE_CODE, WELL_USE) %>% \n  pull(n) %>% \n  mean()\n\n\n[1] 64.24089\n\n\n\n# express as boxplot of number of samples per site code, at each well use\ncount(gwl, SITE_CODE, WELL_USE) %>% \n  ggplot(aes(WELL_USE, n)) + \n  geom_boxplot() +\n  # limit y axis scale to focus on main bulk of distribution\n  coord_cartesian(ylim = c(0, 250)) \n\n\n\n\n\nQuestion 2: Well depths\nNow that we understand a bit about the relative proportion and\nspatial distribution of wells, let’s compare total completed depths,\nwhich measure how deep the well is and all else being equal, relates to\nthe well’s ability to access groundwater.\nWe made a plot that explored these trends in the previous mapmaking\nmodule.\n\n\n\nWe have the spatial distribution of these values, but now let’s\nsummarize the distribution of well depth values themselves.\n\n\ngwl_minimal %>% \n  ggplot(aes(WELL_USE, WELL_DEPTH)) +\n  geom_boxplot() + \n  coord_flip(ylim = c(0,1000)) # zoom in on main data distribution\n\n\n\n\nIt is clear that irrigation wells tend to be much deeper than\nresidential, observation, and stock watering wells. There’s about a 140\nfoot difference between median irrigation and residential well depth. We\ncan calculate this exact difference as follows:\n\n\n# median well depths\nmedian_well_depths <- filter(gwl_minimal, \n                             WELL_USE %in% c(\"Residential\", \"Irrigation\")) %>% \n  group_by(WELL_USE) %>% \n  summarize(med_depth = median(WELL_DEPTH, na.rm = TRUE)) %>% \n  st_drop_geometry() # don't need spatial data here, drop geometry column\n\nmedian_well_depths\n\n\n# A tibble: 2 × 2\n  WELL_USE    med_depth\n* <chr>           <dbl>\n1 Irrigation        317\n2 Residential       180\n\n# difference of median well depths\ndiff(median_well_depths$med_depth)\n\n\n[1] -137\n\nThe diff() function returns the difference between each\nvalue pair in a vector. Try diff(c(5, 20, 30)).\n\nPause and think\nWould we get a different boxplot if we passed in gwl\ninstead of gwl_minimal? Which is correct to use and\nwhy?\n\n\nClick for Answers!\n\nWe would indeed have a different result if we passed in\ngwl instead of gwl_minimal. Recall that\ngwl has 31735 rows but there are 494 unique\nSITE_CODEs in gwl. If we pass in\ngwl instead of gwl_minimal, we’re computing a\nboxplot on duplicate values of WELL_DEPTH for each\nSITE_CODE, where the number of samples per individual\nwell can influence the computed summary statistics. It’s correct to\nuse gwl_minimal because there’s only one\nWELL_DEPTH for each SITE_CODE. This is easier\nto visualize than explain. Note the subtle difference in boxplots.\n\n\n# verify that using gwl v gwl_minimal gives us different results\npbox1 <- gwl_minimal %>% \n  ggplot(aes(WELL_USE, WELL_DEPTH)) +\n  geom_boxplot()\n\npbox2 <- gwl %>% \n  filter(!is.na(WELL_USE)) %>% \n  ggplot(aes(WELL_USE, WELL_DEPTH)) +\n  geom_boxplot()\n\npbox1 + pbox2\n\n\n\n\nWe can also look at raw numbers to spot differences in median values\ncomputed from gwl and gwl_minimal.\n\n\n# demonstrate differences in median well depth \ngwl %>% \n  group_by(WELL_USE) %>% \n  summarise(med = median(WELL_DEPTH, na.rm = TRUE)) %>% \n  st_drop_geometry()\n\n\n# A tibble: 8 × 2\n  WELL_USE        med\n* <chr>         <dbl>\n1 Industrial       85\n2 Irrigation      310\n3 Observation     250\n4 Other           440\n5 Residential     185\n6 Stockwatering   191\n7 Unknown         205\n8 <NA>            210\n\ngwl_minimal %>% \n  group_by(WELL_USE) %>% \n  summarise(med = median(WELL_DEPTH, na.rm = TRUE)) %>% \n  st_drop_geometry()\n\n\n# A tibble: 7 × 2\n  WELL_USE        med\n* <chr>         <dbl>\n1 Industrial     248.\n2 Irrigation     317 \n3 Observation    140.\n4 Other          420 \n5 Residential    180 \n6 Stockwatering  175 \n7 Unknown        166 \n\nThis is all to highlight that it’s important to remember what data\nyou’re feeding into functions. Many a nightmarish bug has been caused by\nthe data analyst thinking their data is in one form, when it’s actually\nin another!\n\nQuestion 3:\nGroundwater level change through time\nWe’ve been working mostly with station data above for the 494 unique\nstations. In fact, we could have performed most of our analyses above\nusing only the stations.csv file we saw in previous\nmodules. Now we drill down into the groundwater data itself, which\ncontains many more observations (n = 31735).\nLet’s start by plotting all depths to groundwater elevations per\nsite. Recall that GSE_WSE is the depth to groundwater in\nfeet below land surface. We need to add group here to group\nobservations by the station or SITE_CODE.\n\n\ngwl %>% \n  ggplot(aes(MSMT_DATE, GSE_WSE, group = SITE_CODE)) +\n  geom_line(alpha = 0.5)\n\n\n\n\nIt generally appears that depth to groundwater is increasing over\ntime (groundwater depletion), but a few clearly erroneous values >\n2504 feet are impairing the plot. Let’s\nremove these values and re-plot.\n\n\n# create a new object that filters out values > 250\ngwl_filt <- filter(gwl, GSE_WSE <= 250)\n\ngwl_filt %>% \n  ggplot(aes(MSMT_DATE, GSE_WSE, group = SITE_CODE)) +\n  geom_line(alpha = 0.5)\n\n\n\n\nThis is better, but still hard to discern individual trends over\ntime. Let’s facet by well use to see if we can address this, and color\neach line by the SITE_CODE. Note, we turn the legend off\nwith show.legend=FALSE because there are hundreds of\nstations and a legend for each one would overwhelm the plot.\n\n\ngwl_filt %>% \n  ggplot(aes(MSMT_DATE, GSE_WSE, group = SITE_CODE, color=SITE_CODE)) +\n  geom_line(alpha = 0.5, show.legend=FALSE) +\n  facet_wrap(~WELL_USE)\n\n\n\n\nLet’s tie this back to our spatial data to answer the more specific\nquestion, “which areas have experienced the largest drop in\ngroundwater levels over their historical period of record?”\nHere are a few possible ways to constrain this analysis to make\ninterpretation easier.\nWe’ll only include wells that have 30 or more groundwater level\nmeasurements\nWe’ll focus on the historical record by limiting data to\nobservations recorded before 1980-01-01 or earlier.\nWe’ll focus on residential and irrigation wells because they have\nthe most data and overlap in space.\nFirst we need to filter() to the\nSITE_CODEs that meet our constraints.\n\n\n# find SITE_CODEs that meet well use and time constraints\nids_use_time <- gwl_filt %>% \n  filter(WELL_USE %in% c(\"Residential\", \"Irrigation\"), \n         MSMT_DATE <= \"1980-01-01\") %>% \n  pull(SITE_CODE) %>% \n  unique()\n\n# ids that meet time constraints and sample constraints\nids_time_samp <- gwl_filt %>% \n  filter(SITE_CODE %in% ids_use_time) %>% \n  count(SITE_CODE) %>% \n  filter(n >= 30) %>% \n  pull(SITE_CODE)\n\n# total number of well station ids that meet constraints\nids_time_samp %>% length()\n\n\n[1] 74\n\n# the time span these data cover\ngwl_filt %>% \n  filter(SITE_CODE %in% ids_time_samp) %>%  \n  pull(MSMT_DATE) %>% \n  range()\n\n\n[1] \"1942-08-05 UTC\" \"2020-09-17 UTC\"\n\nOf the 415 site codes, 74, or about 15% meet our constraints, and\ninclude observations starting as early as 1942.\nThese IDs represent long term monitoring sites that meet our\nconstraints. We can use them to filter our gwl measurement\ndata and calculate the groundwater level change over the period of\nrecord.\n\n\ngwl_diff <- gwl_filt %>% \n  # use only SITE_CODE that occur in ids_time_samp\n  filter(SITE_CODE %in% ids_time_samp) %>% \n  group_by(SITE_CODE, WELL_USE) %>% # for each site code and well use type\n  arrange(MSMT_DATE) %>%            # arrange dates in ascending order\n  summarise(t1 = first(MSMT_DATE),  # first date\n            t2 = last(MSMT_DATE),   # last date\n            gse_wse_t1 = first(GSE_WSE),    # first gwl measurement\n            gse_wse_t2 = last(GSE_WSE)) %>% # last  gwl measurement \n  mutate(diff = gse_wse_t2 - gse_wse_t1)    # diff btwn last and first gwl\n  \n# preview result\ngwl_diff %>% \n  select(SITE_CODE, t1, t2, diff) %>% \n  head()\n\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -113341.9 ymin: 27273.94 xmax: -102404.7 ymax: 31018.77\nProjected CRS: NAD83 / California Albers\n# A tibble: 6 × 5\n# Groups:   SITE_CODE [6]\n  SITE_CODE          t1                  t2                   diff\n  <chr>              <dttm>              <dttm>              <dbl>\n1 382548N1212908W001 1961-04-13 00:00:00 2012-10-11 00:00:00  15.9\n2 382613N1212086W001 1966-10-21 00:00:00 1994-04-13 00:00:00  21.3\n3 382623N1212973W001 1963-05-10 00:00:00 2020-09-17 00:00:00  15.5\n4 382625N1212626W001 1972-03-09 00:00:00 2020-03-04 00:00:00  21.1\n5 382727N1211718W001 1966-10-21 00:00:00 1997-04-25 00:00:00  27.2\n6 382893N1212127W001 1972-03-09 00:00:00 2005-11-22 00:00:00  35.8\n# … with 1 more variable: geometry <POINT [m]>\n\nNote, we can use first() and last() here\nbecause our data classes are POSIXct (datetime) and\nnumeric, so the data will sort correctly. If our data were\ncharacter or factor, these data may sort\ndifferently than expected.\nLet’s map these changes at our 74 long-term monitoring sites.\n\n\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = gwl_diff, aes(color = diff), size=2.5) +\n  scale_color_viridis_c()\n\n\n\n\nOutliers are a fairly common problem in real world data. For some\nreason, one site shows a -100 change (dark purple dot). Is\nthis a problem with the data or our analysis, or is it a real\nobservation? Let’s inspect it with a plot.\n\n\n# package to help with pasting values/text together\nlibrary(glue) \n\n# find the SITE_CODE associated with the outlier\nid_problem <- gwl_diff %>% \n  filter(diff < -90) %>% \n  pull(SITE_CODE)\n\n# plot the outlier's hydrograph\ngwl_filt %>% \n  filter(SITE_CODE == id_problem) %>% \n  ggplot(aes(MSMT_DATE, GSE_WSE)) +\n  geom_line() +\n  # add label to show the station. Surround variables with {}\n  labs(subtitle = glue(\"Outlier {id_problem}\"))\n\n\n\n\nAh ha! Everything looks okay, until measured values fall off a cliff\naround 1990. This is likely an erroneous value, so we can remove this\nsingle observation. Luckily each water level measurement has a unique ID\n(WLM_ID), so we can use this to remove the value, and then\nrecompute our groundwater level difference. Because we have this\ntransformation already in code, it’s easy to rerun this complex\noperation! One way to find the WLM_ID is to use\nView(gwl_filt), and use the\nSearch option in the upper right hand corner. Copy and paste\nthe id_problem value (386576N1212907W001), and paste it\ninto the box, then hit Enter. We should now see all the\nvalues associated with this id. We can look for a value after 1990 by\nsorting by MSMT_DATE. We should see there’s an extreme\noutlier in WSE (-1.0)! Then look for the\nassociated WLM_ID column for that observation and copy and\npaste that ID (1345292) to use with the code below.\n\n\n# uncomment and run to inspect the problem_id SITE_CODE\n# we filter for the problem ID, then select only the cols we care about\n# gwl_filt %>%\n#   filter(SITE_CODE == id_problem) %>%\n#   select(MSMT_DATE, GSE_WSE, WLM_ID) %>%\n#   View()\n\n# remove one erroneous measurement\ngwl_filt <- filter(gwl_filt, WLM_ID != \"1345292\")\n\n# recompute groundwater level difference\ngwl_diff <- gwl_filt %>% \n  # only SITE_CODE meeting time and sample constraints\n  filter(SITE_CODE %in% ids_time_samp) %>% \n  group_by(SITE_CODE, WELL_USE) %>% # for each site code and well use\n  arrange(MSMT_DATE) %>%            # arrange dates in ascending order\n  summarise(t1 = first(MSMT_DATE), # first date\n            t2 = last(MSMT_DATE),  # last date\n            gse_wse_t1 = first(GSE_WSE),    # first gwl measurement\n            gse_wse_t2 = last(GSE_WSE)) %>% # last  gwl measurement \n  mutate(diff = gse_wse_t2 - gse_wse_t1)    # diff btwn last and first gwl\n\n\n\nNext we can replot our map without this erroneous value, and spruce\nthings up a bit. Let’s also add major rivers in Sacramento County, just\nto demonstrate the LINESTRING {sf} data\ntype.\n\n\n# read in major rivers in Sacramento County\nriv <- read_rds(\"data/sac_co_main_rivers_dissolved.rds\") %>%\n  st_transform(st_crs(sac))\n\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = riv, color = \"blue\") +\n  geom_sf(data = gwl_diff, aes(fill = diff), \n          pch = 21, size = 2.7, alpha = 0.8) +\n  scale_fill_viridis_c(\"GWL \\nchange (ft)\", option = \"B\", direction = -1) +\n  facet_wrap(~WELL_USE) +\n  labs(title = \"Difference in groundwater elevation (ft)\",\n       subtitle = \"For wells with > 30 samples and data from at least 1980-01-01\",\n       caption = \"Larger (darker) values indicate groundwater depletion.\") +\n  theme_void()\n\n\n\n\nIt appears that larger groundwater level changes occur in the\ninterior of Sacramento County, and in the southern portions, both of\nwhich are further from urban areas. Smaller changes along the western\nboundary may result from groundwater recharge from surface water along\nthe Sacramento River.\nFinally, let’s examine the changes in groundwater level at the sites\nthat meet our constraints and add linear trendlines using\ngeom_smooth().\n\n\n# go back and grab gwl measurements at the specified site codes\ngwl_res_ir <- filter(gwl_filt, \n                     SITE_CODE %in% ids_time_samp)\n  \n# plot all groundwater levels and a linear trendline\ngwl_res_ir  %>% \n  ggplot(aes(MSMT_DATE, GSE_WSE)) +\n  geom_line(aes(group = SITE_CODE, color = WELL_DEPTH), alpha = 0.8) +\n  geom_smooth(method = \"lm\", color = \"orange\", se = FALSE, lwd = 2) +\n  facet_wrap(~WELL_USE) +\n  labs(x = \"\", y = \"Depth to groundwater (ft)\")\n\n\n\n\nThese trendlines suggest that observed depths to groundwater have\nincreased during the period of record at the long-term monitoring sites\nidentified by our selection criteria. These declines are likely due\ngroundwater pumping for urban expansion and irrigated agriculture.\nRemember also, that residential wells were substantially shallower than\nirrigation wells (around a 140 foot difference in median depth, see the\nlighter blue colors in the plot for deeper wells), so these groundwater\nlevel changes are likely impacting different aquifer systems.\nQuestion 4:\nRelating groundwater to CES scores\nFinally, let’s incorporate CES scores into our analysis and see how\nthey relate to groundwater level trends. First, let’s look at CES scores\nin Sacramento County on a basemap with {mapview}. Scores\nare assigned per Census Tract. Urban areas tend to have higher CES\nscores due to greater exposures across the range of variables that CES\nmeasures.\n\n\n# CES3 data for Sac County\nst_read(\"data/calenviroscreen/CES3_shp/CES3June2018Update.shp\") %>% \n  st_transform(st_crs(sac)) %>% \n  st_intersection(sac) %>% \n  write_rds(\"data/ces3_sac.rds\")\n\n\n\n\n\nlibrary(mapview)\nlibrary(colormap) # one of many color palette packages in R\nmapviewOptions(fgb = FALSE)\n\n# read pre-processed CES Score spatial file, plot the CES percentile\nces <- read_rds(\"data/ces3_sac.rds\")\n\n# calculate the population density per tract\nces$pop_density <- ces$pop2010 / ces$Shape_Area\n\n# view the data\nmapview(select(ces, CIscoreP), \n        zcol = \"CIscoreP\", \n        layer.name = \"CES Score\") +\n  mapview(select(ces, pop_density), \n          zcol = \"pop_density\", \n          col.regions = colormap(colormaps$magma, nshades = 100),\n          layer.name = \"Pop Density\")\n\n\n\n\nAs a first cut, let’s just look at the CES percentile (higher\nindicates a more negative outcome).\n\n\n# sacramento polygon\nmapview(sac, alpha.regions = 0, color = \"red\", \n        lwd = 2, layer.name = \"Sac Co\") +   \n  # select river name to only that in the table\n  mapview(select(riv, HYDNAME), color = \"blue\",\n          legend = FALSE) +\n  # select CES to only show CES score in table\n  mapview(select(gwl_minimal, `CES 3.0 Percentile`), \n          zcol = \"CES 3.0 Percentile\", \n          layer.name = \"CES score\")\n\n\n\n\nNext, we might be interested to examine the groundwater level decline\nper census tract at our long-term monitoring sites.\n\n\n# select minimal subset of data to join back to gwl\ngwl_diff_df <- st_drop_geometry(gwl_diff) %>% \n  select(SITE_CODE, diff)\n\n# join differenced data back to groundwater level\ngwl_ces <- gwl_filt %>% \n  filter(SITE_CODE %in% ids_time_samp) %>% # selection criteria\n  group_by(SITE_CODE) %>% # slice 1st row per group as CES scores duplicate\n  slice(1) %>% \n  ungroup() %>% \n  left_join(gwl_diff_df, by = \"SITE_CODE\")  # add groundwater level diff\n  \n\ngwl_ces %>% \n  ggplot(aes(diff, `CES 3.0 Percentile`)) +\n  geom_point() + \n  geom_smooth(method=\"lm\", se=FALSE)\n\n\n\n\nIt appears that if there is any trend at all, there’s a slight\nnegative relationship between CES and large groundwater withdrawal. This\nis likely because CES scores tend to be higher in and near urban areas,\nand most groundwater pumping takes place in rural areas away from urban\ndevelopment. To confirm, let’s inspect points with a difference in\ngroundwater level >= 25 feet.\n\n\nm1 <- filter(gwl_ces, diff >= 25) %>% \n  select(diff) %>% \n  mapview(zcol = \"diff\", layer.name = \"GWL <br>delcine (ft)\")\n\nm1\n\n\n\n\nIf we toggle the basemap to “Esri.WorldImagery” it’s clear that areas\nwith large groundwater declines are on the leading edges of expanding\nsuburban zones and in rural areas.\nLet’s now define a class of monitoring points called\nhigh_priority that is characterized by high CES scores and\nlarge groundwater declines.\n\n\n# add a new column \"high_priority\" if gwl change >= 25 feet and CES >= 50%\ngwl_ces <- gwl_ces %>% \n  mutate(high_priority = ifelse(diff >= 25 & `CES 3.0 Percentile` >= 50, TRUE, FALSE))\n\n# verify that our mutate worked\ngwl_ces %>% \n  ggplot(aes(diff, `CES 3.0 Percentile`, color = high_priority)) +\n  geom_point(size = 3, alpha = 0.9) +\n  scale_color_manual(name = \"High Priority\", values = c(\"grey\", \"red\"))+\n  theme_classic()\n\n\n\n# plot location priority\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = gwl_ces, aes(color = high_priority), size=3) +\n  scale_color_manual(\"High Priority\", values = c(\"grey\", \"red\")) +\n  theme_void()\n\n\n\n\nInspecting these locations in closer detail over a satellite basemap\n(“Esri.WorldImagery”) may give us clues into what’s happening\nhere.\n\n\nm2 <- filter(gwl_ces, high_priority == TRUE) %>% \n  select(diff, `CES 3.0 Percentile`) %>% \n  mapview(zcol = \"diff\", layer.name = \"GWL <br>decline (ft)\")\nm2\n\n\n\n\nCompared to our previous map, which only showed areas with\ngroundwater declines >= 25 feet over the historical record, the “high\npriority” groundwater monitoring sites shown here with high CES scores\n(i.e., they are associated with census tracts that are at higher risk of\nimpacts from environmental pollutants) appear to be located at or near\nthe suburban fringe.\n\nPause and think\nDid this EDA spark any questions for you? What questions would you\nlike to explore with these data given what you’ve seen?\n\nLearn more\nEDA is the synthesis of every module we’ve covered so far, and many\nmore that are beyond the scope of this course. As you learn more data\ntransformation, visualization, and modeling skills, the depth of your\nEDA capabilities will increase. A good place to pick up general R\nknowledge and practice new skills is to walk through a textbook like R for Data Science, which will cover\nthe basics and give you a good foundation on which to stand.\nCommunicate results\nAn EDA may generate tables, visualizations, text, and code that all\nencapsulate the greater meaning that you’ve derived from the data. An\nexcellent, R-centric way to share the tables, visualizations, text, and\ncode that result from your EDA is to use an RMarkdown\n(.Rmd) document, which is the topic of the next module.\n\n\nPrevious\nmodule: 11. Spatial Data\nNext\nmodule: 13. RMarkdown\n\nDWR Periodic\nGroundwater level database.↩︎\nOEHHA CalEnviroscreen\n3.0 data.↩︎\nStatistical modeling is beyond the scope of this module,\nbut if you are interested, you can read more about statistical modeling\nin R here.↩︎\nThis is real world data, and real world data is messy.\nDepths to groundwater > 250 happen all the time, but in this plot, we\nsee that these values suddenly spike by 300 to 400 feet. It’s safe to\nassume that this amount of variance is physically impossible and likely\ndue to errors in data entry. We can zoom into these data and filter them\nout, but the plot also tells us that we can remove these few values with\na simple filter() at the 250 ft threshold.↩︎\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_functions.html",
      "title": "9. Function Junction",
      "description": "Use, re-use, and write code to do stuff!\n",
      "author": [],
      "contents": "\n\nContents\nFunctions\nWhat’s in a Function?\nRunning a Function\nNaming Functions in R\n\nUse a Function from a Package\nGetting Water Data with {dataRetrieval}\nHow do we know it worked?\n\nUsing Custom Functions\nConvert Flow Data to cms\n\nMore Practice\nGetting Water Quality Stations\nSave Our Data!\n\nAdditional Water Packages of Interest\n\n\nFunctions\nUnderstand what a function is\nUnderstand how to use functions in R\nWrite your own simple function\n\nFunctions\nWhat is a function? A function is simply a set of instructions or code written for a computer to follow. These can be very simple (multiply one column of data by another) to much more complex (multi-step model and visualization). Packages are usually sets of functions that focus on similar topics or themes, sort of like a cookbook for a specific kind of food.\nFunctions ultimately make our code more reusable and reproducible, because we can use functions for repeated processes (like cleaning and tidying data!).\nWhat’s in a Function?\nIn R, functions have a name, an input which may consist of one or more arguments (sometimes called parameters) and a output or the thing that the function returns.\nHere’s an example function to convert discharge from cubic meters per second (cms) to cubic feet per second (cfs). Can you identify the different parts?\n\n\ncms_to_cfs <- function(discharge) {\n  cfs_out <- discharge * 35.3146662\n  return(cfs_out)\n}\n\n\n\nRunning a Function\nUsing functions requires we know what pieces go where inside the function (the arguments), our data is of the right class or shape for the function (input), and we know what the function should return to our Environment! Once we have this basic understanding, there are thousands of functions that are already written and part of packages we can use. In fact, we’ve been using functions in every lesson thus far. In this module, we focus on building our own functions to meet our data science needs.\nTo use the cms_to_cfs function we wrote above, we need to provide a value to the discharge argument. Let’s convert 10 cms to cfs, using our function.\n\n\ncms_to_cfs(discharge = 10)\n\n\n[1] 353.1467\n\nThe function returns a numeric value to our console. If we wanted to save this value, we could assign it (<-) to an object. Functions allow us to iterate over data, or repeat the same task on different datasets without needing to re-write code every time.\nNaming Functions in R\nIt’s important to adopt consistent and descriptive names in R. This isn’t just for functions, but it’s a particularly good habit to practice when naming functions. For example, a general recommendation is to 🐍 use_snake_case 🐍. Other options include:\nMaybeUseCamelCase\nor.separate.by.periods\norJust_try.everythingBecause.CHAOS\n\nFor more useful info on naming conventions see r4ds or example styles.\nUse a Function from a Package\nWe’ve already been doing this throughout the previous lessons with things like read.csv(), dplyr::filter(), or ggplot2::ggplot(), but let’s use a new package to pull some surface water river flow data. Here we’ll use the {dataRetrieval} package.\nGetting Water Data with {dataRetrieval}\nThe {dataRetrieval} package is a way to access hydrologic and water quality data from the USGS and EPA. There are several vignettes (or tutorials) available on the package website. Here, we will show how to download some surface water data from the American River near Sacramento. First we load the package with the functions we want to use. We load packages once per session.\n\n\n# load the functions via packages!\nlibrary(tidyverse)\nlibrary(dataRetrieval) # a spellbook of water data functions\n\n# use a function to download data:\nflow_site <- readNWISdata(site = \"11427000\", # USGS Station ID\n                          service = \"iv\", # instantaneous measurements\n                          parameterCd = \"00060\", # discharge in cfs\n                          startDate = \"2019-10-01\",\n                          endDate = \"2021-09-30\",\n                          tz = \"America/Los_Angeles\")\n\n\n\nThere are a lot of arguments in that function! How did we know what each one was or what to put there? There are a few different options. For functions or packages you aren’t familiar with, it’s worth using built-in help.\n\nThe troubleshooting module has a section on how to read R’s built-in help.\nLook up help for a function with ?readNWISdata and look at the Usage and Arguments sections!\nUse tab autocomplete as much as you can in RStudio! Once inside a function’s parenthesis, hit tab! There are generally a list of arguments that will pop up, each with some info about them. This is the same info we should find in the Help tab when we look up the function.\nHow do we know it worked?\nAlways visualize and inspect your data… we should have an object in our environment called flow_site. Let’s take a look at this data, tidy it, and plot!\n\n\n# check data\nsummary(flow_site)\n\n\n  agency_cd           site_no             dateTime                  \n Length:70080       Length:70080       Min.   :2019-10-01 00:00:00  \n Class :character   Class :character   1st Qu.:2020-03-31 13:26:15  \n Mode  :character   Mode  :character   Median :2020-09-30 13:07:30  \n                                       Mean   :2020-09-30 12:39:55  \n                                       3rd Qu.:2021-04-01 11:48:45  \n                                       Max.   :2021-09-30 23:45:00  \n X_00060_00000     X_00060_00000_cd      tz_cd          \n Min.   :   9.68   Length:70080       Length:70080      \n 1st Qu.:  55.50   Class :character   Class :character  \n Median : 115.00   Mode  :character   Mode  :character  \n Mean   : 323.72                                        \n 3rd Qu.: 411.00                                        \n Max.   :4580.00                                        \n\nhead(flow_site)\n\n\n  agency_cd  site_no            dateTime X_00060_00000\n1      USGS 11427000 2019-10-01 00:00:00           101\n2      USGS 11427000 2019-10-01 00:15:00           106\n3      USGS 11427000 2019-10-01 00:30:00           106\n4      USGS 11427000 2019-10-01 00:45:00           106\n5      USGS 11427000 2019-10-01 01:00:00           106\n6      USGS 11427000 2019-10-01 01:15:00           106\n  X_00060_00000_cd               tz_cd\n1                A America/Los_Angeles\n2                A America/Los_Angeles\n3                A America/Los_Angeles\n4                A America/Los_Angeles\n5                A America/Los_Angeles\n6                A America/Los_Angeles\n\n# fix column names so we can use a more informative parameter name\nflow_site <- dataRetrieval::renameNWISColumns(flow_site)\n\n# check data\nnames(flow_site)\n\n\n[1] \"agency_cd\"    \"site_no\"      \"dateTime\"     \"Flow_Inst\"   \n[5] \"Flow_Inst_cd\" \"tz_cd\"       \n\n# visualize:\nggplot(data = flow_site) + \n  geom_line(aes(x = dateTime, y = Flow_Inst), color = \"darkblue\") +\n  theme_classic() +\n  labs(x = \"\", y = \"Flow (cfs)\", \n       title = \"Discharge from USGS Station 11427000\",\n       caption = \"data pulled using {dataRetrieval} package in R\")\n\n\n\n\nAwesome! We plotted some discharge data from a river. That’s great. We could save this data if we wanted (see the import/export module), or do additional analysis (see next module on joins!). Let’s try using a different function here.\n\nChallenge 1: You Try!\nUsing the code template below, create a flow_huc object using the readNWISdata() function but fill in the arguments with the following information:\nInstead of site, let’s use huc. We’ll try getting stations from HUC 18020111.\nInstead of iv (instantaneous), let’s use dv.\nUse the same startDate, endDate, and parameterCd as we used above.\n%>% the output from readNWISdata() directly to the renameNWISColumns() function and then to the addWaterYear() function.\n\nHow many unique site_no are there?\nCan you visualize the data using a ggplot() with geom_line and use a different color for each site_no?\nCode template:\nflow_huc <- readNWISdata(___,\n                         ___,\n                         parameterCd = \"00060\", \n                         startDate = \"2019-10-01\",\n                         endDate = \"2021-09-30\") %>% \n  ___ %>% \n  ___\n  \n\nClick for Answers!\n\n\n\n# look for stations with daily values of discharge in the whole watershed\nflow_huc <- readNWISdata(huc = \"18020111\", # using a huc id number now\n                         service = \"dv\", # daily values\n                         parameterCd = \"00060\", \n                         startDate = \"2019-10-01\",\n                         endDate = \"2021-09-30\") %>% \n  renameNWISColumns() %>%  # the pipe infers that the data is passed through\n  addWaterYear()\n\nnames(flow_huc) # names look ok!\n\n\n[1] \"agency_cd\" \"site_no\"   \"dateTime\"  \"waterYear\" \"Flow\"     \n[6] \"Flow_cd\"   \"tz_cd\"    \n\nunique(flow_huc$site_no) # 2 stations\n\n\n[1] \"11446500\" \"11447360\"\n\n# visualize:\nggplot() + \n  geom_line(data = flow_huc, \n            aes(x = dateTime, y = Flow, color = site_no), \n            show.legend = FALSE) +\n  theme_classic() +\n  labs(x = \"\", y = \"Flow (cfs)\", \n       title = \"Discharge from USGS Stations: 11446500, 11447360\",\n       caption = \"data pulled using {dataRetrieval} package in R\") +\n  # a new way to view data with facets...add a free scale!\n  facet_grid(rows = vars(site_no), scales = \"free\") \n\n\n\n\n\nUsing Custom Functions\nSince we already wrote a custom function to convert cms to cfs, let’s make another similar function but for cfs to cms, and apply it to the flow data we just downloaded. Let’s look at how we might use a function on an existing data frame or dataset that we have in R.\nConvert Flow Data to cms\nLet’s use some of the dplyr we looked at in the last lesson. Here we want to make a new column that is our Flow data, but instead of in cfs, our units will be in cms. Let’s use the flow_site data frame.\n\n\n# the function\ncfs_to_cms <- function(discharge) {\n  cms_out <- discharge * 0.028316847\n  return(cms_out)\n}\n\n# add a new column to existing dataset with mutate\nflow_site <- flow_site %>% \n  mutate(Flow_cms = cfs_to_cms(Flow_Inst)) # apply the function to the column\n\n# quick plot to check!\nggplot() + \n  geom_line(data = flow_site, \n            aes(x = dateTime, y = Flow_cms), color = \"darkblue\") +\n  theme_classic() +\n  labs(x = \"\", y = \"Flow (cms)\", \n       title = \"Discharge from USGS Station 11427000\",\n       caption = \"data pulled using {dataRetrieval} package in R\")\n\n\n\n\nSo what happened here? We made a function cfs_to_cms() to apply to our data. In this case, we wanted to apply our function to a column in our dataframe, which if we remember the earlier lesson on data, in a dataframe a column is a vector of the same type of data. That means the function can very quickly iterate through every observation in the vector and convert the data from cfs to cms.\nMore Practice\nThis section uses some more functions, but with some more advanced applications. We’ll want to have the {sf} package installed for the following code to work fully.\nGetting Water Quality Stations\nWhat if we wanted to find all the possible water quality stations that are upstream or downstream of the gage we pulled data from earlier (USGS: 11446500)? A great function that will not only help us figure that out, but also help us pull in spatial data we can use to make maps is the findNLDI() function from the {dataRetrieval} package.\nThe function findNLDI() has arguments for the type of data we want to use (nwis, huc,location), the direction along a stream to look (upstream or downstream, or in mainstem or tributaries), the data we want to get back (find), and the distance we should search upstream or downstream (distance_km).\nLet’s look only in the mainstem river for 50 km upstream or downstream of our USGS gage and return any NWIS (water quality or flow) sites and the associated flowlines we searched.\n\n\nlibrary(sf)\n\n# get NWIS and flowline data 50km downstream of the USGS gage\namer_ds_nimbus <- findNLDI(\n  nwis = \"11446500\", # Below Nimbus Dam\n  nav = c(\"UM\",\"DM\"), \n  find = c(\"nwis\", \"flowlines\"),\n  distance_km = 50)\n\nnames(amer_ds_nimbus)\n\n\n[1] \"origin\"       \"UM_nwissite\"  \"UM_flowlines\" \"DM_nwissite\" \n[5] \"DM_flowlines\"\n\nNote, this returned a named list of dataframes. We can access individual parts of a list just as we can access a single column in a dataframe using the $. This function returned 5 parts, the origin which is the site we used to start our search, the UM_nwissite which is the NWIS sites upstream on the mainstem from our USGS gage, the DM_nwissite which is the NWIS sites downstream of our USGS gage, and finally, a mainstem flowline upstream and downstream of our gage. These are dataframes with an underlying spatial component, which is nice because it allows us to easily map and visualize these data if we want. See the mapmaking lesson for more info.The dataframes are an sf class, from the simple features {sf} package, but they can also be converted to into a dataframe.\n\n\n# check the object class \nclass(amer_ds_nimbus$DM_nwissite)\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# get just downstream NWIS sites and make into dataframe\nnwis_sites <- amer_ds_nimbus$DM_nwissite %>% \n  st_drop_geometry() # drop the sf class\n\n# verify the class has changed\nclass(nwis_sites)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nLet’s visualize these sites with a fancy map! See the mapmaking lesson for more details on making maps in R.\n\n\n\nSave Our Data!\nFinally, let’s go ahead and save (export) our list of NWIS sites, and all our spatial components from the American River below Nimbus Dam so we can use it for future lessons.\n\n\n# save out\nwrite_csv(nwis_sites, path = here(\"data\", \"nwis_sites_american_river.csv\"))\nwrite_rds(amer_ds_nimbus, path = here(\"data\", \"nwis_sites_amer_ds_nimbus_sf.rds\"))\n\n\n\nAdditional Water Packages of Interest\nThere are many packages that work with water or water data. Here’s a list of just a few that you may find interesting or helpful. The R CRAN repository has a list of all packages that relate to Hydrology that is also a good place to look.\n{dataRetrieval}\n{nhdplusTools}\n{EGRET}\n{geoknife}\n\n\nPrevious module: 8. Spreadsheets & Pivots Next module: 10. Joins and Binds\n\n\n\n",
      "last_modified": "2022-05-11T22:12:53-07:00"
    },
    {
      "path": "m_getting_started.html",
      "title": "2. Getting Started with R for Water Resources Data Science",
      "description": "Unleash your potential to work with any kind of data...\n",
      "author": [],
      "contents": "\n\nContents\nWelcome!\nWhy R? Why Data Science?\n\nCourse Objectives\nWhy should I invest time in learning R?\n\nR/RStudio Fundamentals\nExecuting code in RStudio (Console)\nStoring Variables in the Environment\nInstalling Packages (Files etc tab)\nCRAN\nInstalling {tidyverse}\nRemaining Tabs in the Files etc Pane\n\nGetting Help\nHelp from the Console\nRStudio Cheatsheets\n\n\n\nWelcome!\nThe goals of this training are to expose you to fundamentals and to develop an appreciation of what is possible with R. Importantly, we hope to instill the idea that data science should be transferable and reproducible, no matter the specific tools (i.e., R) or the dataset used. Reproducibility is a framework and mindset. An introductory course will not make you an expert, but hopefully it will draw you in, build excitement about using reproducible approaches to your own work, and show you how to be more efficient and less stressed when faced with messy real-world data!\nWe hope you will find this workshop both fun and helpful, and we appreciate your patience as we continue to develop this course, both virtually and elsewhere! We would also like to thank Allison Horst for allowing us to use her incredible monsteRs and R illustrations that you will see included throughout this course.\nWhy R? Why Data Science?\nComputer literacy is essential in all aspects of the working world. The ability to reproduce a set of common or repetitive tasks efficiently and reliably is the root goal of reproducibility. Whether in science, management, or other fields, the skills required to increase productivity, maintain transparency, and feel competent and able to tackle multiple tasks all boil down to data science and project management tools.\nData management skills are needed for entering data without errors, storing it in a usable way, and extracting key aspects of the data for analysis. Basic programming is required for everything from accessing and managing data, data visualization, to statistical analysis and modeling. Data science combines the skills and tools that allow for acquisition, processing, analyzing, communicating, and maintenance data of any type.\nCourse Objectives\nThis course’s main objective is to provide students with a introduction to the power and possibility that an open source (free) and reproducible programming language such as R provides by demonstrating how to import, explore, visualize, and communicate different types of data. Using water resources based examples, this course will guide students through basic data science skills and strategies for continued learning and use of R. R is a language for statistical computing and a general purpose programming language. It is one of the primary languages used in data science and for data analysis across many of the natural sciences. This course will provide lessons in:\nData and file management: understanding RProjects and file paths\nUnderstanding and identifying different data formats (i.e., wide, long, tidy) and data structures (i.e., vectors, dataframes, lists)\nImporting and exporting various water resources data\nStrategies for Exploratory Data Analysis (EDA)\nStrategies for troubleshooting (reading documentation, reproducible examples)\nTransforming data with {dplyr}\nData visualization with {ggplot2}\nData presentation and communication with RMarkdown\nWhy should I invest time in learning R?\nThere are many programming languages available and each has specific benefits. R was originally created as a statistical programming language but now is largely viewed as a ‘data science’ language. R is also an open-source programming language - not only is it free, but this means anybody can contribute to its development. Furthermore, R has powerful and nearly limitless plotting/visualization functionality, and you can adjust nearly any aspect of a graph to communicate your data effectively.\nAs Hadley Wickham, a prominent R developer, states:\n\n“R is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, R is a much more flexible language than many of its peers… it helps you think about problems as a data scientist, while supporting fluent interaction between your brain and the computer. (R4DS)”\n\n\n\n\nFigure 1: R is hard at first…but it gets easier! Artwork by @allison_horst\n\n\n\nR/RStudio Fundamentals\nIn the old days, the only way to use R was directly from the Console - this is a bare bones way of running R only with direct input of commands. Now, RStudio is the go-to Interactive Development Environment (IDE) for R. Think of it like a car that is built around an engine. RStudio is built around the R Console (engine) and includes many other features to improve the user’s experience.\nLet’s get familiar with RStudio. If you haven’t done so already, download and install RStudio from the link above for the most recent version. After it’s installed, find the RStudio shortcut and fire it up. You should see something like this:\n\n\n\nFigure 2: The RStudio window.\n\n\n\nThere’s a great cheatsheet on using the RStudio IDE here\nThere are four panes in RStudio (starting from the top right and moving clockwise):\nSource\nEnvironment, History, etc.\nFiles, Plots, etc.\nConsole\nExecuting code in RStudio (Console)\nThe first part of RStudio that we will work in is called the Console, which tells you what code R is running. We can use many of the same commands found in a calculator or Microsoft Excel.\nType the following text into the console at the line that ends with >, press Enter, and you should see the following results:\n\n\n# enter in console (after the \">\" mark)\n8 / 4\n\n[1] 2\n\n\n\n# enter in console (after the \">\" mark)\n4 + 8\n\n[1] 12\n\nStoring Variables in the Environment\nYou can also create variables with custom values (we’ll talk much more about this later). But here are the basics, the first part of the code is a name of your choosing. Meaningful variable names are better, but the only rules are: 1) that it can’t start with a number and 2) it must not have any spaces. The second bit, <-, is the assignment operator. This tells R to take the result of whatever is on the right side of the <- and save it as a new object in your R Environment.\nType the following into your Console and press enter after each one to see their output:\n\n\n# assign the value 4 to the variable name \"stream\"\nstream <- 4\n\n\n\n\n# assign the value 8 to the variable name \"pebble\"\npebble <- 8\n\n\nYou might notice is no output in the Console for the lines of code you have just run. Instead, they have been stored in your R Environment in the top right pane of your RStudio window. Click on that tab and take a look! Because they have been stored, you can print these variables by typing their name in the Console and pressing Enter.\nGenerally there are two possible outcomes when you run code. First, the code will simply print output directly in the console, as it did with the calculations you entered above. Second, there is no output because you have stored it as a variable in the Environment. The Environment is the collection of named objects that are stored in memory for your current R session. Anything stored in memory will be accessible by the variable name without running the original script that was used to create it.\nAdd the variables you just created together, and examine the output:\n\n\nstream + pebble\n\n[1] 12\n\nYou can also create new variables using existing variables like so:\n\n\nhabitat <- stream + pebble\n\n\nKeep in mind R is case-sensitive! Details like spacing and spelling matter in coding (the computer will do only exactly what you say, nothing more, nothing less), so if we use Habitat or habitatt or ha_bitat hoping to get the value of habitat, we’re out of luck.\nPlease note: Clicking on the broom button in the Environment will permanently clear out your existing variables. Only do this if you are certain you want to remove/reset all saved variables and datasets.\nIn this same pane in the RStudio window is the History tab, which will record all the code you’ve run, and the Connections tab will show connections to other databases, etc.\nInstalling Packages (Files etc tab)\nImmediately below the Environment is the third section of RStudio, where all of your Packages are stored. The base or core installation of R is quite powerful, but because R is open-source, there are thousands of packages (pieces of code we can download and use) that dramatically extend the capability of R, from statistical analysis, to modeling, to geospatial mapping, to website and document creation. Packages are the collection of code or functions that are a standardized way of extending R with new methods, techniques, and programming functionality.\nCRAN\nOne of the reasons for R’s popularity is CRAN, The Comprehensive R Archive Network. CRAN is where you download R and also where you can gain access to additional packages. All of the packages we will use during this tutorial will be downloaded from CRAN. As of 2024-01-18, there are 20,282 packages on CRAN!\nInstalling {tidyverse}\nWhen a package is installed, that means the source code is downloaded and put into your library. Let’s give it a shot using the {tidyverse}, a set of packages assembled for data tidying and visualization purposes.\nWe’re going to use our very first function: install.packages() to install this package. Type the following into your Console and press enter:\ninstall.packages(\"tidyverse\")\nYou should see it appear in the Packages tab. To find it, you can either scroll through the list, or type the package name into the search bar at the top of the pane.\nIn order to use a package, you must load it into your current workspace. This is sometimes called attaching a package, but we prefer to avoid clicking whenever possible, so we’ll load this package using the library() function. Type the following into your Console and press Enter:\nlibrary(tidyverse)\nNow your package is loaded, and ready to use. You can be certain your package is attached if there is a check mark next to the package name in the Packages tab.\nGeneral rule of thumb: Load packages once per session, install packages once per computer or R version!\nRemaining Tabs in the Files etc Pane\nThe remaining tabs in this pane allow you to see:\nFiles in your computer,\nPlots you’ve created,\nHelp documents for when you get stuck,\nViewer view additional HTML content you might create\nGreat job! You’ve opened up RStudio, learned some of the basic functionality, and now we’re ready to get going on your first R project!\n\nGetting Help\nBeing able to find help and interpret that help is probably one of the most important skills for learning a new language. We have a whole lesson devoted to help and troubleshooting, but here’s a quick overview for “local” help that is already built into R and RStudio. Help on functions and packages can be accessed directly from R.\nHelp from the Console\nGetting help from the Console is straightforward and can be done numerous ways. If you know the name of the function or package we can type that as follows:\n\n\n# Using the help command/shortcut\n# When you know the name of a function\nhelp(\"print\") # Help on the print command\n?print # Help on the print command using the `?` shortcut\n\n# When you know the name of the package\nhelp(package = \"dplyr\") # Help on the package `dplyr`\n\n\nIf we don’t know the name, or maybe just want to look for a part of something, we can use the following in the Console:\n\n\n# Don't know the exact name or just part of it?\n\napropos(\"print\") # Returns all available functions with \"print\" in the name\n\n??print # shortcut, but also searches demos and vignettes in a formatted page\n\n\nRStudio Cheatsheets\nAs we move deeper into learning how to use RStudio and various packages within R, there are some handy cheatsheets that can be helpful as a broad overview or quick references. Check them out!\n\n\nPrevious module: 1. Install R/RStudio\nNext module: 3. Project Management\n\n\n\n",
      "last_modified": "2024-01-21T12:17:11-08:00"
    },
    {
      "path": "m_ggplot.html",
      "title": "5. Data visualization with ggplot2",
      "description": "The grammar of graphics.\n",
      "author": [],
      "contents": "\n\nContents\nData visualization with {ggplot2}\nExamples\nCreating your first ggplot\nAdding geom_ layers\nAesthetics\nFaceting\nSaving plots\nSaving with a graphical device\nSaving with ggsave()\n\nColorblindness\nBespoke plots\nExtending ggplot2\n\n\nLearning objectives\nLearn how to create beautiful and informative plots from data\nUnderstand the grammar of graphics implemented in {ggplot2} and how to create plots in “layers”\nLearn core {ggplot2} geometries like geom_point(), geom_line(), geom_col(), geom_boxplot(), geom_histogram()\nUnderstand how to customize and save plots\n\nData visualization with {ggplot2}\nData visualization allows us to effectively explore data in an Exploratory Data Analysis (EDA), and allows us to communicate the results of an analysis or modeling exercise. A powerful package for data visualization in R is {ggplot2} (part of the {tidyverse} set of packages). {ggplot2} stands for “grammar of graphics plot” and generally provides a semantic syntax for data visualization that breaks down graphs in terms of aesthetics, geoms, coordinates, scales, and more, which allows users to create visualizations that range from simple to extraordinarily complex. The act of creating visualizations is not only useful, but also fun and motivating, so we cover it early, before more advanced data manipulation in subsequent modules. By the end of this module you will be able to create beautiful ggplots made entirely in R, and take your first steps towards reproducible and elegant data visualization.\n\n\n\n\nFigure 1: Illustration by @allison_horst.\n\n\n\nExamples\nBefore we begin, below are a few plots created entirely with {ggplot2} to illustrate the possibilities for data visualization enabled by this package.\nThese are only a few examples of the many possibilities that can be made. Because ggplots are so customizable, some people even use {ggplot2} to create artwork1. Once you are familiar with how to customize a ggplot, you may be surprised to see them appear in major news outlets and scientific publications.\n{ggplot2} art by Antonio Sánchez Chinchón @aschinchon.\n\n\n\n\n\n\nFigure 2: Evolution of a ggplot, by Cedric Scherer shows how one can progressively refine and customize a gpplot.\n\n\n\n\n\n\n\nFigure 3: Created by Timo Grossenbacher @grssnbchr.\n\n\n\n\n\n\n\nFigure 4: Travelling to Outer Space by Cédric Scherer @CedScherer.\n\n\n\nCreating your first ggplot\nTo begin, we load the {ggplot2} package. We can load it independently, but let’s load it along with the {tidyverse} suite of R packages used in this course. We also need to load some data. Below we load a pre-processed dataframe that we’ll create later in the module on joins and binds.\n\n\n# includes ggplot2\nlibrary(tidyverse)\n\n# groundwater level from a single station in Sacramento County\ngwl <- read_csv(\"data/gwl/gwl.csv\")\n\n# inspect the data\nhead(gwl)\n\n# A tibble: 6 × 10\n  SITE_CODE   MSMT_DATE    WSE GSE_WSE WLM_ORG_NAME LATITUDE LONGITUDE\n  <chr>       <date>     <dbl>   <dbl> <chr>           <dbl>     <dbl>\n1 384121N121… 2020-07-02 -25.2    159. Southeast S…     38.4     -121.\n2 384121N121… 2020-06-26 -24.8    159. Southeast S…     38.4     -121.\n3 384121N121… 2020-06-19 -25.4    159. Southeast S…     38.4     -121.\n4 384121N121… 2020-06-12 -25.2    159. Southeast S…     38.4     -121.\n5 384121N121… 2020-04-17 -21.1    155. Southeast S…     38.4     -121.\n6 384121N121… 2020-03-20 -21.8    156. Southeast S…     38.4     -121.\n# ℹ 3 more variables: COUNTY_NAME <chr>, WELL_DEPTH <dbl>,\n#   WELL_USE <chr>\n\nThe 10 columns in the data contain are a SITE_CODE or unique identifier, a date-time MSMT_DATE, a water surface elevation WSE_FT, and a depth to groundwater GSE_WSE, the agency that collected the measurement WLM_ORG_NAME, a latitude and longitude, the county, the well depth, and the well use.\nPerhaps we want to visualize the depth to groundwater at this site over time. To do this, we put MSMT_DATE on the x-axis and GSE_WSE on the y-axis.\nTo create a ggplot we start with the function ggplot().\n\n\nggplot()\n\n\n\nThe above plot appears blank because we haven’t added any layers to it! A ggplot is nothing until we layer on one or more geometries or geoms.\nAdding geom_ layers\n\nggplot can also create maps with geom_sf(), covered in the mapmaking module.\nLet’s add a geom_line() layer to the ggplot() we created above.\n\n\nggplot(data = gwl) +\n  geom_line(mapping = aes(x = MSMT_DATE, y = GSE_WSE))\n\n\n\nLet’s break down what we just did:\n\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\n\nSee a full list of available geoms on the {ggplot2} online documentation.\nWe start creating a plot with the ggplot() function with our data (data = gwl). With {ggplot2}, each layer or piece of the ggplot is added by using a +. We added a geom_line() which connects data points with a line, but there are many other geoms, including geom_point(), geom_col(), geom_boxplot(), geom_histogram(), and so on. Inside each geom_, there are many options, but the core arguments that allow us to tell the geom_line() function to put the date on the x-axis and depth to groundwater on the y-axis are arguments data and mapping . For the data, we are using the data specified in the ggplot(data = gwl) line, which means that data is available for any future layer or geom we use. To specify what pieces go on the x-axis and y-axis, we use the mapped aesthetics, mapping = aes(x = MSMT_DATE, y = GSE_WSE).\nIn practice, we don’t need to explicitly write out all of the full argument names. {ggplot2} understands that mapping aes(MSMT_DATE, GSE_WSE) means that the measurement date and depth to groundwater belong on the x and y axis respectively. However, it is always a good idea to explicitly specify the data argument no matter what to avoid potential errors.\n\nIf using more than one dataframe, you can specify data in each geom function, rather than in ggplot().\n\n\nggplot(data = gwl) +\n  geom_line(aes(MSMT_DATE, GSE_WSE))\n\n\nWe can change the geom to a point:\n\n\nggplot(data = gwl) +\n  geom_point(aes(MSMT_DATE, GSE_WSE))\n\n\n\nMany points overlap, making them hard to pick apart. Let’s make them 50% opaque with the alpha argument.\n\n\nggplot(data = gwl) +\n  geom_point(aes(MSMT_DATE, GSE_WSE), alpha = 0.5)\n\n\n\nWe can change the geom to an area (notice that the y axis scale now has a minimum of 0):\n\n\nggplot(data = gwl) +\n  geom_area(aes(MSMT_DATE, GSE_WSE))\n\n\n\nWe can add more than one geom to the plot (just link them with the +), for instance a point and line. We can also set the color of a geom with the color argument.\n\n\nggplot(data = gwl) +\n  geom_point(aes(MSMT_DATE, GSE_WSE), alpha = 0.5) +\n  geom_line(aes(MSMT_DATE, GSE_WSE), color = \"red\", alpha = 0.7)\n\n\n\nIt’s often useful to summarize large datasets using summary values like the median, mean, interquartile range, and so on. What does the distribution of depths to groundwater look like at this particular well?\n\n\nggplot() + \n  geom_histogram(data = gwl, aes(x = GSE_WSE))\n\n\n\nNotice that geom_histogram() takes only one argument for the x-axis and computes the bins for the y-axis. It appears that depths to groundwater range from around 130 to 160, with a median around 145. The default number of bins used is 30, but we can change this value.\n\n\nggplot(data = gwl) + \n  geom_histogram(aes(x = GSE_WSE), bins = 100)\n\n\n\n\nChallenge 1: You Try!\nModify the plot above to make the histogram blue (Hint: use fill = \"blue\").\nIn the code above, change geom_histogram() to geom_boxplot and remove any bins and fill arguments.\n\n\nClick for Answers!\n\n\n\nggplot(data = gwl) + \n  geom_histogram(aes(GSE_WSE), bins = 100, fill = \"blue\")\n\n\n\n\n\nggplot(data = gwl) + \n  geom_boxplot(aes(GSE_WSE))\n\n\n\n\nAesthetics\nAesthetics, or aes() as we have seen, are how geoms map variables onto a plot. So far, we have only used the x and y aesthetics to map variables onto an x and y coordinate system. We can also add a third variable, like color to the aesthetics, and map values in the data to different colors.\n\nOther aesthetics you can map variables to include shape, size, and fill.\nSo far, we’ve been working with one monitoring site in Sacramento County, but to illustrate how mapping a variable to color can be useful, let’s read in a slightly larger version of this groundwater level dataset that includes data from 10 monitoring sites in Sacramento and Placer counties.\n\n\n# groundwater level from 10 monitoring sites in Sacramento and Placer counties\ngwl_10 <- read_csv(\"data/gwl/gwl_10.csv\")\n\n# plot\nggplot(data = gwl_10) +\n  geom_point(aes(MSMT_DATE, GSE_WSE))\n\n\n\nPause for a moment to consider why does this data does not appear to have a clear trend.\nLet’s verify there are 10 unique sites in the data, then re-plot, and color the points by the SITE_CODE by assigning the variable SITE_CODE to the color argument inside the aes().\n\n\n# unique site codes in the dataframe\nunique(gwl_10$SITE_CODE)\n\n [1] \"382913N1213131W001\" \"384082N1213845W001\" \"385567N1214751W001\"\n [4] \"386016N1213761W001\" \"387511N1213389W001\" \"388974N1213665W001\"\n [7] \"383264N1213191W001\" \"382548N1212908W001\" \"388943N1214335W001\"\n[10] \"384121N1212102W001\"\n\n# re-plot with color as an aesthetic\nggplot(data = gwl_10) +\n  geom_point(aes(x = MSMT_DATE, y = GSE_WSE, color = SITE_CODE), alpha = 0.5)\n\n\n\nIt’s now clear now which points belong to which group. Above, we mapped colored points by a categorical variable (SITE_CODE), but we can also color by a continuous variable, like the well depth.\n\n\n# color the continuous well depth variable\nggplot(data = gwl_10) +\n  geom_point(aes(x = MSMT_DATE, y = GSE_WSE, color = WELL_DEPTH))\n\n\n\nWhat if we were curious to know the distribution of depth to groundwater (y axis) at each of the 10 sites (x axis) in the gwl_10 dataset?\n\n\nggplot(data = gwl_10) +\n  geom_boxplot(aes(x = SITE_CODE, y = GSE_WSE))\n\n\n\nThose x axis labels are hard to read. What happens if we switch the x and y aesthetics?\n\n\nggplot(data = gwl_10) +\n  geom_boxplot(aes(x = GSE_WSE, y = SITE_CODE))\n\n\n\nThat’s easier to read. Let’s also add some more intuitive labels.\n\n\nggplot(data = gwl_10) +\n  geom_boxplot(aes(x = GSE_WSE, y = SITE_CODE, color = WELL_USE)) +\n  labs(y = \"\", \n       x = \"Depth to groundwater (ft)\",\n       color = \"Well type\",\n       title = \"Depth to groundwater at 10 monitoring sites\",\n       subtitle = \"Sacramento and Placer county (1960-present)\",\n       caption = \"Source: Periodic groundwater level database, CA-DWR.\")\n\n\n\nFaceting\nFaceting is a powerful way to split a plot by a categorical variable into many subplots, or facets.\n\n\nggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE)) +\n  facet_wrap(~SITE_CODE)\n\n\n\nWe can improve the plot above by noticing that there are 10 facets, which would fit well into a grid of 5 rows and 2 columns, and also if we “freed” the scales so that they didn’t all have the same x and y axis limits. We can achieve these changes by modifying the nrow, ncol, and scales arguments.\n\n\nggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE)) +\n  facet_wrap(~SITE_CODE, ncol = 2, scales = \"free\")\n\n\n\nYou can also facet by 2 variables with facet_grid(). Here we facet by the county name and well use, and also separate individual sites within each facet by specifying group = SITE_CODE.\n\n\nggplot(gwl_10) +\n    geom_line(aes(MSMT_DATE, GSE_WSE, color = WELL_USE, group = SITE_CODE)) +\n    facet_grid(COUNTY_NAME~WELL_USE, scales = \"free\")\n\n\n\nWhat would happen if group = SITE_CODE were not included?\n\nChallenge 2: You Try!\nModify the plot above to color by the well use. (Hint: add color = WELL_USE inside the aes() function).\nUsing the gwl_10 dataset, create a new ggplot. Use geom_line() to map MSMT_DATE to the x-axis and GSE_WSE to the y-axis. Then, color by the SITE_CODE, and facet by the WELL_USE.\n\n\nClick for Answers!\n\n\n\nggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = WELL_USE)) +\n  facet_wrap(~SITE_CODE, ncol = 2, scales = \"free\")\n\n\n\n\n\n# color the continuous well depth variable\nggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = SITE_CODE)) + \n  facet_wrap(~WELL_USE, ncol = 1)\n\n\n\n\nSaving plots\nThere are two main ways to get plots out of R and into a file, using a graphical device, or using a ggplot function ggsave.\nSaving with a graphical device\nTo save using a graphical device, we essentially prepare a file of the type we want (i.e., a pdf or png or jpg). This is the “graphical device”. Once we’ve opened our blank graphical file, we print the plot, and then close the device. This method (using a graphical device) works with any graphical output from R, not just ggplot.\n\n\n# create a plot and save it to a variable\nmy_plot <- ggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = WELL_USE)) +\n  facet_wrap(~SITE_CODE, ncol = 2, scales = \"free\")\n\n# open a PDF graphical device\npdf(\"results/my_plot.pdf\")\n\n# print the plot\nmy_plot\n\n# close the graphical device\ndev.off()\n\nquartz_off_screen \n                2 \n\nWe can print multiple plots into a PDF graphical device.\n\n\n# create a plot and save it to a variable\nmy_plot_2 <- ggplot(gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = SITE_CODE)) + \n  facet_wrap(~WELL_USE, ncol = 1)\n\n# open a PDF graphical device\npdf(\"results/my_plots.pdf\")\n\n# print the plot\nmy_plot\nmy_plot_2\n\n# close the graphical device\ndev.off()\n\nquartz_off_screen \n                2 \n\n# we will get a message about how many screens we still have open...this is ok!\n\n\nWe can save a png file in the same way, and specify output height and width.\n\n\n# open a PNG graphical device\npng(\"results/my_plot.png\", width = 10, height = 7, units = \"in\", res = 300)\n\n# print the plot\nmy_plot\n\n# close the graphical device\ndev.off()\n\nquartz_off_screen \n                2 \n\nSaving with ggsave()\nAnother way to save ggplots that involves less code is to use the ggsave() function. By default, ggsave() height and width arguments are understood to be in units of inches. This approach only works with plots that have been generated using the {ggplot2} package. To save different file types, we simply change the file extension to the format we want.\n\n\nggsave(\"results/my_plot_ggsave.pdf\", my_plot, height = 10, width = 7)\nggsave(\"results/my_plot_ggsave.png\", my_plot)\n\n\nColorblindness\nGlobal estimates suggest that 8% of men and 0.5% of women experience some form of colorblindness. When creating data visualizations, default palettes may not be colorblind-safe. Fortunately, {ggplot2} includes options for colorblind-safe scales. These can be used with both color and fill aesthetics by adding the scale_<color or fill>_viridis_<c or d> functions to our ggplots.\n\n\nggplot(data = gwl_10) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = SITE_CODE)) +\n  scale_color_viridis_d() # a \"discrete\" viridis color palette.\n\n\n\nAbove, the variable we want to map WELL_USE is mapped to a color and is a discrete variable, therefore, we use scale_color_viridis_d().\nIf we were mapping a continuous variable to color we would use scale_color_viridis_c().\n\n\nggplot(data = gwl) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = MSMT_DATE)) +\n  scale_color_viridis_c(trans = \"date\")\n\n\n\nWe can toggle color palettes, and reverse them. See the {viridis} vignette for more info.\n\nColor is a vibrantly discussed and very customizable in R. A few resources for color palette selection include colorbrewer2, sciviscolor.org, and colorgorical.\n\n\nggplot(data = gwl) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = MSMT_DATE)) +\n  scale_color_viridis_c(option = \"B\", direction = -1, trans = \"date\")\n\n\n\nBespoke plots\nWhat we’ve covered is just the tip of the iceberg. There are many, many more geoms, aesthetics, themes, scales, coordinates, and graphical libraries that extend the capabilities of ggplot. Once you have a handle on the grammar of graphics, bespoke plots are not far from your reach.\nAdjusting the theme() of a plot is a powerful way to customize the look and feel of the visualization. For example, we may improve the plot below by simply applying one of many preset themes (“theme_<name>”) and increasing the base_size.\n\n\np <- ggplot(data = gwl) +\n  geom_line(aes(MSMT_DATE, GSE_WSE, color = MSMT_DATE)) +\n  scale_color_viridis_c(option = \"B\", direction = -1, trans = \"date\") +\n  theme_minimal(base_size = 14) \np\n\n\n\nLet’s remove minor gridlines with a general call to the theme() function and specifying that the panel.grid.minor argument is a blank element.\n\n\np <- p +\n  theme(panel.grid.minor = element_blank())\np\n\n\n\nWe can move the legend inside the plot area with another theme() argument, legend.position.\n\n\n# place the legend position 90% along the x axis, and 25% along the y axis\np + theme(legend.position = c(0.9, 0.25))\n\n\n\nWe can also move the legend to the top of the plot. If you type legend.position and read the documentation in the RStudio script editor, you’ll notice the position argument options.\n\n\n\n\n\n\n\np + theme(legend.position = \"top\")\n\n\n\nNow let’s make the colorbar longer and thinner, move the legend title to the top, and center it. We’ll also overwrite our former plot with this new one.\n\n\np <- p + \n  theme(legend.position = \"top\") + \n  guides(color = guide_colorbar(barwidth = unit(20, \"lines\"), \n                                barheight = unit(0.5, \"lines\"),\n                                title.hjust = 0.5, \n                                title.position = \"top\"))\np\n\n\n\nWe can add a trend line by fitting a linear model to these data.\n\n\np <- p +\n  geom_smooth(data = gwl, aes(MSMT_DATE, GSE_WSE), \n              method = \"lm\", se = FALSE, linetype = \"dashed\")\np\n\n\n\nFinally, let’s format some labels.\n\n\np <- p +\n  labs(x = \"\", y = \"Depth to groundwater (ft)\",\n       title = \"Site code: 384121N1212102W001\",\n       caption = \"Source: Periodic groundwater level database, CA-DWR.\")\np\n\n\n\nJust about every aspect of plots can be customized with arguments in the theme() function. See ?theme for a full list of options to customize.\nBuilt-in themes are also a quick way to change the look of your plots. Let’s start over with a basic plot, view some built-in themes.\n\nFor even more themes, check out the {ggthemes} package.\n\n\n# basic plot\np <- ggplot(data = gwl) +\n  geom_line(aes(MSMT_DATE, GSE_WSE))\n\n# a built in theme \np + theme_bw()\n\n\n# another built in theme\np + theme_dark(base_size = 18)\n\n\n\n\nExtra Practice\nThere are many built-in themes in {ggplot2}. Add a theme you haven’t tried before to the p object from above. Type “theme_” and hit Tab to view options.\n\n\nClick for Answers!\n\n\n\np + theme_classic() \n\n\n\n\nExtending ggplot2\nIf you want to learn more about {ggplot2}, check out these two free online resources:\nggplot2: elegant graphics for data analysis: This is the definitive ggplot2 guide written by the package author that emphasizes code and ggplot2 fundamental skills and concepts.\nFundamentals of Data Visualization: This book emphasizes principles of good data visualization that will inform the graphics you create.\n\n\nPrevious module: 4. Import/Export Data\nNext module: 6. Data Structures\n\nSee https://art.djnavarro.net/ or https://www.data-imaginist.com/art↩︎\n",
      "last_modified": "2024-01-21T12:17:11-08:00"
    },
    {
      "path": "m_importing_and_exporting_data.html",
      "title": "4. Importing and Exporting Data",
      "description": "A journey of ten thousand steps begins with a single <font color=\"#009E73\">**import**<\/font> and ends with an <font color=\"#0072B2\">**export**<\/font>.\n",
      "author": [],
      "contents": "\n\nContents\nWhat’s a\nfunction?\nRead (import)\ndata\ncsv (comma separated\nvalues)\nxlsx and xls (Excel files)\nshp\n(ArcGIS shapefiles)\ndbf\nrds and .rda\nsqlite\n\nWrite\n(export) data\n\n\nLearning objectives\nKnow the basics of how a function works\nUnderstand how to read (import) data into R\nExplore data within R using View()\nUnderstand how to write (export) data from R\nUnderstand how to read and write different data formats (e.g.,\nexcel, csv, shp, rds, dbf, access)\n\n\nWhat’s a function?\nFunctions are how work gets done in R, and before we\njump into reading and writing data, we need to know how functions work\nbecause we will use functions to perform these tasks.\nA function takes any number of\narguments and, performs some transformations, and\nreturns an output.\nFor example, the R function sum() takes any\nnumber of numeric arguments and adds them together (recall you can view\nthe documentation for sum() by entering ?sum).\nLet’s add 1 and 2 like so:\n\n\nsum(1, 2)\n\n\n[1] 3\n\nIn R, we can create sequences easily. If we wanted to\ncreate a sequence of numbers from 1 to 10, we can use the function\nseq(), which takes 3 arguments: from (start\nvalue), to (end value), and by (increment of\nthe sequence).\n\n\nseq(0, 10, 1)\n\n\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\nConvince yourself that creating sequences of arbitrary length is\npossible. Can you create a sequence from 0 to 1 million by an increment\nof 500?\nBecause creating sequences incremented by 1 are so common, there’s a\nspecial shorthand for these sequences, 1:10. Let’s take the\nsum of the sequence from 1 to 10 by providing it as an argument to the\nfunction sum():\n\n\nsum(1:10)\n\n\n[1] 55\n\nsum() and seq() are two of many functions\nyou’ll encounter in R. Like all functions, they take inputs\n(arguments) and return an output. To take advantage of functions, we\nneed to apply them to our data. Let’s now use import and export\nfunctions in R to explore some water resources data.\n\nRead (import) data\nData come in many formats, and R has utilities for\nreading and writing all kinds of data. In this lesson, we’ll explore\nsome of the most common data formats you’ll encounter in the wild, and\nthe functions used to import these data.\ncsv (comma separated values)\nThe comma separated value, or .csv, is a simple and\neffective way to store tabular data. To read a csv file, we first import\nthe {readr} library, which contains the function\nread_csv(). Let’s read a file from our\ndata/gwl folder that contains station information for\ngroundwater level monitoring sites in California. You can also type\n“data/” and press Tab with your cursor just after the\n“/” to view all files in that path.\n\n\nlibrary(readr)\n\n# read the \"stations\" csv, save it as an object called \"stations\", and print the object\nstations <- read_csv(\"data/gwl/stations.csv\")\n\nhead(stations)\n\n\n# A tibble: 6 × 15\n  STN_ID SITE_CODE       SWN   WELL_NAME LATITUDE LONGITUDE WLM_METHOD\n   <dbl> <chr>           <chr> <chr>        <dbl>     <dbl> <chr>     \n1  51445 320000N1140000… <NA>  Bay Ridge     35.6     -122. USGS quad \n2  25067 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n3  25068 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n4  39833 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n5  25069 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n6  38479 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n# … with 8 more variables: WLM_ACC <chr>, BASIN_CODE <chr>,\n#   BASIN_NAME <chr>, COUNTY_NAME <chr>, WELL_DEPTH <dbl>,\n#   WELL_USE <chr>, WELL_TYPE <chr>, WCR_NO <chr>\n\nYou can also pass a URL to read_csv().\n\n\n# read the \"stations\" csv from the Github URL\nstations <- read_csv(\"https://github.com/r4wrds/r4wrds/blob/main/intro/data/gwl/stations.csv?raw=true\")\n\n\n\nR tells us upon import that this data has 43,807 rows and 15 columns.\nWe can verify this with the nrow() and ncol()\nfunctions, and the dim() function:\n\n\nnrow(stations)\n\n\n[1] 43807\n\nncol(stations)\n\n\n[1] 15\n\ndim(stations)\n\n\n[1] 43807    15\n\nWhenever we see rectangular data like this in R, it’s probably a\ndata.frame object, but just to check, we can always ask R\nto tell us what the class of the object is:\n\n\nclass(stations)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nThe printed output shows us the first few rows of the stations data\nwe just read, but if we wanted to dig a bit deeper and see more than 10\nrows and 7 columns of data, we can use the function View(),\nwhich in RStudio opens a data viewer.\n\n\nView(stations)\n\n\n\nWithin the viewer, we can search the data.frame, sort rows, and\nscroll through the data to inspect it.\nA data.frame is made of many vectors of the same length. We can\naccess a column using the $ operator, and subset the vector\nwith bracket notation [. To access the first row in the\nWELL_TYPE column:\n\n\nstations$WELL_TYPE[1]\n\n\n[1] \"Part of a nested/multi-completion well\"\n\nIf we wanted the WELL_TYPE entries 1 through 10, we can\nsubset by a vector of the sequence from 1 through 10:\n\n\nstations$WELL_TYPE[1:10]\n\n\n [1] \"Part of a nested/multi-completion well\"\n [2] \"Unknown\"                               \n [3] \"Unknown\"                               \n [4] \"Unknown\"                               \n [5] \"Unknown\"                               \n [6] \"Unknown\"                               \n [7] \"Unknown\"                               \n [8] \"Part of a nested/multi-completion well\"\n [9] \"Unknown\"                               \n[10] \"Unknown\"                               \n\nSometimes it’s helpful to count unique variables in a column,\nespecially for categorical data such as the well type.\n\n\ntable(stations$WELL_TYPE)\n\n\n\nPart of a nested/multi-completion well \n                                  1778 \n                           Single Well \n                                 12053 \n                               Unknown \n                                 29976 \n\n\nxlsx and xls (Excel files)\nExcel files are very common, and R has great utilities for reading in\nand processing excel files. Calenviroscreen data\ncomes in excel format, which we can read in like so:\n\n\nlibrary(readxl)\nces <- read_xlsx(\"data/calenviroscreen/ces3results.xlsx\")\nhead(ces, 10) # print the first 10 rows\n\n\n# A tibble: 10 × 57\n   `Census Tract` `Total Population` `California County`   ZIP\n            <dbl>              <dbl> <chr>               <dbl>\n 1     6019001100               3174 Fresno              93706\n 2     6071001600               6133 San Bernardino      91761\n 3     6019000200               3167 Fresno              93706\n 4     6077000801               6692 San Joaquin         95203\n 5     6019001500               2206 Fresno              93725\n 6     6037204920               2598 Los Angeles         90023\n 7     6077000300               2396 San Joaquin         95203\n 8     6019001000               4106 Fresno              93706\n 9     6037206050               2146 Los Angeles         90023\n10     6019000400               6343 Fresno              93721\n# … with 53 more variables:\n#   `Nearby City \\r\\n(to help approximate location only)` <chr>,\n#   Longitude <dbl>, Latitude <dbl>, `CES 3.0 Score` <dbl>,\n#   `CES 3.0 Percentile` <dbl>, `CES 3.0 \\r\\nPercentile Range` <chr>,\n#   `SB 535 Disadvantaged Community` <chr>, Ozone <dbl>,\n#   `Ozone Pctl` <dbl>, PM2.5 <dbl>, `PM2.5 Pctl` <dbl>,\n#   `Diesel PM` <dbl>, `Diesel PM Pctl` <dbl>, …\n\nBy default, read_xlsx() reads in the first sheet.\nHowever, there may be many sheets in an excel file. If we want to read\nin a different sheet, we can tell R which sheet to read in, and even how\nmany lines to skip before reading in data.\n\n\nmetadata <- read_xlsx(\"data/calenviroscreen/ces3results.xlsx\", \n                      sheet = 2, \n                      skip  = 6)\nmetadata\n\n\n# A tibble: 66 × 3\n   `Variable Name`                        Description `CalEnviroScre…`\n   <chr>                                  <chr>       <chr>           \n 1 \"Census Tract\"                         Census Tra… <NA>            \n 2 \"Total Population\"                     2010 popul… <NA>            \n 3 \"California County\"                    California… <NA>            \n 4 \"ZIP\"                                  Postal ZIP… <NA>            \n 5 \"Nearby City \\r\\n(to help approximate… City or ne… <NA>            \n 6 \"Longitude\"                            Longitude … <NA>            \n 7 \"Latitude\"                             Latitude o… <NA>            \n 8 \"CES 3.0 Score\"                        CalEnviroS… <NA>            \n 9 \"CES 3.0 Percentile\"                   Percentile… <NA>            \n10 \"CES 3.0 Percentile Range\"             Percentile… <NA>            \n# … with 56 more rows\n\n\nChallenge 1\nopen the documentation for read_xlsx() using\n? (Hint: type ?read_xlsx in the console and\nhit enter)\nread through the Arguments to get a sense of what else the function\ncan do\nRead in the “Index Summary” sheet of\n\"data/healthy_watersheds/CA_PHWA_TabularResults_170518.xlsx\",\nand select the appropriate number of rows to skip.\nWhat happened to column names during the read?\n\n\nClick for Answers!\n\n\n\nhealth <- read_xlsx(\"data/healthy_watersheds/CA_PHWA_TabularResults_170518.xlsx\", \n                    sheet = 2, \n                    skip  = 4)\nhead(health)\n\n\n# A tibble: 6 × 14\n  `Watershed Name`      HUC12 ECOREGION STATE Score...5 Percentile...6\n  <chr>                 <chr>     <dbl> <chr>     <dbl>          <dbl>\n1 Red Spring-Colorado … 1503…        14 CA        0.866           57.2\n2 Shadow Canyon         1503…        14 CA        0.861           55.0\n3 Eagle Pass            1503…        14 CA        0.844           49.4\n4 Mohave Valley-Colora… 1503…        14 CA        0.811           42.4\n5 Monumental Pass       1503…        14 CA        0.879           63.2\n6 Lobecks Pass          1503…        14 CA        0.855           53.1\n# … with 8 more variables: Score...7 <dbl>, Percentile...8 <dbl>,\n#   `Top 10%` <chr>, `Top 25%` <chr>, Score...11 <dbl>,\n#   Percentile...12 <dbl>, Score...13 <dbl>, Percentile...14 <dbl>\n\nSome column names during the read were renamed because they’re the\nsame in the Excel sheet. In R, a data.frame can have only one unique\nname per column – duplicate names aren’t allowed! Thus, R renamed those\nduplicate names. In a later module, we will see how to rename columns\nwithin R.\n\n\nshp (ArcGIS shapefiles)\nGeospatial data is ubiquitous. So is the ArcGIS data format, the\nshapefile. A georeferenced shapefile is, at minimum made of 4 files:\n.shp, .prj, .dbf, and\n.shx.\n\n\n# unzip Sacramento county shapefile\nunzip(\"data/shp/sac_county.zip\", exdir = \"data/shp/sac\")\n\n# read the shapefile\nlibrary(sf)\nsac_county <- st_read(\"data/shp/sac/sac_county.shp\")\n\n\nReading layer `sac_county' from data source \n  `/Users/richpauloo/Documents/GitHub/r4wrds/intro/data/shp/sac/sac_county.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -13565710 ymin: 4582007 xmax: -13472670 ymax: 4683976\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\n\nlibrary(ggplot2)\nggplot(sac_county) + geom_sf()\n\n\n\n\ndbf\n.dbf files are one kind of database file. If you’ve ever\nopened a shapefile with attribute information, you’ve used a\n.dbf file. The foreign package allows us to\nread .dbf files into R. Since this is a new package, we\nneed to install it with install.packages(\"foreign\").\nWe’ve been loading entire packages with the library()\nfunction, but you can also call a function from a package without\nloading it by using\n<package_name>::<function_name> syntax. Let’s\nload the .dbf file from our Sacramento County polygon.\n\n\nforeign::read.dbf(\"data/shp/sac/sac_county.dbf\")\n\n\n  OBJECTID        COUNTY_NAM COUNTY_ABB COUNTY_NUM COUNTY_COD\n1       34 Sacramento County        SAC         34         34\n  COUNTY_FIP   Shape        Shape.STAr    Shape.STLe geometry\n1        067 Polygon 4203503574.269531 406962.476293     <NA>\n\nrds and .rda\n.rds and .rda (.rda is\nshorthand for .RData) are a special R-based data formats\nused to store R objects. These files can be read just like another other\nimport functions shown above. Let’s use it to import the groundwater\nlevel station data we read in earlier. Note that a .rds\nfile can hold any single R object.\n\n\nstations <- read_rds(\"data/gwl/stations.rds\")\nhead(stations)\n\n\n# A tibble: 6 × 15\n  STN_ID SITE_CODE       SWN   WELL_NAME LATITUDE LONGITUDE WLM_METHOD\n   <dbl> <chr>           <chr> <chr>        <dbl>     <dbl> <chr>     \n1  51445 320000N1140000… <NA>  Bay Ridge     35.6     -122. USGS quad \n2  25067 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n3  25068 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n4  39833 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n5  25069 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n6  38479 325450N1171061… 19S0… <NA>          32.5     -117. Unknown   \n# … with 8 more variables: WLM_ACC <chr>, BASIN_CODE <chr>,\n#   BASIN_NAME <chr>, COUNTY_NAME <chr>, WELL_DEPTH <dbl>,\n#   WELL_USE <chr>, WELL_TYPE <chr>, WCR_NO <chr>\n\nSometimes, you may create an intermediate result that is\ntime-consuming to recreate from scratch each time, and you want to save\nthat intermediate result to streamline future analyses. You can export,\nor write this object to any number of data formats like a csv, SQL\ndatabase, or shapefile. Unlike these data formats however, data saved as\n.rds are saved as one of R’s object classes, like\ndata.frame, vector, list, and so on. In practice, only R is used to read\n.rds and .rda files, so these formats are\nchosen when we expect to use R to read these data at a later time.\nOne quick difference between .rds and .rda\nfiles, for .rds files we can only store a single R object\n(of any kind). For an .rda file, we can store\nmany. .rda are also compressed by default.\nThere are pros and cons we will discuss later.\nsqlite\nSQLite is an open-source database format based on SQL that’s useful\nfor storing large datasets locally on your computer. The methods to\nconnect to a SQLite database, list tables, read tables, and send queries\nare similar across other cloud databases you may encounter in the wild,\nlike Postgres, and enterprise\ndatabase systems like Microsoft Access. We use the {here}\npackage to construct relative paths in the RProject.\n\n\nlibrary(RSQLite)\nlibrary(here)\n# location of an sqlite database\ndbpath <- here(\"data/gwl/gwl_data.sqlite\")\n\n# actually connect to the database\ndbcon <- dbConnect(dbDriver(\"SQLite\"), dbpath)\n\n# list all the tables in the database\ndbListTables(dbcon)\n\n\n[1] \"measurements_sep\" \"perforations\"     \"stations\"        \n\n# get one of the tables into a dataframe\nhead(dbReadTable(dbcon, \"stations\"))\n\n\n  STN_ID          SITE_CODE           SWN WELL_NAME LATITUDE\n1  51445 320000N1140000W001          <NA> Bay Ridge  35.5604\n2  25067 325450N1171061W001 19S02W05K003S      <NA>  32.5450\n3  25068 325450N1171061W002 19S02W05K004S      <NA>  32.5450\n4  39833 325450N1171061W003 19S02W05K005S      <NA>  32.5450\n5  25069 325450N1171061W004 19S02W05K006S      <NA>  32.5450\n6  38479 325450N1171061W005 19S02W05K007S      <NA>  32.5450\n  LONGITUDE WLM_METHOD WLM_ACC BASIN_CODE                 BASIN_NAME\n1  -121.755  USGS quad Unknown       <NA>                       <NA>\n2  -117.106    Unknown Unknown      9-033 Coastal Plain Of San Diego\n3  -117.106    Unknown Unknown      9-033 Coastal Plain Of San Diego\n4  -117.106    Unknown Unknown      9-033 Coastal Plain Of San Diego\n5  -117.106    Unknown Unknown      9-033 Coastal Plain Of San Diego\n6  -117.106    Unknown Unknown      9-033 Coastal Plain Of San Diego\n  COUNTY_NAME WELL_DEPTH    WELL_USE\n1    Monterey         NA Residential\n2   San Diego         NA     Unknown\n3   San Diego         NA     Unknown\n4   San Diego         NA     Unknown\n5   San Diego         NA     Unknown\n6   San Diego         NA     Unknown\n                               WELL_TYPE WCR_NO\n1 Part of a nested/multi-completion well   <NA>\n2                                Unknown   <NA>\n3                                Unknown   <NA>\n4                                Unknown   <NA>\n5                                Unknown   <NA>\n6                                Unknown   <NA>\n\nhead(dbReadTable(dbcon, \"measurements_sep\"))\n\n\n  STN_ID          SITE_CODE  WLM_ID  MSMT_DATE WLM_RPE WLM_GSE\n1   4775 384931N1212618W001 1443624 2004-03-01   118.4   117.4\n2   4775 384931N1212618W001 1443625 2003-10-01   118.4   117.4\n3   4775 384931N1212618W001 1443622 2003-03-15   118.4   117.4\n4   4775 384931N1212618W001 1443620 2002-10-01   118.4   117.4\n5   4775 384931N1212618W001 1443621 2001-10-01   118.4   117.4\n6   4775 384931N1212618W001 1443623 2001-03-15   118.4   117.4\n  RDNG_WS RDNG_RP   WSE RPE_WSE GSE_WSE WLM_QA_DESC WLM_DESC\n1       0   127.0  -8.6   127.0   126.0        <NA>  Unknown\n2       0   121.7  -3.3   121.7   120.7        <NA>  Unknown\n3       0   119.5  -1.1   119.5   118.5        <NA>  Unknown\n4       0   128.9 -10.5   128.9   127.9        <NA>  Unknown\n5       0   131.4 -13.0   131.4   130.4        <NA>  Unknown\n6       0   116.5   1.9   116.5   115.5        <NA>  Unknown\n                     WLM_ACC_DESC WLM_ORG_ID\n1 Water level accuracy is unknown          1\n2 Water level accuracy is unknown          1\n3 Water level accuracy is unknown          1\n4 Water level accuracy is unknown          1\n5 Water level accuracy is unknown          1\n6 Water level accuracy is unknown          1\n                   WLM_ORG_NAME MSMT_CMT COOP_AGENCY_ORG_ID\n1 Department of Water Resources     <NA>               1074\n2 Department of Water Resources     <NA>               1074\n3 Department of Water Resources     <NA>               1074\n4 Department of Water Resources     <NA>               1074\n5 Department of Water Resources     <NA>               1074\n6 Department of Water Resources     <NA>               1074\n                          COOP_ORG_NAME\n1 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n2 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n3 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n4 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n5 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n6 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n\n# pass a query to the database\ndbGetQuery(dbcon, \"SELECT * from measurements_sep WHERE STN_ID = 4775 LIMIT 5\")\n\n\n  STN_ID          SITE_CODE  WLM_ID  MSMT_DATE WLM_RPE WLM_GSE\n1   4775 384931N1212618W001 1443624 2004-03-01   118.4   117.4\n2   4775 384931N1212618W001 1443625 2003-10-01   118.4   117.4\n3   4775 384931N1212618W001 1443622 2003-03-15   118.4   117.4\n4   4775 384931N1212618W001 1443620 2002-10-01   118.4   117.4\n5   4775 384931N1212618W001 1443621 2001-10-01   118.4   117.4\n  RDNG_WS RDNG_RP   WSE RPE_WSE GSE_WSE WLM_QA_DESC WLM_DESC\n1       0   127.0  -8.6   127.0   126.0        <NA>  Unknown\n2       0   121.7  -3.3   121.7   120.7        <NA>  Unknown\n3       0   119.5  -1.1   119.5   118.5        <NA>  Unknown\n4       0   128.9 -10.5   128.9   127.9        <NA>  Unknown\n5       0   131.4 -13.0   131.4   130.4        <NA>  Unknown\n                     WLM_ACC_DESC WLM_ORG_ID\n1 Water level accuracy is unknown          1\n2 Water level accuracy is unknown          1\n3 Water level accuracy is unknown          1\n4 Water level accuracy is unknown          1\n5 Water level accuracy is unknown          1\n                   WLM_ORG_NAME MSMT_CMT COOP_AGENCY_ORG_ID\n1 Department of Water Resources     <NA>               1074\n2 Department of Water Resources     <NA>               1074\n3 Department of Water Resources     <NA>               1074\n4 Department of Water Resources     <NA>               1074\n5 Department of Water Resources     <NA>               1074\n                          COOP_ORG_NAME\n1 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n2 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n3 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n4 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n5 SACRAMENTO MUNICIPAL UTILITY DISTRICT\n\nWrite (export) data\nTo write (export) data in R you need 2 things: data to write, and a\nlocation and format to write the data. For example, if we wanted to\nwrite our stations data to a csv in the “data_output”\nfolder, we would do the following:\n\n\n# write \"stations\" to a file in the data_output folder called \"my_stations.csv\"\nwrite_csv(stations, \"data_output/my_stations.csv\")\n\n\n\nNow check that location and verify that your station data was\nwritten.\nWe can do the same for other files:\n\n\n# write the Sacramento county polygon to a shapefile\nst_write(sac_county, \"data_output/sac_county.shp\")\n\n# write the Sacramento county polygon to an rds file\nwrite_rds(sac_county, \"data_output/sac_county.rds\")\n\n\n\nAs before, navigate to these folders to verify these data were\nwritten. We can also check to see if these data exist from within R:\n\n\nmy_results <- list.files(\"data_output\")\nmy_files  <- c(\"sac_county.shp\", \"sac_county.rds\")\n \n# test if your files are in the data_output folder\nmy_files %in% my_results\n\n# another handy function is `file.exists`, which tells you if your file exists\nfile.exists(\"data_output/sacramento_county.shp\")\nfile.exists(\"data_output/sac_county.shp\")\nfile.exists(\"data_output/sac_county.rds\")\n\n\n\n\nChallenge 2\ncreate an object called breakfast and assign it a\nstring with what you had for breakfast.\nwrite that object to breakfast.rds file in\n/data_output\nverify this worked by reading the string back in and inspecting\nit\n\n\nClick for Answers!\n\n\n\n# create a string and write it to an rds file\nbreakfast <- \"green eggs and ham\"\n\n# write_rds takes two arguments: the object to write and location to write it\nwrite_rds(breakfast, \"data_output/breakfast.rds\")\n\n# read the rds file back into R and save it as a variable\nmy_breakfast <- read_rds(\"data_output/breakfast.rds\")\n\n# use the `cat()` function (concatenate) to announce your breakfast\ncat(\"Today for breakfast I ate\", my_breakfast)\n\n\n\n\n\n\nPrevious\nmodule: 3. Project management\nNext\nmodule: 5. Data visualization\n\n\n\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_install_R.html",
      "title": "1. Software Installation & Setup",
      "description": "Getting your computer <font color=\"#009E73\">*ready*<\/font> so you can <font color=\"#0072B2\">**learn**<\/font>!\n",
      "author": [],
      "contents": "\n\nContents\nStep 1. Install   \nWindows: Download and install R\nMacOS: Download and install R\n\nStep 2. Install   \nWindows: Download and install RStudio Desktop (free)\nmacOS: Download and install RStudio Desktop (free)\nCheck Install!\n\nStep 3. Install R Packages\nStep 4. Install Geospatial Packages\nInstalling from Source (with Compilation)\nWindows Geospatial Instructions (from source):\nMacOS Geospatial Instructions (from source):\nInstall Spatial R Packages\n\nStep 5. Download Workshop Datasets\nQuick Test!\n\n\nWelcome! There’s a few things we’d like everyone to try and do before the workshop starts. For this workshop and the following lessons, we need to install the following programs/software in this order (more details below for different operating systems):\n\nOur goal is for you to learn how to use these tools so you can get stuff done. You can learn to be an expert later! These are guidelines to install the necessary tools. Feel free to let us know if you run into trouble!\nInstall  \nInstall  \nInstall R Packages & Test They Work\nInstall Geospatial Packages and \nDownload Data for the Workshop\nBelow, we provide extra details appropriate for Windows and MacOSX operating systems. For additional installation options, see here.\nStep 1. Install   \nR is the underlying statistical computing environment, or the engine we use to do things.\nWindows: Download and install R\nGo to CRAN and download the R installer for Windows. Make sure to choose the latest stable version. Download the .exe file and double click to install.\nYou can click next through the standard dialogs and accept most defaults. But at the destination\nscreen, please verify that R is installing C:\\Program Files\\R (version number may vary), and you may need administrative privileges to do this if you’re not working on your personal computer.\nAt the “Select Components” screen, you can accept the default and install both 32-bit and 64-bit versions.\n\nAt this screen, uncheck “Create a desktop icon” because non-admin users in Windows will be unable to delete it.\n\nMacOS: Download and install R\nDownload and install R from the CRAN website here\nSelect the .pkg file for the latest R version\nDouble click on the downloaded file to install R and follow the setup steps\nInstall XQuartz if you don’t already have it (some packages require it)\nStep 2. Install   \nUsing R alone is possible, but less ideal (and less fun!). RStudio is an open-source graphical Integrated Development Environment or IDE that makes using R much easier and more interactive. In this course, we will use the free RStudio Desktop version.\nWindows: Download and install RStudio Desktop (free)\nDownload and install from RStudio\nSelect RStudio x.yy.zz - Windows Vista/7/8/10 (where x, y, z are version numbers)\nDouble-click the installer. It will ask for your administrator credentials to install, so you may need IT assistance if using a work computer.\nAccept all the default options for the RStudio install\nmacOS: Download and install RStudio Desktop (free)\nGo to the RStudio download page\nUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit) (where x, y, and z represent version numbers)\nDouble click the file to install RStudio\nOnce it’s installed, open RStudio to make sure it works and you don’t get any error messages.\nCheck Install!\nOnce installed, RStudio should be accessible from the start menu. Start up RStudio.\nFind “Console” window: By default the console window will be on the left side of RStudio. Find that window. It will looking something like:\n\nCopy and paste the following code: Once in that console window, copy the code below and paste it into the Console window (just to the right of the little >). Then hit ENTER.\nversion$version.string\nVerify your installed R version: Ideally you should be running the latest stable release. If you have an older version, please install the newest version using the instructions above.\nHere is what a current R version looks like.\n\n[1] \"R version 4.3.1 (2023-06-16)\"\n\nStep 3. Install R Packages\nThe core set of packages we will need are as follows. There may be a few more we need to install during the course, but this should be the majority.\nWe install packages available for all R users from an online repository called CRAN, by pasting the following code into the RStudio Console window and hitting ENTER.\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"viridis\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"readxl\")\ninstall.packages(\"lubridate\")\n\n\nYou may see a message\nThere are binary versions available but the source versions are later:\nfollowed by a list of the packages you are trying to install. This is usually followed by this message:\nDo you want to install from sources the package which needs compilation? (Yes/no/cancel)\nCompilation means the code associated with the package needs to be translated into R and built for your operating system. Generally we can use the binary option, so after that message type “No” and hit “Enter” on the keyboard. In some cases we do need/want to compile the package to get the most recent updates. For this course, try installing from binary (type No) first!\nYou may see this:\n\nThat’s OK! There’s a dialog box hiding behind RStudio asking if you want to create a personal folder in your Documents folder. Click Yes. It will look something like this when done:\n\nStep 4. Install Geospatial Packages\nFor some of the mapping and spatial lessons, we need a set of geospatial tools/packages. For the majority of the content we will cover, this requires installation of the {sf} package. When we run the lines below, we may get a question in the Console that is preceded by a list of package names and columns with binary, source, and needs_compilation. That’s ok!\nThe First Option should be to try and install without compilation. So, if you run the lines below:\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"mapview\")\n\n\nAnd you see a message similar to below (yours will probably look different!!):\n There are binary versions available but the source versions are later:\n          binary source needs_compilation\nsf         1.0-6  1.0-7             FALSE\nmapview   2.10.3 2.11.0              TRUE\n\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel) \nJust make sure to type “No” at the end of the line and hit Enter!\nInstalling from Source (with Compilation)\nHowever, more commonly, we may need to install/update from source to have the most updated functions and options from a package (don’t worry if you have no idea what this all means yet!). This generally means the code needs to be translated and built or compiled so your computer can interpret it. To do so, we need to follow instructions below for the operating system of your choice.\nWindows Geospatial Instructions (from source):\nTo install {sf} successfully from source, Windows users will need to install Rtools. Download the 64-bit compiler. Use the “R-release” version, not the R-devel version. R-release is the stable version, and the devel version is in development.\nEnsure that the Rtools installation matches the version of R you downloaded. For example if you downloaded R version 4.2.3, you would want Rtools version 4.2.\nMacOS Geospatial Instructions (from source):\nFor MacOS users, follow the recommended instructions on the {sf} webpage, which if possible, requires opening a Terminal window and successfully installing Homebrew with the code at this website. You can test if brew is installed by typing brew config in the Terminal and hitting Enter.\nWe can then install a few spatial tools (via Terminal still!) with the following code:\nbrew install pkg-config\n# then hit enter and wait\n\nbrew install gdal\n# then hit enter and wait\nAt this point, you should now be able to install {sf} from binary on a Mac. Proceed to the next section!\nInstall Spatial R Packages\nAfter this has successfully installed, return to the RStudio “Console”, and install the following packages by typing the following into the console and pressing “return” on the keyboard:\n\n\ninstall.packages(\"sf\")\ninstall.packages(\"mapview\")\n\n\nFor {sf}, try installing from binary first (so type “No” in your R Console window and hit Enter).\nSuccess should return something like this (not an error message) when you load the library with library(sf):\nlibrary(sf)\n# Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nAdditional geospatial installation instructions can be found here, but try the listed approach first.\nAs a final sanity check, copy/paste the following in the console in RStudio and press Enter. This will open an interactive map of the built-in dataset breweries in the “Viewer” pane, and will look like the map below. Click on points and pan around to celebrate a successful install!\n\n\nlibrary(mapview)\nmapview(breweries)\n\n\n\nStep 5. Download Workshop Datasets\nWe’ll be working with a few different datasets for this workshop (see here). We’ve zipped all these data and created an RStudio project into a single folder, which can be downloaded here. Bear in mind this .zip file is ~30 MB in size, and may take a few minutes to download depending on internet connections.\nDownload the zipped file of all intro course data.\nWe will help you set up a project in the upcoming lessons!\nQuick Test!\nJust to make sure each package installed successfully, do the following:\nOpen up RStudio\nFind the “Console.” This is usually the bottom left pane in RStudio\nIn the “Console”, find the R Prompt (the part that starts with >)\nType in (or copy and paste from here will also work) the following command at the R Prompt and hit return\n\n\n# check that these packages are installed successfully:\nc(\"tidyverse\", \"sf\", \"viridis\", \"mapview\") %in% installed.packages()\n\n[1] TRUE TRUE TRUE TRUE\n\nIf you have successfully installed packages, you should see a list of “TRUE” repeated once for each package listed inside the c( ).\n\n\nPrevious module: Introduction\nNext module: 2. Getting started\n\n\n\n",
      "last_modified": "2024-01-21T12:17:11-08:00"
    },
    {
      "path": "m_intro_mapmaking.html",
      "title": "11. {sf} and map making in R",
      "description": "Spatial data and Mapmaking 101\n",
      "author": [],
      "contents": "\n\nContents\nGeospatial data with {sf}\nReading spatial data\nConverting a dataframe to\nsf\nProjecting spatial data\nSpatial join with\nst_intersection()\n\nPlotting {sf}\ndata\nInspect data with\nplot()\n\nInteractive mapping with\n{mapview}\nBasic use\nCustomizing mapview\n\nStatic maps with {ggplot2}\nBasic use\nCustomizing maps in\nggplot2\nExporting maps in ggplot2\n\nAdditional Resources\n\n\nLearning objectives\nRead spatial data\nUnderstand how to convert a dataframe into an {sf} object\nPractice transforming {sf} spatial data between projections\nDemonstrate a spatial join, st_intersection()\nCreate interactive webmaps\nCreate static maps with the base plot() method and\n{ggplot2}\nSave/export static maps\n\nGeospatial data with {sf}\nR is a powerful tool for working with spatial data and making maps.\nWithin R, you can do nearly everything that a GUI type program can do\n(e.g., ArcGIS or QGIS), and moreover, you can write a script to automate\nor scale up routine analyses, thus saving time on repetitive tasks and\nensuring you create reproducible workflows.\nThere are a variety of spatial mapping/plotting packages in R.\nHowever, the best option being used widely for vector-based spatial data\nin R is the {sf} package.\n{sf} is a powerful, clean, and fairly\nstraightforward approach because it has an easy to use syntax, it is\nfast and it can do most if not all of the tasks commonly done in other\ngeospatial software programs. Even better,\n{sf} spatial objects are simply\ndata.frames with a geometry column, so all of the tidy,\nwrangle, join, and plot capabilities from {dplyr} and {ggplot2} also\nwork on {sf} objects. Therefore, it is\npossible to use R to address all your data processing needs in a single\nenvironment without the need to move between tools for tabular data\n(e.g., Excel) and geospatial data (e.g., ArcGIS or QGIS).\nsf stands for “simple features” which is a way of\nrepresenting spatial data. See the R {sf}\npackage documentation for more details. “Sticky geometry”\nin the figure means that all geometry information is contained in a\nsingle column, and that column follows the data, no matter how you work\nwith it.\n(ref:ah-sf) Illustration by @allison_horst.\n\n\n\nFigure 1: (ref:ah-sf)\n\n\n\nReading spatial data\nThe {sf} package can read spatial data\nin various formats (e.g., shapefile, GeoJSON, PostGIS, kml), and is very\nstraightforward if the data is already spatial, requiring only a call to\nthe function st_read().\n\n\n# First we load the sf and here packages\nlibrary(sf)\nlibrary(tidyverse)\n\n# may need to unzip first:\nunzip(\"data/shp/sac_county.zip\", exdir = \"data/shp/sac\")\n\n# Read a shapefile of Sacramento county \nsac <- st_read(\"data/shp/sac/sac_county.shp\", quiet = TRUE) \n\ncolnames(sac)\n\n\n [1] \"OBJECTID\"   \"COUNTY_NAM\" \"COUNTY_ABB\" \"COUNTY_NUM\" \"COUNTY_COD\"\n [6] \"COUNTY_FIP\" \"Shape\"      \"Shape.STAr\" \"Shape.STLe\" \"geometry\"  \n\n\nNote, the quiet = TRUE option suppresses printing\ninformation about the imported spatial data. Sometimes this information\nis helpful, but see read_sf() which is the same as\nst_read() but defaults to quietly reading in data.\nConverting a dataframe to sf\nAt other times, you may need to convert tabular data to a spatial\nformat. To make an {sf} object, we are\ncreating a geometry column. This contains all the\ngeospatial information we need, and it lives within the dataframe.\nImportantly, this geometry column is “sticky”, meaning whatever we do to\nour data (tidy, filter, mutate etc) the associated geometry\ncolumn will stick with the data. What’s awesome is this column can\ncontain anything from information for a point to a line to a complex\npolygon – all in one column. To make an {sf} object, we need to know\ntwo important pieces of information…\ncoords: The columns that contain the\ngeospatial coordinates (either name or column number)\ncrs: The projection or EPSG (CRS, SRID, etc) that the data in1\nTo practice, let’s read all groundwater level monitoring stations in\nSacramento County, stations_sac.csv, and convert this\ntabular data to an sf object with the function\nst_as_sf(). We will specify which columns contain the\ncoordinates (coords), and what the\nprojection, or coordinate reference system\n(crs) the data is in. In this case, we\nknow the data is in NAD83, or EPSG 4269.\n\nNotice that all functions in {sf} begin\nwith an “st_” prefix. This makes it easy to type “st_” and\nuse tab-completion to find {sf} functions, and it also\nmakes it easier to read code and spot spatial transformations.\n\n\n# read groundwater level stations in Sacramento county as dataframe\nstations <- read_csv(\"data/gwl/stations_sac.csv\")\n\n# check object class\nclass(stations)\n\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n# convert stations into an sf object by specifying coordinates and crs\nstations <- st_as_sf(stations, \n                     coords = c(\"LONGITUDE\", \"LATITUDE\"), # note x goes first\n                     crs = 4269, # projection, this is NAD83\n                     remove = FALSE) # don't remove lat/lon cols from dataframe\n\n# check the class of the new object\nclass(stations)\n\n\n[1] \"sf\"          \"spec_tbl_df\" \"tbl_df\"      \"tbl\"        \n[5] \"data.frame\" \n\nProjecting spatial data\nA common problem in geospatial analysis is when two different\ndatasets are in different projections. We can check the projection of\nour sac and stations objects with the function\nst_crs(), and transform our data (or re-project) with the\nst_transform() function.\nWe know stations is in NAD83, but what about\nsac? Let’s check with st_crs(). Line 2 of the\noutput indicates WGS84.\n\n\nst_crs(sac)\n\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nWe can re-project, or transform the projection (crs) with\nst_transform() and by specifying the EPSG code to transform\nthe data to. Here we use NAD83 (EPSG:\n4269), so the Sacramento county boundary (sac) is in\nthe same projection as the groundwater level monitoring points\n(stations).\n\n\nsac <- st_transform(sac, crs = 4269)\n\n\n\nLastly, we verify our transformation worked.\n\n\nst_crs(sac)\n\n\nCoordinate Reference System:\n  User input: EPSG:4269 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Geodesy.\"],\n        AREA[\"North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands.  British Virgin Islands.\"],\n        BBOX[14.92,167.65,86.46,-47.74]],\n    ID[\"EPSG\",4269]]\n\nWe can even ask R if the projection (crs) of sac and\nstations are identical, and they are.\n\n\nidentical(st_crs(stations), st_crs(sac))\n\n\n[1] TRUE\n\nWe can also transform data using the actual sf object or dataframe,\nwithout needing to find the specific EPSG or CRS code. For example, if\nwe want to transform our sac county\npolygon into the same projection as our\nstations data, we can do the\nfollowing:\n\n\nsac <- st_transform(sac, crs = st_crs(stations))\n\n# verify these are the same\nidentical(st_crs(stations), st_crs(sac))\n\n\n[1] TRUE\n\n# or look at just the EPSG code:\nst_crs(sac)$epsg\n\n\n[1] 4269\n\nst_crs(stations)$epsg\n\n\n[1] 4269\n\nSpatial join with\nst_intersection()\nWith all of our spatial data in the same projection, we can perform a\nspatial join. Perhaps the most common spatial join is an intersection.\nFor example, above, the stations object only contains\nstations in Sacramento County, but it came from a much larger set of\nstations (n = 43,807). Let’s bring in all groundwater stations in the\nstate of California, convert it to an sf object class, and\nplot the data.\n\n\n# all gw stations in California\nall_gw_stations <- read_csv(\"data/gwl/stations.csv\") %>% \n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), \n           crs = 4269, \n           remove = FALSE) \n\n\n\n\n\n\nAs we can see above, groundwater monitoring stations are concentrated\nin Bulletin 118 subbasins.2\nUsing dplyr::filter() we can subset these stations to\nSacramento County.\n\n\n# filter to Sacramento county & verify this worked\nstations_sac <- stations %>% \n  filter(COUNTY_NAME == \"Sacramento\") \n\n# verify this worked\nunique(stations_sac$COUNTY_NAME)\n\n\n[1] \"Sacramento\"\n\nBut what if we didn’t have the county data already, or if we wanted\nto filter these data by a polygon that wasn’t detailed in one of the\nexisting variables in our dataframe? In this case, we can use\nst_intersection() which is a spatial join\nthat takes two arguments, the sf object we want to filter\n(x), and another sf object to filter by\n(y). If we use x = all_gw_stations and\ny = sac it will return all of the points in\nall_gw_stations that fall within sac\ncounty.\nBefore performing the spatial join, we must re-project our data from\na geographic coordinate reference system (CRS) to a\nprojected coordinate reference system.3\n\n\n# it's good practice to ensure your spatial data are in a projected CRS\n# like meters before performing spatial operations, so we transform to 3310\nall_gw_stations_3310 <- st_transform(stations, 3310)\nsac_3310 <- st_transform(sac, 3310)\n\n# perform the intersection \nstations_sac_3310 <- st_intersection(all_gw_stations_3310, sac_3310) \n\n# number of observations in each county\ntable(stations_sac_3310$COUNTY_NAME)\n\n\n\n     Placer  Sacramento San Joaquin      Sutter \n          2         485           1           1 \n\n\nEPSG 3310 is UTM NAD83, which is a metric grid with meters as\nthe unit of measurement. It’s typically more accurate and easier to use\nfor geospatial operations and measurements compared to degrees\n(latitude/longitude).\nInterestingly, there are 4 counties in our data after the spatial\njoin. Why is that? If we visualize the data, we can see that all of\nthese points are right on the border of Sacramento County, and the\nprocess that previously added county names to these data must have used\na slightly different Sacramento county polygon than the one we are using\nin this module.\n\n\n\nThere are many many more methods available beyond intersections,\nincluding area, distances, buffers, crops, voronoi polygons, nearest\nneighbor calculations, convex hull calculations, centroid calculations,\nand much, much more. The list of operations within {sf} are\nshown below.\n\n\nmethods(class = 'sfc')\n\n\n [1] [                     [<-                   as.data.frame        \n [4] c                     coerce                format               \n [7] fortify               identify              initialize           \n[10] mapView               obj_sum               Ops                  \n[13] print                 rep                   scale_type           \n[16] show                  slotsFromS3           st_area              \n[19] st_as_binary          st_as_grob            st_as_s2             \n[22] st_as_sf              st_as_text            st_bbox              \n[25] st_boundary           st_buffer             st_cast              \n[28] st_centroid           st_collection_extract st_convex_hull       \n[31] st_coordinates        st_crop               st_crs               \n[34] st_crs<-              st_difference         st_geometry          \n[37] st_inscribed_circle   st_intersection       st_intersects        \n[40] st_is_valid           st_is                 st_line_merge        \n[43] st_m_range            st_make_valid         st_nearest_points    \n[46] st_node               st_normalize          st_point_on_surface  \n[49] st_polygonize         st_precision          st_reverse           \n[52] st_sample             st_segmentize         st_set_precision     \n[55] st_shift_longitude    st_simplify           st_snap              \n[58] st_sym_difference     st_transform          st_triangulate       \n[61] st_union              st_voronoi            st_wrap_dateline     \n[64] st_write              st_z_range            st_zm                \n[67] str                   summary               type_sum             \n[70] vec_cast.sfc          vec_ptype2.sfc       \nsee '?methods' for accessing help and source code\n\nTo learn more about advanced spatial operations, see the Spatial data module in the Intermediate to Advanced wrds\ncourse, and online books and\nresources.\nPlotting {sf} data\nWith all of our spatial data in the same projection, we can start\nmaking maps! We will cover the built-in plot() method for\n{sf} objects, interactive maps with {mapview},\nand plotting with {ggplot2}.\nInspect data with\nplot()\nAfter reading spatial data you may want to plot it to make sure that\nit imported correctly, and to understand the fields. For a quick plot,\nyou can simply use plot().\n\n\n# plot the geometry\nplot(sac$geometry)\n\n\n\n\nIf we don’t specify the “geometry” column, plot() will\nplot the first 10 columns in the dataframe (you can control the number\nof subplots shown with the max.plot argument). Here we can\nsee there are 4 distinct basins (BASIN_CODE) in Sacramento County.\n\n\nplot(stations)\n\n\n\n\nInteractive mapping with\n{mapview}\nOne of the easiest and coolest packages you’ll find for interactive\nmapping is {mapview}. As long as data are\nin sf format, you can quickly make an interactive map.\nFirst let’s make sure we have an sf class of data.\n\n\nclass(stations)\n\n\n[1] \"sf\"          \"spec_tbl_df\" \"tbl_df\"      \"tbl\"        \n[5] \"data.frame\" \n\nclass(sac)\n\n\n[1] \"sf\"         \"data.frame\"\n\nBasic use\nNext we can use the simple mapview() function to create\nan interactive webmap!\n\n\nlibrary(mapview)\n\nmapview(sac)\n\n\n\n\nWe can add {mapview} objects to one another in the same\nway we add layers to a ggplot, by using a\n+. We can then toggle them on and off from\nthe interactive map from the top-left hand layer control icon. We can\nalso change the basemap layers being used on the map from this same\nmenu.\n\n\n# combine sac and stations data\nmapview(sac) + mapview(stations) \n\n\n\n\nNote that we can open a {mapview} object in our default\nweb browser by clicking on the little box and arrow to expand and view.\nThis is particularly helpful when pop-up tables contain dense\ninformation, as is the case with our stations\ndataframe.\n\n\n\nCustomizing mapview\nNot only can you combine mapview objects, but you can also customize\ntheir appearance by adjusting a variety of built-in arguments to the\nmapview() function.\n\n\n# make sac polygon transparent, with a thick red outline\nmapview(sac, alpha.regions = 0, color = \"red\", lwd = 2, layer.name = \"Sac Co\") +\n  # color points by the well depth\n  mapview(stations, zcol = \"WELL_DEPTH\", layer.name = \"Well depth (ft)\") \n\n\n\n\n\nChallenge 1: You\nTry!\nCreate a new mapview object of Sacramento County (sac),\nplus stations colored by “WELL_USE”. Add the argument\nburst = TRUE, and read the mapview documentation to learn\nwhat this does (Hint: Enter ?mapview and scroll to\n“Arguments”).\nToggle all layers off except for irrigation and residential wells.\nRecall this relationship for the next module on EDA.\n\n\nClick for Answers!\n\n\n\nmapview(sac,                    # sacramento county sf polygon\n        alpha.regions = 0,      # transparent interior\n        color = \"red\",          # red outline\n        lwd = 2,                # thick outline\n        layer.name = \"Sac Co\",  # layer name \n        legend = FALSE) +       # hide legend\n    mapview(stations,           # stations sf points\n            zcol = \"WELL_USE\",  # color by the well use\n            burst = TRUE)       # split each category into a layer\n\n\n\n\n\n\nAdditional Info\n\n{mapview} is a great package for quickly visualizing and\nsharing spatial data. To export and save a .html map that can be shared\nwith others, it’s currently advisable to specify\nmapviewOptions(fgb = FALSE) after loading the library. This\nallows us to save the map as a self-contained html file. To do so, click\non the Viewer tab, and then on\nExport > Save as Web\nPage. This .html file can then be zipped and\nemailed or shared, and opened in most web browsers.\n\nStatic maps with {ggplot2}\nInteractive maps are useful for fast data exploration and integration\ninto web applications, however, depending on the project, static maps\nmay more appropriate for reports, presentations, and sharing. Mapmaking\nwith {sf} objects in {ggplot2} follows the\nsame syntax we practiced in the ggplot2 module, using the\ngeom_sf() function.\nBasic use\n\n\n# put data in geoms rather than ggplot() as we have multiple datasets in one plot\np <- ggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = stations, color = \"blue\", alpha = 0.5) \n\n\n\nAdding a north arrow and scale bar is achieved with the\n{ggspatial} package.\n\n\np + \n  # north arrow (top left) & scale bar (bottom right) \n  ggspatial::annotation_north_arrow(location = \"tl\") +\n  ggspatial::annotation_scale(location = \"br\") +\n  labs(x = \"Longitude (NAD83)\", y = \"Latitude\", \n       title = \"Groundwater monitoring staions\",\n       subtitle = \"Sacramento County\") +\n  theme_minimal()\n\n\n\n\nJust as before in {ggplot2}, we can connect an aesthetic,\naes, like color to one of the column\nvariables. We can also facet and change the theme(). For\nmaps, theme_void() is useful because it removes graticules\n(grid lines), axis ticks, and labels which allows us to focus on the\ndata explored in the plot.\n\n\np <- ggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = stations, aes(color = WELL_DEPTH)) +\n  scale_color_viridis_c(\"Well depth (ft)\") + \n  theme_void() \np\n\n\n\n\n\nChallenge 2: Debug and\nmodify.\nFix the following code that, as written, will return an error. Then,\nwith the fixed code, map the color aesthetic to how the well is used\n(WELL_USE), and add a viridis color scale to this\ndiscrete variable.\n\n\nggplot() %>% \n  geom_sf(data = sac) +\n  geom_sf(data = stations) \n\n\n\n\n\nClick for Answers!\n\n\n\n# Use `+` instead of `%>%` to add ggplot objects together\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = stations) \n\n\n\nMap “WELL_USE” to color in the stations dataframe, and\nensure the color scale is a discrete variable with:\nscale_color_viridis_d()\n\n\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = stations, aes(color = WELL_USE)) +\n  scale_color_viridis_d(\"Well type\", option = \"B\") +\n  theme_void()\n\n\n\n\n\nCustomizing maps in ggplot2\nBecause we’re working in {ggplot2}, we can also\nfacet_wrap() and customize the theme().\n\n\n# facet the plot we created above, p, by the well use\np +\n  facet_wrap(~WELL_USE, nrow = 2) +\n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(barwidth = unit(20, \"lines\"), \n                                barheight = unit(0.5, \"lines\"),\n                                title.hjust = 0.5, \n                                title.position = \"top\"))\n\n\n\n\nWe might notice that a few extremely deep wells dominate the color\nscale and cause it to extend to large values that prevent us from seeing\nvariation in shallower well depths. We can improve this visualization by\ndrawing on some skills from the dplyr module. Let’s\noverwrite large well depth values with dplyr::mutate() and\nimprove our labeling on the map.\n\n\n# only 4.7% of wells have a well depth that exceeds 700 feet\n# but the color bar goes all the way to 1600 feet!\nsum(stations$WELL_DEPTH >= 700, na.rm = TRUE) / nrow(stations)\n\n\n[1] 0.04703476\n\n# overwrite large values & assign the resulting dataframe to a new object\nstations_viz <- stations %>% \n  mutate(WELL_DEPTH = ifelse(WELL_DEPTH >= 700, 700, WELL_DEPTH))\n\n# replot with the new stations_viz object\np <- ggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = stations_viz, aes(color = WELL_DEPTH)) +\n  facet_wrap(~WELL_USE, nrow = 2) + \n  # improve labels to reflect the changes made to values in these data\n  scale_color_viridis_c(\"Well depth (ft)\",\n                        # break points for labels along the colorbar\n                        breaks = seq(0, 700, 100),\n                        # labels to insert at each of the breaks\n                        labels = c(seq(0, 600, 100), \"> 700\")) + \n  theme_void() +\n  theme(legend.position = \"top\") +\n  guides(color = guide_colorbar(barwidth = unit(20, \"lines\"), \n                                barheight = unit(0.5, \"lines\"),\n                                title.hjust = 0.5, \n                                title.position = \"top\"))\np\n\n\n\n\nIn this improved visualization, it’s clear that irrigation wells tend\nto be deeper than residential wells. The main cluster of “Other” wells\nis associated with the Aerojet\nSuperfund site, and are fairly deep, around 300-600 ft.\nExporting maps in ggplot2\nOnce a map has been created, how do we get it out of R and into a\nform we can share with others? One option is to save a map directly from\nthe Plot viewer in RStudio.\n\n\n\nFor more control and reproducibility, we can also save a map by using\nthe methods covered in the “Saving\nplots” section of data visualization module, which include\nggsave(). It’s good to practice this way of doing things\nbecause it will build your understanding and help you automate saving\nhundreds of plots at a single time down the road.\n\n\nggsave(\"results/sac_well_depth_type.png\", p, height = 5, width = 8)\n\n\n\nAdditional Resources\nWe covered 3 common frameworks in which to plot spatial data in R:\nthe plot() method for {sf} objects,\n{mapview} package, and {ggplot2}. Although\nthese are popular and powerful approaches to visualize and map spatial\ndata, other packages in the R ecosystem are available including:\n{leaflet} and\nextensions like {leafpop}\n{tmap}\n{mapdeck}\n{rasterVis}\nfor raster data\n{plotly}\nThe online books/guides Geocomputation with\nR, RSpatial.org,\nand Spatial Data\nScience are additional resources for a deeper dive into spatial\ndata processing and mapping in R.\n\n\nPrevious\nmodule: 10. Joins and Binds\nNext\nmodule: 12. EDA\n\nCoordinate reference systems are an entire lesson unto\nthemselves. For a good overview, check out this Data\nCarpentry lesson. ↩︎\nBulletin 118 subbasins are DWR desginated zones that\ngenerally correspond to productive aquifers, and can be viewed\nhere.↩︎\nA discussion on coordinate reference systems is a\ncomplex topic in and of itself, and for the purposes of this module, we\nsummarize it as follows: A geographic CRS is round and\nbased on angular units of degrees (lat/lng), whereas a\nprojected CRS is flat and has linear units\n(meters or feet). Many functions in {sf} that make\ncalculations on data expect a projected CRS, and can return inaccurate\nresults if an object in a geographic CRS is used. This is a fascinating\ntopic with lots written about it! For more reading see this Esri\nblog, the Data Carpentry geospatial\nlesson, and the online\nGeocomputation with R book.↩︎\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_intro_Rmarkdown.html",
      "title": "13. Intro to Rmarkdown",
      "description": "Save time and have fun with reproducible documents + code! \n",
      "author": [],
      "contents": "\n\nContents\nRmarkdown: Knitting Things\nTogether\nReproducible\nCombine Text + Code + Figures\n\nRmarkdown Components\nyaml\nheader\nBody Text\ncode\nchunks\n\nKnit\nEarly and Often\nCustomizing Visuals\nTables\nInteractive\nMaps\nExtending R Markdown\nAdditional Resources\n\n\nLearning Rmarkdown\nLearn the main components of an Rmarkdown (.Rmd)\ndocument\nUnderstand how to create a new Rmd and add text and images\nLearn how to stitch (or knit) a document together and\nshare it\n\nRmarkdown: Knitting Things\nTogether\nThis entire course website is built using Rmarkdown files.\nDissertations, papers, reports, and interactive documents are all very\npossible using Rmarkdown. While it would be impossible to show you how\nto do all of these things, the good news is while Rmarkdown is highly\ncustomizable, there are only a few key components to learn. We’ll go\nover some of the main pieces, demonstrate some fun tricks that are made\neasy using Rmarkdown, and provide a bunch of great resources to help you\nlearn more!\nCheck out the RMarkdown Gallery\nfor examples and inspiration.\n(ref:AHrmdplot) Illustration by @allison_horst\n\n\n\nFigure 1: (ref:AHrmdplot)\n\n\n\nReproducible\nDepending on the tasks you are faced with, analysis is rarely\nsomething that happens a single time. We are constantly faced with tasks\n(sometimes small, sometimes large) that have repetitive information, or\nrepetitive steps to complete. Making this process more reproducible by\nusing reusable tools (e.g. {dplyr}) and skills\n(R/Rmarkdown!) means you save your future self time and\nenergy, as well as make it easier to communicate and share your\nwork.\n(ref:AHreproducible) Illustrations from the Openscapes blog Tidy Data\nfor reproducibility, efficiency, and collaboration by Julia Lowndes\nand Allison Horst”\n\n\n\nFigure 2: (ref:AHreproducible)\n\n\n\nCombine Text + Code + Figures\nRmarkdown documents may initially seem scary and a bit overwhelming,\nbut once we understand the 3 critical components of an Rmd document,\nthings get easier. There are many flavors and options to customize each\nof these, but let’s cover the basics.\nRmarkdown Components\nThere are three main parts of an Rmarkdown\ndocument:(yaml, text, and\ncode). The first of the three is required no matter\nwhat. The other two, text and\ncode are more flexible and are not\nnecessarily required.\nRStudio provides an excellent Rmarkdown editor, and even better, the\nmost recent version of this software provides a nice Visual\nEditor mode which makes it even easier to learn the basics of\nMarkdown and Rmarkdown. Let’s create a new Rmarkdown\ndocument in RStudio.\nFile > New File > R Markdown\nWe should see something like this:\n\n\n\nFigure 3: New R Markdown file\n\n\n\nUse the default settings for HTML (we can always change it later),\nand click OK.\nOnce the Rmarkdown document appears and opens in RStudio, we should\nsee something like this:\n\n\n\nFigure 4: The blank R Markdown template\n\n\n\nIf you look at the top right of the code editor window, there’s a\nsmall “A” shaped compass1.\n\n\n\nIf you click on that icon, the R Markdown document will render into\nsomething that is visually more appealing and essentially will look much\nmore like a word processor might.\n\n\n\nFigure 5: Visual markdown editor\n\n\n\nYou can toggle the outline/table of contents on and off with the\nbutton immediately to the left of the “A” shaped compass.\nyaml header\nyaml stands for\nyet another markup language. Save that for a trivia\nquestion in the future. Every Rmarkdown document uses a\nyaml header (the bits between the --- marks)\nat the top of the document to tell your computer how R should knit your\ndocument together (stitch all the different parts together into a single\nseamless document).\nThere are many possible options here and they can depend on the type\nof document we want to create, but typically the key pieces are:\ntitle: Name of our document\noutput: what we end up\nknitting our R markdown into (e.g., .pdf,\n.html, .docx). See the RStudio R Markdown\nwebsite for more details on different format options.\nNote, spacing and indentation matters in yaml. If we’d\nlike to specify additional options for a specific output, we need to add\nmore lines and indent the argument we want to add.\nLet’s focus on making an .html file, and add some\nparameters about the table of contents. We can add\ntoc: true and toc_float: true to our yaml so\nit looks like this:\n---\ntitle: \"Untitled\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n---\nNow each section and subsection of our document will appear in the\ntable of contents. How do we make a section or subsection? Read on!\nBody Text\nThe body of our document is typically text written in Markdown2. Markdown is a simple text language\nthat helps make it very easy to just sit down and type without worrying\nabout formatting, and it works across many different operating systems\nand applications. There are a few basic formatting options to do things\nlike make font bold, italicized, add numbered or bulleted lists, or add\nsection headers. To learn more, while in RStudio, go to the menu and\nlocate Help at the top of the screen. Under Help >\nMarkdown Quick Reference you’ll find a handy\n“cheatsheet” to help learn these options. There are many additional\nRStudio cheatsheets for many topics here, and a\nspecific pdf on using R Markdown is available as a pdf here.\nFor example, to add a figure using Markdown, we can use the\nfollowing:\n![alternate text](happy_face.png)\nWe’ll talk more about how to customize image sizes and add captions\nwithin RMarkdown later in this module.\nIllustration by @allison_horstcode chunks\nThe third and final component of an R Markdown document is what makes\nit R Markdown. The inclusion of\ncode! What’s great is while the default is\ntypically R code, there are quite a few additional code\nlanguage options that can be stitched together. Look for the little\n+C icon in green at the top of your R Markdown, or go to\nCode > Insert Chunk. The keyboard\nshortcut to do this is Ctrl + Alt + i.\n\n\n\nLet’s go ahead and create a new R code chunk and look at\nsome of the chunk options.\n\n```{r}\n# my empty code chunk!\n```\nThere are many code chunk options we can use. Put your cursor after\nthe {r, and hit tab. There are tons! If\nyou want to learn more, check the R\nMarkdown reference guide. The key options that we want to know:\necho: TRUE or FALSE. If TRUE, whatever\ncode we put in the chunk will be displayed when we knit the\ndocument.\neval: TRUE or FALSE. If TRUE, the code\ninside the chunk will be evaluated or run.\ninclude: TRUE or FALSE. If FALSE, the\ncode is still run, but the results won’t be displayed and neither will\nthe code.3\nfig.cap: A quoted character string,\nadds captions to figures from code outputs. One caption per chunk.\nout.width/out.height: A quoted\npercentage, i.e., “100%”. This controls the size of the image being\noutput from a given code chunk. Very easy and effective for changing\ngraphic output sizes.\nKnit Early and Often\nOne thing that makes writing in R Markdown fun is the ability to get\ninstant feedback. Clicking the Knit button when a\nsection of your document has been updated will (re)-generate the\ndocument. We can change the default location the outputs appear by\nchanging the options via the little gear wheel to the right of the knit\nbutton. Look for “Preview in Viewer Pane” and\nselect it. From that point on, every time you click knit, most outputs\nshould appear in the Viewer pane of RStudio.\n\n\n\nKnitting frequently can be tedious if you have a large document with\nlots of figures or visuals. However, it’s a great way to learn, and also\nto ensure things are rendering correctly and successfully.\nCustomizing Visuals\nThere’s lots of great visualization materials out there, but there\nare a few helpful tips to keep in mind when using images or graphics in\nyour R Markdown documents. While the default Markdown option does permit\nadding images with the ![](image.png) syntax, it doesn’t\npermit as much fine control.\nIf we use a handy function from the {knitr} package, we\ncan have more more concise control over the size and placement of an\nimage without having to learn a lot of special code. For example, we can\nuse either a local file path, or a url! In addition we can add some\narguments that help us specify the size of the image, using the\nout.width or out.height parameters. Finally,\nwe can add a caption with fig.cap. Note, here we want to\nhide the code and just show the image, so we use\necho=FALSE.\nBy using fig.cap in the code chunk, the figure caption\nnumbers will be automagically assigned and updated each time you\nknit\n```{r rivphoto, echo=FALSE, out.width='80%', fig.cap=\"A tranquil river (photo: R Peek)\"}\n\nknitr::include_graphics(\"images/river_peek.JPG\")\n\n```\n\n\n\nFigure 6: A tranquil river (photo: R Peek)\n\n\n\nSimilarly, we can include a URL.\n```{r delta, echo=FALSE, fig.cap=\"Sacramento-San Joaquin Delta, color infrared (image: R Pauloo)\"}\n\nurl <- \"https://raw.githubusercontent.com/richpauloo/rp/master/static/img/delta_cir_2.png\"\nknitr::include_graphics(url)\n\n```\n\n\n\nFigure 7: Sacramento-San Joaquin Delta, color infrared (image: R\nPauloo)\n\n\n\nTables\nSharing data in tables is a common need for reporting, summary, and\nanalysis. There are a number of packages in R that may be helpful for\nmaking tables. This list is not exhaustive, and is meant to just show a\nfew options that may be useful.\n{DT}: Great for\ninteractive and dynamic html based tables\n{kable} & {kableExtra}: Highly\ncustomizable static or dynamic tables\n{gt}: Very customizable static\nor dynamic tables\nLet’s load some data we can play with. We’ll use some existing\ndatasets that come with packages you already have installed. The {dplyr}\npackage comes with a number of different datasets, as do many R\npackages4. We’ll use the\nstorms dataset since it’s large (+10,000\nobservations), and our nwis_sites from the American River\nwhich we created in a previous module.\n\n\n# large dataset: dplyr::storms\nstorms <- dplyr::storms\n\n# sites\nnwis_sites <- read_csv(\"data/nwis_sites_american_river.csv\")\n\n\n\nFor large unwieldy tables that you may want to just be able to\nquickly search or explore, the {DT} package is great,\nespecially for .html documents. Let’s make a table of the\nstorms data. A nice feature of the datatable()\nis we can search and filter our table interactively.\n\n\nlibrary(DT)\n\nstorms %>% \n  slice(1:200) %>% # take first 100 rows\n  datatable() # that's it!\n\n\n\n\n\nFor a quick and easy static table, the kable() function\nfrom {knitr} is great. We can simply pass a dataframe to\nthe function and we get a table! For more fancy options, we can use the\nfunctions from the {kableExtra} package.\n\n\nlibrary(knitr)\nstorms %>% \n  slice(1:10) %>% # take first 10 rows\n  kable()\n\n\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\n-1\n25\n1013\nNA\nNA\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\n-1\n25\n1013\nNA\nNA\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\n-1\n25\n1013\nNA\nNA\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\n-1\n25\n1013\nNA\nNA\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\n-1\n25\n1012\nNA\nNA\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\n-1\n25\n1012\nNA\nNA\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\n-1\n25\n1011\nNA\nNA\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\n-1\n30\n1006\nNA\nNA\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\n0\n35\n1004\nNA\nNA\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\n0\n40\n1002\nNA\nNA\n\n\nHere’s a slightly fancier option using {kableExtra}. See\nthe great vignette\nfor more details.\n\n\nlibrary(kableExtra)\n\nstorms %>% \n  slice(1:10) %>% # take first 10 rows\n  kbl() %>%\n  kable_paper(\"hover\", full_width = T)\n\n\n\nname\n\n\nyear\n\n\nmonth\n\n\nday\n\n\nhour\n\n\nlat\n\n\nlong\n\n\nstatus\n\n\ncategory\n\n\nwind\n\n\npressure\n\n\ntropicalstorm_force_diameter\n\n\nhurricane_force_diameter\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n27\n\n\n0\n\n\n27.5\n\n\n-79.0\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1013\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n27\n\n\n6\n\n\n28.5\n\n\n-79.0\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1013\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n27\n\n\n12\n\n\n29.5\n\n\n-79.0\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1013\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n27\n\n\n18\n\n\n30.5\n\n\n-79.0\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1013\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n28\n\n\n0\n\n\n31.5\n\n\n-78.8\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1012\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n28\n\n\n6\n\n\n32.4\n\n\n-78.7\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1012\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n28\n\n\n12\n\n\n33.3\n\n\n-78.0\n\n\ntropical depression\n\n\n-1\n\n\n25\n\n\n1011\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n28\n\n\n18\n\n\n34.0\n\n\n-77.0\n\n\ntropical depression\n\n\n-1\n\n\n30\n\n\n1006\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n29\n\n\n0\n\n\n34.4\n\n\n-75.8\n\n\ntropical storm\n\n\n0\n\n\n35\n\n\n1004\n\n\nNA\n\n\nNA\n\n\nAmy\n\n\n1975\n\n\n6\n\n\n29\n\n\n6\n\n\n34.0\n\n\n-74.8\n\n\ntropical storm\n\n\n0\n\n\n40\n\n\n1002\n\n\nNA\n\n\nNA\n\n\n\nFor nicely formatted tables and lots of control, the\n{gt} package is good. It does take a little more wrangling\nto get things formatted, but there’s great documentation for this\npackage as well.\n\n\nlibrary(gt)\ntab1 <- nwis_sites %>% \n  select(-sourceName) %>% \n  slice(1:10) %>% # get just first 10 rows \n  gt()\n\ntab2 <- tab1 %>%\n  cols_label(identifier = \"NWIS ID\",\n             comid = \"COMID\",\n             X = \"Longitude\",\n             Y = \"Latitude\") %>% \n  tab_header(\n    title = \"NWIS Sites on the American River\",\n    subtitle = \"Downstream of Nimbus, Sacramento County\") %>% \n  fmt_number(\n    columns = vars(identifier, comid),\n    decimals = 0, use_seps = FALSE)\ntab2\n\n\n\nNWIS Sites on the American River\n    Downstream of Nimbus, Sacramento County\n    NWIS ID\n      COMID\n      Longitude\n      Latitude\n    11446500\n948021150\n-121.2277\n38.63546383729121181000\n15025011\n-121.3038\n38.6246311447540\n15039173\n-121.5206\n38.56832383438121204200\n15024993\n-121.3461\n38.57713383609121293200\n15024919\n-121.4933\n38.60240383052121324401\n15039157\n-121.5456\n38.51454383205121310901\n15039157\n-121.5192\n38.53481383457121254900\n15024941\n-121.4313\n38.58241383515121264400\n15024941\n-121.4466\n38.5874111446980\n15024969\n-121.3883\n38.56713\n\nInteractive Maps\nAnd as you’ve seen from previous modules, we can also add maps! This\nis also a really nice way to share lots of information. We can only add\n{mapview} maps if we are using an html-based output, but\nthey work well in R Markdown.\nLet’s plot our NWIS Stations from the American River that we already\nloaded above. First we need to convert these data to {sf}. Let’s show\ncode that does that. Remember to use eval=TRUE and\necho=TRUE! One additional tip is to label your R code\nchunks. This helps us troubleshoot when things are not knitting\nproperly, and helps keep things organized.\nCode chunk labels show up in the outline, so making them informative\nis good. Note that we can only use characters and “-” in a code chunk\nlabel!\n```{r make-nwis-sf, echo=TRUE, eval=TRUE}\n\nlibrary(sf)\n# make nwis_sites spatial by converting to sf\nnwis_sites_sf <- st_as_sf(nwis_sites, coords=c(\"X\", \"Y\"), remove=FALSE, crs=4326)\n\n```\nNext we can make our map with mapview(). Note, there may\nbe a minor rendering issue with {mapview}. Adding\nfgb=FALSE seems to address this!5\nNote, here we are going to hide our code (echo = FALSE in\nthe code chunk options) so that we only show the results!\n\n\n\nExtending R Markdown\nWe’ve demonstrated that R Markdown can be used for creating html\nfiles. This is the tip of the iceberg, and the foundation of building\nwebsites and dashboards with Rmd. Remember\nthat Rmd files can also knit to\n.doc and .pdf files, although we don’t cover\nit in this module. Also, because Rmd files\nare code, as you level up your R skills, you can begin to automate\ngenerating tens, hundreds, even thousands of reports from a single\ntemplate. To extend your understanding of the capabilities of\nRmd, check out One\nR Markdown document, 14 demos, a video of a talk by R Markdown\ncreator and developer Yihui Xie at rstudio::conf(2020).\nAdditional Resources\nThere many different types of outputs possible, including slide\npresentations (see {xaringan} and RStudio’s page on\nslides), pdfs, and word docs.\nHere’s a sample of some good resources that are freely available\nonline:\nOne\nR Markdown document, 14 demos: a video of a talk by R Markdown\ncreator and developer Yihui Xie at rstudio::conf(2020)\nR Markdown (online\nbook): all the tips and details you could ever want\nTips\nfor Working with Images in R Markdown: Helpful tips on sizing and\nimage resolution\nWorking\nwith pdf’s via R Markdown using the {tinytex} package: you’ll need a\nLaTeX distribution installed in order to render or knit to pdf. The {tinytex} package is a great\nsolution.\n\n\nPrevious\nmodule: 12. EDA\nNext\nmodule: 14. Troubleshooting\n\nDrafting\ncompasses are really cool.↩︎\nRead more on the simple markup language Markdown\nhere.↩︎\nCode that has been evaluated in one chunk can be\nreferred to or used in subsequent chunks. So you can load objects at the\nstart of your document and use them throughout the entire document.↩︎\nTo figure out what datasets are available across all\ninstalled packages, try\ndata(package = .packages(all.available = TRUE)).↩︎\n{mapview} is not a static package. Rather, it’s under\nactive development. At the time of writing, the solution to inserting\nmapview objects into an html document is to use\nmapviewOptions(fgb=FALSE). In the future, this may change\nso that it’s not needed and mapview objects in an html document “just\nwork”. A benefit of using open source projects is that conversations\nabout these fixes and updates can sometimes come from the package\ndevelopers themselves, as in the case of this mapview\nissue discussed on Github that provided our fix.\n```{r make-nwis-mapview, echo=FALSE, eval=TRUE}\nlibrary(mapview) mapviewOptions(fgb=FALSE)\nmapview(nwis_sites_sf, col.regions=“cyan4”, layer=“NWIS Sites”)\n```↩︎\n",
      "last_modified": "2023-01-24T09:52:20-08:00"
    },
    {
      "path": "m_joins.html",
      "title": "10. Joining Data",
      "description": "How to merge your data!\n",
      "author": [],
      "contents": "\n\nContents\nJoining Data\nCommon Variables\nSame Data, Different Names\n\nJoin Types\nLeft Join\nInner Join\nFull Join\nSemi Join\nAnti Join\n\nJoining With Common Variable Names\nUsing the defaults\nPiping Joins\n\nJoining With Different Variable Names\nSave it Out!\n\n\nJoining Data\nUnderstand different types of joins and how to use them\nUnderstand how to join datasets by shared data columns\nJoin data with different variable names\n\nJoining Data\nWe often need to take several datasets and merge them into a single dataset that we can use for analysis. This “join” can at times be tricky, and can be made more so if we aren’t exactly sure what data matches up between the two datasets, or if the same data is named differently in each dataframe. Sometimes joining data can resemble more of a gridlock than a clean and seamless merge.\n\n\n\n\n\nFigure 1: Merging Data? (source Wikipedia Commons)\n\n\n\n\nCommon Variables\nThe trick that makes a join possible is to have one or more shared variables across each dataset. This may be an ID column, or a specific data class. Importantly, these data can actually have different column names, but typically each needs to have the same class of data (i.e., character and character) and some shared observations across each dataset in order to join the datasets.\nWhile this may seem daunting at first, it’s not! R has some excellent tools built into the {dplyr} package that make joining datasets fairly straightforward. First let’s import some data we can use to join. Here we will build off of the datasets we used in the previous {dplyr} module.\nFirst let’s load the libraries and data we’ll be using. We’ll be continuing to use some groundwater data as shown in previous modules, including a dataframe of stations, measurements through time, and information on perforations. We’ll also join these data to the CalEnviroscreen dataset which “identifies California communities by census tract that are disproportionately burdened by, and vulnerable to, multiple sources of pollution”. One of the most notable uses of the CalEnviroscreen dataset has been to inform CalEPA’s identification of disadvantaged communities. We’ll be using CalEnviroscreen v3.0 in the code that follows.\n\n\nlibrary(tidyverse)\n\n# GW stations data\ngw_stations <- read_csv(\"data/gwl/stations.csv\")\n\n# GW measurements data for Sac, El Dorado, Placer Counties\ngw_measurements <- read_csv(\"data/gwl/measurements_sep.csv\")\n\n# GW depths\ngw_perf <- read_csv(\"data/gwl/perforations.csv\")\n\n# CalEnviroscreen Data\ncalenviro <- read_csv(\"data/calenviroscreen/ces3results_data.csv.zip\")\n\n\n\nDid you know you can read a .zip file directly with read_csv()? Saves disk space\n\n\nChallenge 1: Find the Commonalities\nLook at the 3 different groundwater datasets (gw_measurments, gw_perf, and gw_stations) and find potential common columns/variables we can use to join. How many are there?\n\nClick for Answers!\n\nThere are many ways to view our data. We want to not only know the column names, but also what kind of data exists in each. What common columns can we use?\nSITE_CODE and STN_ID is in each of the gw_measurments, gw_perf, and gw_stations datasets.\nA great package to use for cleaning and checking things like this is the {janitor} package. We can use the compare_df_cols function to find out what data types exist across the dataframes we want to potentially join.\n\n\nlibrary(janitor)\n# tells us the column class in each dataframe, NA's for non-existent cols\ncompare_df_cols(gw_stations, gw_perf, gw_measurements)\n\n\n          column_name gw_stations   gw_perf gw_measurements\n1          BASIN_CODE   character      <NA>            <NA>\n2          BASIN_NAME   character      <NA>            <NA>\n3             BOT_PRF        <NA>   numeric            <NA>\n4  COOP_AGENCY_ORG_ID        <NA>      <NA>         numeric\n5       COOP_ORG_NAME        <NA>      <NA>       character\n6         COUNTY_NAME   character      <NA>            <NA>\n7             GSE_WSE        <NA>      <NA>         numeric\n8            LATITUDE     numeric      <NA>            <NA>\n9           LONGITUDE     numeric      <NA>            <NA>\n10           MSMT_CMT        <NA>      <NA>       character\n11          MSMT_DATE        <NA>      <NA> POSIXct, POSIXt\n12            RDNG_RP        <NA>      <NA>         numeric\n13            RDNG_WS        <NA>      <NA>         numeric\n14            RPE_WSE        <NA>      <NA>         numeric\n15          SITE_CODE   character character       character\n16             STN_ID     numeric   numeric         numeric\n17                SWN   character      <NA>            <NA>\n18            TOP_PRF        <NA>   numeric            <NA>\n19             WCR_NO   character      <NA>            <NA>\n20         WELL_DEPTH     numeric      <NA>            <NA>\n21          WELL_NAME   character      <NA>            <NA>\n22          WELL_TYPE   character      <NA>            <NA>\n23           WELL_USE   character      <NA>            <NA>\n24            WLM_ACC   character      <NA>            <NA>\n25       WLM_ACC_DESC        <NA>      <NA>       character\n26           WLM_DESC        <NA>      <NA>       character\n27            WLM_GSE        <NA>      <NA>         numeric\n28             WLM_ID        <NA>      <NA>         numeric\n29         WLM_METHOD   character      <NA>            <NA>\n30         WLM_ORG_ID        <NA>      <NA>         numeric\n31       WLM_ORG_NAME        <NA>      <NA>       character\n32        WLM_QA_DESC        <NA>      <NA>       character\n33            WLM_RPE        <NA>      <NA>         numeric\n34                WSE        <NA>      <NA>         numeric\n\n# note SITE_CODE and STN_ID both exist across all 3 datasets\n\n\n\n\nSame Data, Different Names\nNow we’d like to know if there are similar data columns between the groundwater datasets and the CalEnviroscreen data. To do this let’s inspect our stations and calenviro data with either head or names or str. All will tell us similar information. At this point, we know we can tie the groundwater data together, but we need to find something to crosswalk or join the CalEnviroscreen data to the groundwater data. In this case, the CalEnviroscreen data are at a census tract level, and contain data with ZIP codes, counties, and census tracts.\n\n\n\n\nFigure 2: Artwork by @allison_horst\n\n\n\nThere are a lot of variables in the CalEnviroscreen dataset, so we’re only showing the first 10 here.\n\n\nnames(calenviro) %>% head(10)\n\n\n [1] \"Census Tract\"                                     \n [2] \"Total Population\"                                 \n [3] \"California County\"                                \n [4] \"ZIP\"                                              \n [5] \"Nearby City \\n(to help approximate location only)\"\n [6] \"Longitude\"                                        \n [7] \"Latitude\"                                         \n [8] \"CES 3.0 Score\"                                    \n [9] \"CES 3.0 Percentile\"                               \n[10] \"CES 3.0 \\nPercentile Range\"                       \n\nnames(gw_stations)\n\n\n [1] \"STN_ID\"      \"SITE_CODE\"   \"SWN\"         \"WELL_NAME\"  \n [5] \"LATITUDE\"    \"LONGITUDE\"   \"WLM_METHOD\"  \"WLM_ACC\"    \n [9] \"BASIN_CODE\"  \"BASIN_NAME\"  \"COUNTY_NAME\" \"WELL_DEPTH\" \n[13] \"WELL_USE\"    \"WELL_TYPE\"   \"WCR_NO\"     \n\nWhat do we see? Looks like we have county names in both of these datasets, but they have different column names (COUNTY_NAME in stations and California County in calenviro. So we know we can join our three groundwater datasets together, and we know we can join the CalEnviroscreen data with the stations dataset by county. Let’s talk about the types of joins we may use now!\nJoin Types\nThere are quite a few different join types that are available via the {dplyr} package. Here are some great animations by Garrick Aden-Buie that help illustrate the various join types.\nLeft Join\nReturns all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. Probably one of the most common joins, where we want to keep everything from our first dataframe (x), and anything that matches x from the second dataframe (y).\n\nThe left join is perhaps the most commonly used join, and if you only know one join, this should be it.\n\n\n\nInner Join\nThe matching join, returns all rows from x where there are matching values in y, and all columns from x and y.\n\n\n\nFull Join\nReturns all rows and all columns from both x and y. Where there are not matching values, the function returns NA for that observation. This is a full merge, where no data is lost from either dataset, and any non-matching data gets an NA.\n\n\n\nSemi Join\nReturns all rows from x where there are matching values in y, keeping only columns from x. Also known as a filtering join because you are basically filtering one dataframe, using another dataframe as the match. This can be useful when you don’t actually want to join with data, but you do want to subset the data to match observations in another dataset.\n\n\n\nAnti Join\nAlso a filtering join. This keeps all rows from x where there are not matching values in y, keeping just columns from x. This join is useful to find out which data is not joining. Note, the dataframe you put first matters: if you switch x and y you may get different answers.\n\n\n\n\n\n\nJoining With Common Variable Names\nLet’s demonstrate some joins with our groundwater dataframes. First, we’ll filter the station data to only stations in Sacramento County, and then join these data with the other groundwater data.\n\n\nsac_stations <- gw_stations %>% \n  filter(COUNTY_NAME == \"Sacramento\")\n\ndim(sac_stations) # get number of ROWS , COLUMNS\n\n\n[1] 494  15\n\nIt looks like we have 494 groundwater stations in Sacramento County. Let’s use this subset of data to join to our other groundwater dataframes.\nUsing the defaults\nThe various joins outlined above are very flexible, but the default options are also very good. That means, if we don’t specify any details, the functions will often try to match and join based on common columns (that have identical names and data types), though this isn’t a great habit to set because that may not always be what we want 👾!\nLet’s left_join() our Sacramento stations with the gw_measurments data. Note, we expect to get the same number of stations as the left part of our join (the x data.frame), because left_join should keep everything on the left side, and only keep what matches from the right side (y).\n\n\n# default join, only give x, y\nsac_stations_measurements <- left_join(sac_stations, gw_measurements)\n\ndim(sac_stations_measurements)\n\n\n[1] 28208    32\n\nWhat happened? Since there are two matching columns, the join used both STN_ID and SITE_CODE. We ended up with lots more observations, and additional columns. It’s important to check and validate things worked.\n\nChallenge 2: Verify the Join\nHow can we verify we successfully joined our two datasets?\nCan you use either unique() or distinct() to prove we only have groundwater stations from Sacramento County in our joined dataset?\n\nClick for Answers!\n\nIt’s always a good idea to double check both visually and numerically that things are working as expected. In the example above, there are many gw_measurements per station so it may seem difficult to know this worked. However, we can double check the number of unique stations in our joined dataset…it should still equal the total number stations in our sac_stations dataset (n = 494).\n\n\n# look at how many unique SITE_CODE are in orig sac only dataset\ndistinct(sac_stations, SITE_CODE) %>% nrow()\n\n\n[1] 494\n\n# look at how many unique are in joined dataset\ndistinct(sac_stations_measurements, SITE_CODE) %>% nrow()\n\n\n[1] 494\n\n# look at top 10 stations with the most measurements:\nsac_stations_measurements %>% \n  group_by(SITE_CODE) %>% # group by SITE_CODE\n  tally() %>% # count how many measurements by STATION\n  arrange(desc(n)) %>% # arrange by count (n), descending\n  head(10) # top 10 records only\n\n\n# A tibble: 10 × 2\n   SITE_CODE              n\n   <chr>              <int>\n 1 384121N1212102W001  1582\n 2 383264N1213191W001   642\n 3 382548N1212908W001   595\n 4 384082N1213845W001   541\n 5 385567N1214751W001   510\n 6 386016N1213761W001   508\n 7 383204N1214430W001   499\n 8 384403N1212921W001   358\n 9 386151N1214467W001   321\n10 384147N1214507W001   311\n\n\nIf we use an inner_join(), do we get a different number of stations? Why?\n\n\n# default join, only give x, y\nsac_stations_inner <- inner_join(sac_stations, gw_measurements)\n\n# check station number\ndistinct(sac_stations_inner, SITE_CODE) %>% nrow()\n\n\n[1] 420\n\nThere’s a different number of stations here! What happened? Remember, inner_join() only keeps the rows that match in both data frames. So, this means while the original sac_stations dataset had 494 stations, the gw_measurements dataset does not have data for every station in the sac_stations dataset, thus n = 420 instead n = 494. When using left_join(), these missing observations get filled with NA, whereas with inner_join(), they are dropped.\nPiping Joins\nWe saw how we can join two datasets, but let’s try piping these together so we can join all three groundwater datasets together in one single chain of code. We’ll start with our Sacramento County stations only.\nHere we’ll pipe the data to the function, which means we only need to add the y argument. In R, a “.” is like a placeholder that represents the data we are piping along. It’s not strictly required (try running the function without it!), but here it helps us see the pieces that go into the function (x and y).\n\n\n# note, we aren't specifying the columns we want to join on (keep defaults)\nsac_gw_data_joined <- sac_stations %>% \n  left_join(., gw_measurements) %>% # passing this result to the next join\n  left_join(., gw_perf)\n\n# quick check of stations, should be n=494, because left_join!\ndistinct(sac_gw_data_joined, SITE_CODE) %>% nrow()\n\n\n[1] 494\n\nJoining With Different Variable Names\nWhat about joining dataframes with different variable names? This is much more common when trying to merge different datasets. Let’s try to join the CalEnviroscreen data with the Sacramento County groundwater stations data.\n\n\ncalenviro_stations <- left_join(calenviro, sac_stations)\n\n\n\nError: `by` must be supplied when `x` and `y` have no common variables.\nℹ use by = character()` to perform a cross-join.\nRun `rlang::last_error()` to see where the error occurred.\n\n\n\n\nFigure 3: Artwork by @allison_horst\n\n\n\nUh-oh, an error message! But if we look more closely, this error message is pretty descriptive. It tells us the two pieces we are working with (x = calenviro and y = sac_stations) don’t have a common variable name. Then it gives us a suggestion, use by = character(). Let’s try specifying the column names we want to join using the by argument. Note, we need to quote the variable names here (a character() vector), and they need to match the same order the dataframes were provided (x and y).\n\n\ncalenviro_stations <- inner_join(sac_stations, calenviro,\n                                by = c(\"COUNTY_NAME\" = \"California County\"))\n\ndim(calenviro_stations)\n\n\n[1] 156598     71\n\nGreat this worked! But let’s figure out why there are so many records if there were only n = 494 stations.\n\n\n# how many stations per census tract?\ncalenviro_stations %>% count(`Census Tract`)\n\n\n# A tibble: 317 × 2\n   `Census Tract`     n\n            <dbl> <int>\n 1     6067000100   494\n 2     6067000200   494\n 3     6067000300   494\n 4     6067000400   494\n 5     6067000500   494\n 6     6067000600   494\n 7     6067000700   494\n 8     6067000800   494\n 9     6067001101   494\n10     6067001200   494\n# … with 307 more rows\n\n# how many census tracts in Sacramento County\ncalenviro_stations %>% count(`Census Tract`)\n\n\n# A tibble: 317 × 2\n   `Census Tract`     n\n            <dbl> <int>\n 1     6067000100   494\n 2     6067000200   494\n 3     6067000300   494\n 4     6067000400   494\n 5     6067000500   494\n 6     6067000600   494\n 7     6067000700   494\n 8     6067000800   494\n 9     6067001101   494\n10     6067001200   494\n# … with 307 more rows\n\nSo there are 317 census tracts in Sacramento County, and 494 unique groundwater stations. If we multiply 317 * 494, we get the number of observations in our dataset (n = 156,598). So, it seems every single station is being joined to every single census tract…that’s because we only have county to join on.\nFor these sorts of operations, we actually want a more spatially explicit join to reduce this sort of duplication. Thankfully we’ve done this already. Let’s grab this data now so we can use it to make a better join of the data. Download it here, and see the spatial mapmaking module to see more on spatial joins. Let’s join our full joined Sacramento County groundwater dataset from earlier using the census tracts instead of county.\n\n\n\n\n\n# can use URL here too: \nxwalk <- read_csv(\"data/calenviroscreen/sac_county_crosswalk_to_gw_stations.csv\") %>% \n  # filter to just columns we need\n  select(SITE_CODE, tract)\n\n# join with station data...adding \"tract\" to dataframe\nsac_stations_w_tracts <- left_join(sac_stations, xwalk)\n\n# now join to CalEnviroScreen data\nsac_station_calenviro_by_tracts <- left_join(sac_stations_w_tracts, calenviro,\n                                             by = c(\"tract\" = \"Census Tract\"))\n\n# check dim\ndim(sac_station_calenviro_by_tracts)\n\n\n[1] 495  72\n\nOk, so what happened? We used an intermediate table (xwalk) to crosswalk using a finer resolution join for our stations. So now each census tract is associated with a specific groundwater station in Sacramento County, based on a spatial join of the CalEnviroScreen data with the stations data.\nLet’s close the loop, and join the station-calenviroscreen data back to our gw_perf and gw_measurement data so we have one single dataset for Sacramento County!\n\n\n\nsac_gw_all <- left_join(sac_station_calenviro_by_tracts, gw_perf) %>% \n  left_join(., gw_measurements)\n\n# lets move the CalEnviroScreen variables to the end\nsac_gw_all <- sac_gw_all %>% \n  select(STN_ID:WCR_NO, TOP_PRF:COOP_ORG_NAME, tract:`Pop. Char. Pctl`)\n\n\n\nSave it Out!\nRemember how to do this? Let’s save this using an R specific format, .rds, because it will retain the formatting and allow us to import/read in the data using whatever name we prefer. Here we’ll use the write_rds() function from the {readr} package (part of the {tidyverse}).\n\n\n# remember to use a relative path!\nwrite_rds(sac_gw_all, file = \"data/sacramento_gw_data_w_calenviro.rds\")\n\n\n\n\n\nPrevious module: 9. Functions Next module: 11. Spatial Data\n\n\n\n",
      "last_modified": "2022-05-11T22:12:53-07:00"
    },
    {
      "path": "m_pivots.html",
      "title": "8. Spreadsheets & Pivoting",
      "description": "Tidy data and best practices for spreadsheets!\n",
      "author": [],
      "contents": "\n\nContents\nSpreadsheets\nWhat is Tidy Data?\nDealing with Messy Data\nSame Tools, Different Data\nUnderstanding Messy Spreadsheets\n\nSpreadsheet Best Practices\nKeep Raw Data RAW\nRecord what you did with README tab\nDate Standards\nKeep one table per tab\nDon’t Merge or Color Data\nMissing Data or NA\n\nPivoting\nWide to Long\nLong to Wide\n\n\n\nLearning objectives\nUnderstand what messy data is and how to avoid it\nUnderstand how to effectively use spreadsheets\nLearn the difference between wide and long data, and how to pivot between wide and long formats\n\nSpreadsheets\nSpreadsheets have been woven into nearly every facet and field, and although they are meant to make our lives easier, it is typical to spend lots of time untangling the data [mis]adventures they create.\nHowever, a few simple tips on how to structure and use spreadsheets can make all the difference. There will still always be messy datasets to face, but learning how to more effectively use spreadsheets will make life easier and more reproducible, and hopefully make these encounters less daunting.\nLet’s go over what tidy data should look like, how to wrangle a messy real-life spreadsheet, and how to understand the difference between human readable and computer readable information.\nWhat is Tidy Data?\nUltimately spreadsheets (or any list or table of data) attempt to take human readable data and turn it into computer readable data. The problem is humans often like to add information that is only human readable (i.e., colors in spreadsheet). These bits of “flair” may be nice to visualize things, but computers can only interpret the literal pieces of data in each cell of a spreadsheet, so if we want additional information to be converted (like color, or what it represents), we need to store that information in a computer readable context.\nTypically when we look at data, we want it to be in a “tidy” format. Data in a tidy format, means that each column is a unique variable, and each row is a unique observation of data. Furthermore, any information we want the computer to retain should be stored as variables and not in side comments or coloring. This makes it possible for a computer to easily interpret these data, and avoids any unnecessary conversions or loss of data.\n\n\n\n\nFigure 1: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\nDealing with Messy Data\nIn reality, we will generally encounter non-tidy data in the wild. Messy data comes in many forms and it exists everywhere. Much of data science hinges on cleaning up and reformatting data so it can be used in some way other than the form it exists in. The good news is the concept of tidy data can be applied to any dataset, and there are some very straightforward tips to make this easy to implement!\n\n\n\n\nFigure 2: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\nSame Tools, Different Data\nThe great news is we can use a similar set of tools to deal with tidying and cleaning our data, and once we’ve learned these tools, there are few datasets that will stand in your way (though some may take longer than others!). An important thing to learn is that tidy data starts the moment we begin recording information. The more we can collect and update data in a “tidy” fashion, the less cleaning and wrangling is required to work with that data.\n\n\n\n\nFigure 3: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\nUnderstanding Messy Spreadsheets\nAn important step towards tidy data is learning to quickly spot where data is messy and what needs to be fixed.\nLet’s take a look at the messy spreadsheet data which we downloaded from this repository, unzipped, and can now be found in our project here: data/calenviroscreen/ces3results_messy.xlsx. These data are based on data from the Calenviroscreen data from previous modules.\n\n\n\n\n\n\nCHALLENGE 1\nWhat issues do you notice about this spreadsheet?\nWhat could we do to make this tidy?\n\nClick for Answers!\n\nMultiple tables on one sheet\nMultiple variables in a single column (see T Code and Data Type)\nTables start on different rows\nDifferent date formats\nDifferent county spellings (see California County)\nColor encoding\n\n\n\nExtra Practice\n\nIf you are interested in an excellent lesson on how to improve your spreadsheet skills, checkout the excellent lesson via Data Carpentry: Data Organization in Spreadsheets for Ecologists. This lesson should be read by anyone and everyone who uses spreadsheets (which is everyone), so go take a look!\n\nSpreadsheet Best Practices\nIf you have to work with a multi-tabbed spreadsheet (which we all do at some point), there are some basic rules/tips that can save you and anyone else that needs to work with the same spreadsheet. We also recommend checking out White et al. 20131 and Broman and Woo 20182.\nKeep Raw Data RAW\nAny raw data you have should always stay raw so you can always go back and “reproduce” your analysis. The danger with spreadsheets is its easy to change things with a few clicks and overwrite the raw data, and have no record of what you did.\nSo no matter what shape the data is in, it’s always advisable to make a copy of the tab with the data, append or prepend “RAW” to the tab name, and lock the tab so the data can’t be deleted or altered. To do this:\nright click on the tab and select Move or Copy\ncheck Create a Copy box\nright click on the newly copied tab and select Rename and add RAW\nright click on the RAW tab again, and select Protect Sheet\nthe only two boxes that should be checked are Select locked cells and Select unlocked cells. Click OK and save!\n\n\n\nRecord what you did with README tab\nWhile programming languages like R allow us to write scripts or code that record every step of the process, spreadsheet programs use an interface based on points and clicks. It’s easy to forget what we clicked on or what we did as we begin to clean/work with our data. An easy solution which your future self (or future collaborators) will thank you for is adding a README tab! This is where you can write notes about what things have been done to the data, ideally with sufficient information that future you or someone else would be able to reproduce your analysis or cleaned dataset by reading the README.\n\n\n\nDate Standards\nPerhaps one of the most frustrating and frequent data snafus you may encounter is how dates or datetimes get formatted. An entire lesson could be devoted to dealing with datetime data, but we can simplify it to one rule: keep dates formatted using the world standard3 of YYYY-MM-DD. Better yet, in spreadsheets, split the date into unique columns for Year, Month, and Day. It’s straightforward to paste these back together and format them in R, and because they are then stored as simple integer values there will be no missed translations between people or computers.\n\nThe Dates and Times lesson from R4DS by H. Wickham & G. Grolemund is an excellent overview on dates and times. Does every minute always have 60 seconds?\n\n\n\nKeep one table per tab\nWith spreadsheets, it is tempting to add additional summary tables in the same tab you may be working in, or paste another table into the sheet your are working so you can calculate something using a formula. When these data need to be read in by another program (e.g., R), a computer program cannot interpret the visual space our eyes may see between tables as an indication that these tables are separate. Computers are literal, and thus every sheet or tab will always be interpreted as one table by the computer. Follow tidy data principles and keep one table for every sheet. It makes import and export simpler, and in reality tabs are cheap (as are columns). When in doubt, keep things separate and clearly delineated in a way that is both computer readable and human readable. This includes figures…resist the temptation to include figures in a spreadsheet with a table. Keep them separate!\nDon’t Merge or Color Data\nPlease don’t merge cells. Just don’t. It creates many issues that are outlined above, and generally requires significant data wrangling to “untangle” merged data. Merging cells is generally reserved for creating a final table in a word document for display purposes only.\nAnd while using colors can be fun, remember these data are only human readable. Add a new column variable to encode these data. It’s rare that you will run out of space for new columns or rows, but common that data encoded visually will be forgotten or misinterpreted.\nMissing Data or NA\nRemember, computers are literal, thus any blank cell will be interpreted as missing data or NA by the computer. Use this to your advantage! It’s preferable to have consistent designation of NA (i.e., blank cells) instead of using things like \"No data\" or -999. These are common ways to encode missing values, but they introduce different problems when trying to analyze or filter the data later. Using a character string like \"No data\" may unintentionally convert all the data to a character class when imported into R when it is actually numeric or integer (see module on data structures). If using a numeric value for NA like -999, the computer will interpret these as numbers, and it may be easy to miss or include these data in downstream analyses.\nTypos happen. Imagine the issues that arise when a number used to signify NA gets mistyped. Now you need to find all -99, -999, and investigate any 99 numbers. Was there a typo or is this data?\nAlthough there is a potential risk to accidentally add a space in a blank cell, the simple and easiest approach for missing or NA data is to leave cells blank. This is generally and consistently interpreted as a missing value by computer programming languages and avoids the need to remember which code was used to signify NA. This also means that a 0 is a 0. Any data that is 0 should be encoded as such, and not left blank.\nPivoting\nWhen we enter data in a spreadsheet, the data is typically oriented in a wide format. This makes entry easier and is generally easier for us to see, enter, and explore the data. Although wide data is more human-friendly for data entry, long data is much better for analysis and visualization. In particular, in R, the {ggplot} package prefers data in a long format, which facilitates easier filtering, grouping, and faceting of the data.\nWide to Long\nLet’s use the ces3results.xlsx CalEnviroscreen data, which we can read in and look at the column names and data types. By default, read_xlsx() reads in the first sheet. See the import and export module for more details.\n\n\nlibrary(readxl)\nces <- read_xlsx(\"data/calenviroscreen/ces3results.xlsx\")\nstr(ces)\n\ntibble [8,035 × 57] (S3: tbl_df/tbl/data.frame)\n $ Census Tract                                       : num [1:8035] 6019001100 6071001600 6019000200 6077000801 6019001500 ...\n $ Total Population                                   : num [1:8035] 3174 6133 3167 6692 2206 ...\n $ California County                                  : chr [1:8035] \"Fresno\" \"San Bernardino\" \"Fresno\" \"San Joaquin\" ...\n $ ZIP                                                : num [1:8035] 93706 91761 93706 95203 93725 ...\n $ Nearby City \n(to help approximate location only): chr [1:8035] \"Fresno\" \"Ontario\" \"Fresno\" \"Stockton\" ...\n $ Longitude                                          : num [1:8035] -120 -118 -120 -121 -120 ...\n $ Latitude                                           : num [1:8035] 36.7 34.1 36.7 37.9 36.7 ...\n $ CES 3.0 Score                                      : num [1:8035] 94.1 90.7 86 82.5 82 ...\n $ CES 3.0 Percentile                                 : num [1:8035] 100 100 100 100 99.9 ...\n $ CES 3.0 \nPercentile Range                       : chr [1:8035] \"95-100% (highest scores)\" \"95-100% (highest scores)\" \"95-100% (highest scores)\" \"95-100% (highest scores)\" ...\n $ SB 535 Disadvantaged Community                     : chr [1:8035] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Ozone                                              : num [1:8035] 0.0649 0.0622 0.0622 0.0462 0.0649 ...\n $ Ozone Pctl                                         : num [1:8035] 98.2 91.1 91.1 53 98.2 ...\n $ PM2.5                                              : num [1:8035] 15.4 13.3 15.4 12.5 15.4 ...\n $ PM2.5 Pctl                                         : num [1:8035] 97.2 93.6 97.2 84 97.2 ...\n $ Diesel PM                                          : num [1:8035] 48.5 38.6 47.4 24.1 18.8 ...\n $ Diesel PM Pctl                                     : num [1:8035] 95.5 92.1 95.4 73.5 58.2 ...\n $ Drinking Water                                     : num [1:8035] 681 905 681 279 1000 ...\n $ Drinking Water Pctl                                : num [1:8035] 80.9 96.1 80.9 29.1 98.6 ...\n $ Pesticides                                         : num [1:8035] 2.75 1.37 3.03 12.93 3518.41 ...\n $ Pesticides Pctl                                    : num [1:8035] 47.8 41.3 48.8 60.6 95.2 ...\n $ Tox. Release                                       : num [1:8035] 18552 7494 12455 2388 21791 ...\n $ Tox. Release Pctl                                  : num [1:8035] 97.5 89 95.4 70 98.2 ...\n $ Traffic                                            : chr [1:8035] \"909.14\" \"782.26\" \"576.52\" \"1305.01\" ...\n $ Traffic Pctl                                       : chr [1:8035] \"62.977816769018702\" \"55.658603835067098\" \"39.002381250783301\" \"78.293019175335303\" ...\n $ Cleanup Sites                                      : num [1:8035] 80.5 66.2 22 50.1 60 ...\n $ Cleanup Sites Pctl                                 : num [1:8035] 98.7 97.7 85.1 96.1 97.2 ...\n $ Groundwater Threats                                : num [1:8035] 45.8 36 30.2 132.1 54.2 ...\n $ Groundwater Threats Pctl                           : num [1:8035] 89.9 85.6 81.9 98.4 92.1 ...\n $ Haz. Waste                                         : num [1:8035] 0.795 1.25 0.2 0.795 13.1 ...\n $ Haz. Waste Pctl                                    : num [1:8035] 84.3 88.8 60.5 84.3 99.7 ...\n $ Imp. Water Bodies                                  : num [1:8035] 0 5 0 19 0 7 14 0 7 0 ...\n $ Imp. Water Bodies Pctl                             : num [1:8035] 0 55 0 98.6 0 ...\n $ Solid Waste                                        : num [1:8035] 21.8 12 2.5 27 50.8 ...\n $ Solid Waste Pctl                                   : num [1:8035] 97.8 92.2 57.2 99.1 99.9 ...\n $ Pollution Burden                                   : num [1:8035] 80 81.2 71.2 74.5 80.2 ...\n $ Pollution Burden Score                             : num [1:8035] 9.85 10 8.76 9.17 9.88 ...\n $ Pollution Burden Pctl                              : num [1:8035] 100 100 99 99.6 100 ...\n $ Asthma                                             : num [1:8035] 131.6 60.7 142.1 142.2 90.5 ...\n $ Asthma Pctl                                        : num [1:8035] 97.7 69.8 98.3 98.3 89.5 ...\n $ Low Birth Weight                                   : chr [1:8035] \"7.44\" \"7.04\" \"10.16\" \"6.23\" ...\n $ Low Birth Weight Pctl                              : chr [1:8035] \"93.835704216327102\" \"90.849673202614397\" \"99.7821350762527\" \"80.648468537741905\" ...\n $ Cardiovascular Disease                             : num [1:8035] 14.1 12.9 15 14.7 12.8 ...\n $ Cardiovascular Disease Pctl                        : num [1:8035] 96.3 92.7 97.7 97.2 92.4 ...\n $ Education                                          : chr [1:8035] \"53.3\" \"53.3\" \"42.3\" \"40.799999999999997\" ...\n $ Education Pctl                                     : chr [1:8035] \"95.760787282361804\" \"95.760787282361804\" \"89.061317183951502\" \"87.522079232904403\" ...\n $ Linguistic Isolation                               : chr [1:8035] \"16.2\" \"33.4\" \"16.7\" \"15.3\" ...\n $ Linguistic Isolation Pctl                          : chr [1:8035] \"77.509665377949602\" \"96.253832822290406\" \"78.389548060258605\" \"75.136648446873707\" ...\n $ Poverty                                            : num [1:8035] 76.3 72.5 86.8 61.3 66.4 66.4 76.2 74.5 75.7 83.4 ...\n $ Poverty Pctl                                       : num [1:8035] 97.1 94.6 99.6 85.6 90.2 ...\n $ Unemployment                                       : chr [1:8035] \"17.600000000000001\" \"12.3\" \"16.100000000000001\" \"19.600000000000001\" ...\n $ Unemployment Pctl                                  : chr [1:8035] \"91.724838177433696\" \"71.823835512120795\" \"87.980708211701995\" \"94.973981469729694\" ...\n $ Housing Burden                                     : chr [1:8035] \"26\" \"34.1\" \"40.1\" \"21.1\" ...\n $ Housing Burden Pctl                                : chr [1:8035] \"79.398324447829395\" \"93.754760091393706\" \"97.854785478547896\" \"63.544046712363503\" ...\n $ Pop. Char.                                         : num [1:8035] 92.1 87.4 94.6 86.7 80.1 ...\n $ Pop. Char. Score                                   : num [1:8035] 9.55 9.07 9.81 8.99 8.3 ...\n $ Pop. Char. Pctl                                    : num [1:8035] 99.7 98.1 100 97.7 92.8 ...\n\nNote the different data classes (num, chr, etc). Even though all columns between Ozone and Pop. Char. Pctl should be numeric, there are a number of “chr” or character data. This is because one of the issues with messy spreadsheets we discussed above. If we were to open the spreadsheet and scroll down through these columns, we may notice there are NA’s sprinkled throughout. When importing these data into R, those NA are interpreted as character class, and thus the entire column of numeric data is converted to a character class.\nWe know these data should all be numeric, so let’s fix this using some dplyr knowledge and a cool function called across(). This allows us to apply a function over many columns in a dataset, and it can be used in mutate() or summarize().\nNote, we need to use backticks for `column names` because there are spaces and characters in our column names that aren’t permitted by default in R. Tidy column names are helpful for this. See the excellent {janitor} package for more info.\n\n\nces <- ces %>% \n  mutate(across(c(`Ozone`:`Pop. Char. Pctl`), as.numeric))\n\n\n\n\n\n\nFigure 4: Illustration by @allison_horst\n\n\n\nThe warnings are ok! They are saying NAs introduced by coercion which essentially means when converting these data from character to numeric, the NA text is being converted to an actual NA that R can use. Ok, now let’s look at the size of this dataset.\n\n\nncol(ces) # number of columns\n\n[1] 57\n\nnrow(ces) # number of rows\n\n[1] 8035\n\nSo, if we want to reformat or pivot this data from a wide format (there are 57 columns, many of which are distinct variables measured as part of the CES calculation), we want to retain certain columns and collapse others. Let’s focus on collapsing all the columns that are used to calculate the CES scores into a single column. So from Ozone:Pop. Char. Pctl, we should have one column we’ll call CES_variables which will be all the various variable names from these columns, and a CES_value which will be the value associated with each of the columns we are collapsing or combining. We need the {tidyr} package for this, which is loaded as part of the {tidyverse}.\n\n\nlibrary(tidyr)\nces_long <- pivot_longer(data = ces,\n                         # the columns we want to pivot\n                         cols = c(`Ozone`:`Pop. Char. Pctl`),\n                         # name of column we create for pivoted col names above\n                         names_to = \"CES_variables\",\n                         # name of column we create for pivoted values\n                         values_to = \"CES_values\")\n                          \ndim(ces_long)\n\n[1] 369610     13\n\nstr(ces_long)\n\ntibble [369,610 × 13] (S3: tbl_df/tbl/data.frame)\n $ Census Tract                                       : num [1:369610] 6019001100 6019001100 6019001100 6019001100 6019001100 ...\n $ Total Population                                   : num [1:369610] 3174 3174 3174 3174 3174 ...\n $ California County                                  : chr [1:369610] \"Fresno\" \"Fresno\" \"Fresno\" \"Fresno\" ...\n $ ZIP                                                : num [1:369610] 93706 93706 93706 93706 93706 ...\n $ Nearby City \n(to help approximate location only): chr [1:369610] \"Fresno\" \"Fresno\" \"Fresno\" \"Fresno\" ...\n $ Longitude                                          : num [1:369610] -120 -120 -120 -120 -120 ...\n $ Latitude                                           : num [1:369610] 36.7 36.7 36.7 36.7 36.7 ...\n $ CES 3.0 Score                                      : num [1:369610] 94.1 94.1 94.1 94.1 94.1 ...\n $ CES 3.0 Percentile                                 : num [1:369610] 100 100 100 100 100 100 100 100 100 100 ...\n $ CES 3.0 \nPercentile Range                       : chr [1:369610] \"95-100% (highest scores)\" \"95-100% (highest scores)\" \"95-100% (highest scores)\" \"95-100% (highest scores)\" ...\n $ SB 535 Disadvantaged Community                     : chr [1:369610] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ CES_variables                                      : chr [1:369610] \"Ozone\" \"Ozone Pctl\" \"PM2.5\" \"PM2.5 Pctl\" ...\n $ CES_values                                         : num [1:369610] 0.0649 98.1829 15.4 97.2181 48.5238 ...\n\nWow! We just condensed our dataset from wide to long by collapsing 46 variables into a single column. That means there is some duplication in the data, but these data will be much easier to work with in {ggplot} or other downstream analyses in R. We can double check this worked as expected, because the original wide dataset had 8035 observations, so if we multiply 8035 * 46 variables, we should get the total number of observations in the ces_long dataset (n=369610).\nLong to Wide\nIf we want to do the opposite, and provide these data for folks to add additional variables or enter data, we can use pivot_wider(). Similar to pivot_longer(), we need to specify our columns of interest for the names, and column of interest for the values.\n\n\nces_wide <- pivot_wider(data = ces_long,\n                        names_from = \"CES_variables\",\n                        values_from = \"CES_values\"\n                        )\n\n\nThere are many additional options within the {tidyr} pivot_ functions. We recommend further reading on pivoting at R4DS and the {tidyr} pivoting vignette, as well as checking out the separate() and unite() functions from the same package.\nHappy wrangling!\n\n\n\n\nFigure 5: Illustrations from the Openscapes blog Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst\n\n\n\n\n\nPrevious module: 7. Data Wrangling\nNext module: 9. Functions\n\n White, E. P., E. Baldridge, Z. T. Brym, K. J. Locey, D. J. McGlinn, and S. R. Supp. 2013. Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution 6. ↩︎\n Broman, K. W., and K. H. Woo. 2018. Data Organization in Spreadsheets. The American Statistician 72:2–10 ↩︎\nSee the ISO 8601 standard.↩︎\n",
      "last_modified": "2024-01-21T12:59:22-08:00"
    },
    {
      "path": "m_project_management.html",
      "title": "3. Project Management: Let's Get Organized",
      "description": "Know where you are...\n",
      "author": [],
      "contents": "\n\nContents\nWhere Am I?\nUse Project Workflows\nBasic Folders for Every Project\nRStudio Projects\nSetting up an .Rproj\nMake Folders & Add Data\n\nFile Paths\nThe Working Directory\nUsing the {here} package\n\nBest Practices: Project Organization/Workflow Tips\nAlways start R as a blank slate\nSafe File Naming\nTreat raw data as Read Only\nTreat generated output as disposable\n\nSummary\n\n\nLearning objectives\nUnderstand motivation for project based workflows\nKnow how to organize code, data, and results\nKnow the basics of file paths and directory structures\nBe able to create and use an RStudio project\n\nWhere Am I?\n\n\n\nFigure 1: A disorganized system for finding things\n\n\n\nAny time you are working on your computer, you must navigate amidst a forest of files and folders.\n\nOne of the most common issues we deal with in R programming is to tell your computer where a file or folder is located.\n\nThus, one of the most useful habits you can form (whether using R or not), is to intentionally keep a clear structure and use that same structure across all of your projects. This becomes especially important when using computer programming languages like  in your work. You will need to tell R, very specifically, where you are and where your files are located in the forest of your computer.\nWhere you are is typically referred to as the working directory. The working directory is a folder. In , think of this as your homebase, and everything is relative to this folder on your computer.\nUse Project Workflows\nOne of the great advantages of using tools like RStudio is they make it easy to use an organized “project” workflow. What do we mean by “using projects”? Projects refer to a general pattern or structure used for each work project or analysis. This approach is not specific to R. Any practiced data scientist will use a folder structure and organization scheme, no matter what programming language they use.\nThe general idea is to always use the same structure and naming schemes across every project. Do this every single time with every single project you make, in order to make it a habit. This will save you time and brainpower! Imagine quickly moving between tasks or projects with minimal time spent “trying to find where things are and getting oriented”. You’ll always know where things should be!\nBasic Folders for Every Project\nAt a minimum, it’s useful to have separate directories (folders) for each of the following:\ndata: Ideally keep data in a .csv format, because these simple and universal data. You may have other specialized formats as well. This is generally where original, raw data lives.\ndata_output: This is where you save any data or analysis outputs. Any time you clean, tidy, summarize, or otherwise manipulate the data and save it out, it should end up somewhere clearly different than the raw data location.\nscripts: In the  world, “scripts” are generally .R files. However, you may have .do files if wokring with Stata, .py files for Python, and so on. Using a sequential numbering file naming scheme can be useful. Remember to pad with a zero to make file sorting/ordering easy (e.g., 00_script_a.R, 01_script_b.py, etc).\nresults: These may include model results, data analysis, slides, and so on. Some like to keep figures in this folder and others prefer a specific figures folder.\ndocuments: This is a place you can keep documents, papers, pdfs, etc. These may include files with .docx, .Rmd (for RMarkdown), .pdf or even .html extensions.\n\n\n\nFigure 2: An example project folder structure\n\n\n\nRStudio Projects\nWithin the R environment, something that makes project management and organization much easier is the use of RStudio Projects (.Rproj file). RStudio makes the use of Projects straightforward. One of the nicest parts of using RStudio Projects is that they automatically set the working directory to the folder containing a specified .RProj file. You can make any existing folder an RProject folder, or make a new one. RStudio projects are a great way to quickly and easily organize your workflows; each project can be a specific task, or a larger set of objectives you need to complete. With RStudio Projects, it’s easy to switch between projects, or work on different projects simultaneously without much mental overload, especially if you use the same project folder structure across each project!\n\nAvoid using setwd()! More reasons and rationale linked here.\nSetting up an .Rproj\nLet’s create an RStudio Project as part of this course that you can use throughout each module. We highly recommend that no matter what you are working on in R, that you try to do it within the structure of an RStudio Project!\nOpen RStudio and navigate to the upper right-hand side where it says “Project: (None)”. If we click on this button, we should see some options. Select New Project.\nWe can use either a New Directory, or setup a project in an Existing Directory. Both give similar options.\nSelect “New Project”\n\n\n\nFigure 3: RStudio Project setup\n\n\n\nMake a new sub-directory folder (if you don’t have it) under your Documents folder called: Rprojects\n\nIt’s a good idea to keep all your R projects in one place. You can decide if this should be in your ~/Documents folder, or somewhere else (associated with a version control or cloud service like Dropbox/Box/etc).\nFinally, we can name our new project (remember no spaces in our folder/file names!): intro_r4wrds\n\n\n\nFigure 4: Make a new project named intro_r4wrds\n\n\n\nMake Folders & Add Data\nGreat! Now we can create a folder structure in our project as discussed in the Basic Folders for Every Project section above. More importantly, we can also put all our course data into the data folder in your RStudio project.\n\nWe can’t drag and drop files from an Explorer or Finder window directly into RStudio. We can only go from Explorer to Explorer. In the Files tab, click on More with the little , and then select “Show Folder in New Window”.\nLet’s go ahead and make the following folders (HINT: you can use the “New Folder” button in RStudio, or use the code below!)\ndata\ndata_output\nfigures\nscripts\n\n\n\nWe can download data directly via the download.file() function.\nIn your Console tab, you can enter this code to download the data into your data folder!\n\n\n# downloads the file (data.zip) to your data directory\ndownload.file(\"https://github.com/r4wrds/r4wrds-data-intro/raw/main/data.zip\", \n              destfile = \"data/data.zip\")\n\n# unzip the datasets for the course:\nunzip(zipfile = \"data/data.zip\")\n\n\nFile Paths\nIn R, file paths are always wrapped in quotes. There are 2 basic kinds of file paths:\nAbsolute: Absolute paths list out the full file path, usually starting with your username, which you can also refer to using the shortcut ~. So instead of C:/MyName/Documents or /Users/MyName/Documents, you can type ~/Documents. But generally, the only place an absolute path will work is your computer! It will break on anyone else computer, or anytime you move or rename something!\nRelative: Relative paths are relative to your working directory. So if R thinks we’re in that ~/MyName/Documents/Projects/2020 folder, and we want to access a file in a folder inside it called data, we can type data/my_file instead of ~MyName/Documents/Projects/2020/data.\nThe good news is if you setup an RStudio project as shown above, you can use relative filepaths with ease!\nThe Working Directory\nOur working directory should be the root place where our project lives (as shown above). You can always check what R thinks is the working directory with getwd(), and you should see the folder where your .Rproj file lives! Importantly, everything you do should be relative to that working directory.\nThat means we really don’t want to use things like setwd() (set working directory) to locate a file or folder on our computer, or use a hard path (i.e., a full path like C:/MyUserName/My_Documents/A_Folder_You_May_Have/But_This_One_You_Definitely_Dont/). That’s because this will pretty much never work on anyone’s computer other than your own, and sometimes it may not even work on your own computer if you change a file name or folder! How can we make things reproducible for others, and for our future self? We advise using the {here} package.\nUsing the {here} package\nGood news! There’s a package designed to make project-based workflows work so that code is portable between machines (i.e., you can share your project with a collaborator and they don’t need to fiddle with working directories and file paths). The {here} package makes it easy to create a path relative to the top-level directory (the place where your current project is) with a single function: here(). In addition, we can use here() to build a relative path to a file for saving or loading. Let’s say we’re working in our MyName/Documents/Projects/2020 folder.\n\n\nlibrary(here)\n\n# identify your working directory.\nhere()\n\n# will be something like this:\n#> [1] /Users/MyName/Documents/Projects/MyProject\n\n# read a file from our data folder! `data/nwis_sites_american_river.csv`\nread.csv(here(\"data\", \"nwis_sites_american_river.csv\"))\n\n\n\n\n\nFigure 5: Illustration by @allison_horst.\n\n\n\nUsing the {here} package means we can share our project with other folks and it will work, and if something changes around inside the project, it will remain functional and accessible.\nIn practice however, if we always work from within an RProject, we can simply use relative paths without the here() syntax – that syntax just makes it very explicit what we’re doing.\nBest Practices: Project Organization/Workflow Tips\nAlthough there is no “best” way to lay out a project, there are some general principles to adhere to that will make project management easier. Here’s some sage advice from Jenny Bryan and Jim Hester from What They Forgot to Teach you About R (worth checking out!):\nFile system discipline: put all the files related to a single project in a designated folder.\nThis applies to data, code, figures, notes, etc.\nDepending on project complexity, you might enforce further organization into subfolders.\nUse a standard naming convention for files & folders (no spaces!).\nDon’t use absolute paths!\nFile path discipline: all paths are relative and, by default, relative to the project’s (your working directory) folder.\nAlways start R as a blank slate\nWhen you quit R, do not save the workspace to an .Rdata file. When you launch, do not reload the workspace from an .Rdata file.\nIn fact, we should all make our default setting a blank slate. We should only be loading and working on data and code that we knowingly and willingly open or import into R.\n\nIn RStudio, set this via Tools > Global Options\n\n\n\n\n\nFigure 6: Change defaults to never save your workspace to .RData! (Credit to Jenny Bryan and Jim Hester at rstats.wtf)\n\n\n\nSafe File Naming\nThis is really important and will make life easier for everyone in the long run. Jenny Bryan has the best set of slides on this, so take a few minutes and go read them. Then be the change!\n\nSlides You Need to Read\n\nTL&DR (Too long, didn’t read)\nFile names should be machine readable (i.e., no spaces)\nHuman readable (m_import_clean_data.R)\nMakes default ordering easy (i.e., dates are always YYYY-MM-DD)\nTreat raw data as Read Only\nThis is probably the most important tip for making a project reproducible and hassle free. Raw data should never be edited, because you don’t want to permanently change your starting point in an analysis, and you want to have a record of any changes you make to data. Therefore, treat your raw data as “read only”, perhaps even making a raw_data directory that is never modified. If you do some data cleaning or modification, save the modified file separate from the raw data, and ideally keep all the modifying actions in a script so that you can review and revise them as needed in the future.\nTreat generated output as disposable\nAnything generated by your scripts should be treated as disposable: it should all be able to be regenerated with code. Don’t get attached to anything other than your raw data, and your code! There are lots of different ways to manage this output, and what’s best may depend on the particular kind of project.\nSummary\nThere are many ways to use project-based workflows in R. Importantly, we advise that you find a strategy that works for the majority of the work you do, and then be consistent about organizing things the same way (from folder structure to file naming). This will help folks you work with, it will help you (including your future self), and you spend less time figuring out where you are (and where the pieces you need to do work with are) and more time doing the work you need to do!\nLesson adapted from R-DAVIS, Jenny Bryan and Jim Hester’s What they forgot to teach you about R, and the Data Carpentry: R for data analysis and visualization of Ecological Data lessons.\n\n\nPrevious module: 2. Getting started\nNext module: 4. Import/export data\n\n\n\n",
      "last_modified": "2024-01-21T12:17:11-08:00"
    },
    {
      "path": "m_troubleshooting.html",
      "title": "14. Strategies for Troubleshooting",
      "description": "How to get unstuck... because we all get stuck at times\n",
      "author": [],
      "contents": "\n\nContents\nError messages\nClass\nUsing built-in help\nReading R help documentation\nMinimal reproducible example\nSearching for answers and asking questions\nGoogle\ncommunity.RStudio\nStackOverflow\n\n\n\nLearning objectives\nInterpreting error messages\nUnderstand classes\nUsing built-in help\nReading R help documentation\nMinimal, reproducible example\nHow to search for answers and ask questions\n\nError messages\n\n\n\n\nFigure 1: Jenny Bryan’s rstudio::conf(2020) talk is a comical and educational deep dive into debugging.\n\n\n\n\nInterpreting error messages is a normal part of working with R. Getting errors is expected, and no amount of practice will prevent errors from happening. What practice will change however, is your ability to diagnose and address errors. In this module, we cover a few seasoned approaches to “get unstuck” when you encounter errors in R.\n\n\n\nFigure 2: The fabled error message: ‘object of type 'closure' is not subsettable’. This hard-to-interpret error arises when we try to subset an object that can’t be subset. In this example, the function mean() doesn’t have an index [1, ] so we get this opaque error.\n\n\n\nClass\nIn R, every object has a class, which defines the operations we can perform on it. We can check the class of an object with the class() function.\n\nRecall the classes of atomic vectors (logical, numeric, factor, character) from the module on data structures.\n\n\nlibrary(tidyverse)\n\n# check the class of the function, \"mean\"\nclass(mean)\n\n\n[1] \"function\"\n\n# check the class of other objects we've seen before\nclass(1:10)\n\n\n[1] \"integer\"\n\nclass(\"a string\")\n\n\n[1] \"character\"\n\nclass(tibble(x = 1:10, y = \"a string\"))\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nKnowing the class of the objects you’re working with can help orient questions you may ask online, and will also help you interpret built-in documentation.\nUsing built-in help\nOne of the best features about R is the standardized package documentation. Once you learn how to interpret package documentation, you’ll be able to read about functions in any package and learn what arguments they take, the class of arguments they expect (incorrect objects passed to functions is a common error), and the object and object class they output.\nTo illustrate, below is an example of an error that we can diagnose by consulting the help for ?mean.\n\n\nnumbers <- c(\"10\", \"50\", 100)\nmean(numbers)\n\n\n\n\n\n\n\n\n\nThe help for mean (enter ?mean in the console) indicates that the Argument x only has methods for numeric/logical vectors, and date, date-time, and time interval objects.\n\n\n\nWhat is the class of our object numbers?\n\n\nclass(numbers)\n\n\n[1] \"character\"\n\nWe tried passing a character string to mean() but there are no methods defined for strings, which led to an error. By checking the help and the class of the object, we were able to diagnose this problem. The fix is to change the class of the object to a numeric vector, which mean() supports according to the help documentation.\n\n\n# change the class of the numbers object from character to numeric\nnumbers <- as.numeric(numbers)\n\n# take the mean - this now works!\nmean(numbers)\n\n\n[1] 53.33333\n\nReading R help documentation\nR help documentation is standardized, and thus predictable. Let’s cover the anatomy of R help documentation, discussing each of the key sections and how to interpret them. As before, below is the help documentation for the function ?mean:\n\n\n\n\nOnce you understand this predictable structure of help documentation, you can interpret the documentation for new functions and expand your R vocabulary! This structure is the same across different R packages. From top to bottom, these sections can be interpreted as follows:\n\nlabel\ninterpretation\nmean {base}\nFunction name {package}\nArithmetic Mean\nShort function description\nDescription\nMore detailed function description\nUsage\nFunction usage, showing the name and order of arguments\nArguments\nDescription of the arguments taken by the function\nValue\nDescription of what the function returns\nReferences\nCitation or further documentation associated with the function\nSee Also\nRelated functions\nExamples\nWorking examples to copy and paste that demonstrate the function in action\n\nAlthough the table to the left is based on the help for a function, some packages ship with objects like data.frames that also have documentation. These objects don’t have Arguments or a Value, since unlike functions, they don’t take inputs and create outputs. For example, see the built-in dataset ?rivers.\n\nChallenge 1\nRead the help for the ggplot function from the {ggplot2} package.\nHint: try using ?ggplot2::ggplot. Here the “::” allows us to specify a {package}::{function}\n\nRead the help for two functions from {dplyr}: mutate and filter. Are there other packages with a filter function? How do you know?\nRead the help for two other functions you’ve encountered in this course.\n\n\nMinimal reproducible example\nMore often than not, errors you encounter will not be the result of using the mean() function incorrectly, but due to unwieldy, large, and complicated data processing steps strung together. Sometimes, the sheer amount of data you want to process may be so large that it takes minutes until your code finally errors, and it becomes time-consuming to debug.\n\nThere’s even an R package designed to help R users create reprexes. Many people have weighed in on the best way to create a reprex, including the developer of many of the R packages you’ve used in this course, who shares their thoughts here.\nA tried and true approach to solving errors is to create a minimal, reproducible example, or a reprex for short.\nAccording to Hadley Wickham,\n\nThere are four things you need to include to make your example reproducible: required packages, data, code, and a description of your R environment.\n\nThus, we can think of a reprex in terms of:\nPackages: loads all needed packages at the top of the script.\nData: Uses a minimal dataset to capture the scope of the problem, so that the code solution can scale to the entire dataset later. Uses a dataset that everyone has access to. If not using a built-in dataset like mtcars, use dput() to “dump” your data into a form that can be copy/pasted and assigned to a variable.\nCode: Concisely describes the expected output and shares a minimal code example highlighting the error using the minimal data.\nEnvironment: If you’re sharing the reprex with others, run SessionInfo() in the console and copy/paste this into your reprex. Some errors can be diagnosed by looking at this information.\nYou may create your reprex with the intention of sharing it with others, but find that in creating it, you actually solve your problem! That is not uncommon. The exercise of boiling a problem down to its most essential parts often sparks new ways of approaching a problem and seeing it for what it really is.\nSearching for answers and asking questions\nSometimes, knowing how to check object classes, read help documentation, and abstract your code’s error into a minimal, reproducible example just isn’t enough to solve the problem. Even the best programmers reach this point, and when you too reach this point, you’ll probably turn to the internet for answers. Here we present some guidance on how and where to ask questions and seek answers.\n\n\n\nFigure 3: Learning to code doesn’t need to be a solitary experience. Online and in-real-life comunities of practice can support you on your jouRney. Artwork by @allison_horst\n\n\n\nGoogle\nSay you encountered this error:\n\n\n\n\n\n\nFigure 4: Embrace the error message! It may even tell you something useful. Artwork by @allison_horst\n\n\n\nWhen you use Google as a jumping off point to search for R help, follow this formula:\n\n[R] + [package name] + [object type] + [error]\n\n\n\n\nFigure 5: Authoratitive answers to R programming errors are out there in the wild, and these webpages will usually have the following information: R, package name, object type, error message.\n\n\n\nYou may find posts on any number of sites including community.RStudio, Stack Overflow, listservs, package documentation, blog posts, or otherwise.\nIf you’re having an error, chances are someone else has had the same problem before and you can solve your problem just by asking the right question. Take note of the date that a post was written, and whenever possible, try to find and read more recent answers first. Sometimes there may be answers available but because they were written many years ago, they are applicable to much older versions of R, or out-of-date packages and functions.\nIf you’re still unable to find an answer by reading, the next thing to do is create a reprex and ask a question. When doing this, we recommend two places to ask questions.\ncommunity.RStudio\ncommunity.rstudio.com is a beginner-friendly, safe space to ask questions and learn. Unlike Stack Exchange Sites (e.g., Stack Overflow, Cross Validated, and GIS Stack Exchange), community.RStudio is exclusively for R-based help.\nIt’s free to create a user account and post questions in the forum. Present your problem in the form of a reprex – this will enable others to help you, and is the form of question most likely to receive help. People answering questions on the site generally want to help. Your job is to make it as easy as possible for them to copy/paste your code into their environment so they can take a crack at your error.\n\n\n\nFigure 6: community.rtudio.com is an online question and answer site for R questions.\n\n\n\nStackOverflow\nStack Overflow is an online question and answer site for programming questions of all kinds – you can find questions and answers in just about every language, and its extensive use makes it the de facto hub for finding old posts that may address your error. Like community.RStudio, you can sign up for a free account to ask and answer questions.\nStack Overflow can be a more intimidating place to ask questions, especially because the structure of the site allows users to upvote and downvote questions. Beginners who don’t ask questions in the form of a reprex are often downvoted, and this can feel discouraging.\nUpvotes and downvotes influence a user’s “Reputation” and over time, users can accumulate “Reputation” points to remove ads, unlock moderation privileges, and even put bounties on questions to draw more attention to them.\nDon’t be afraid of Stack Overflow. The majority of people who frequent the site are genuinely curious people who like solving coding problems, and it’s a great way to share a tough problem you’re stuck on and have people across the world try to solve it. The instructors of this course have learned so much from spending time on Stack Overflow.\nWhen posting a question on Stack Overflow, follow these tips to ensure a positive response:\nDo research to make sure the same question hasn’t already been asked, otherwise, it may be downvoted or closed.\nIf your question is novel, be sure to ask it in the form of a reprex to enable others to actually work on your problem. This is the expectation from more senior site users.\nThank others for expressing interest in your question by upvoting helpful answers and comments.\nIf a legitimate answer to your problem is posted, thank the person by “Accepting” their answer with the green check mark. This rewards the person with “Reputation” points for solving your problem, it rewards you with “Reputation” points too, and it directs the attention of future visitors to the page to the “Accepted solution”.\n\n\n\nStack Overflow is for general programming questions, but you may find that your question is more statistical or geospatial in nature. In these cases, associated Stack Exchange sites for statistics (Cross validated), geospatial (GIS Stack Exchange), and other topics are great places to seek out domain-specific expertise.\n\n\nPrevious module: 13. RMarkdown\n\n\n\n",
      "last_modified": "2022-05-11T22:12:53-07:00"
    }
  ],
  "collections": []
}

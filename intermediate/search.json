{
  "articles": [
    {
      "path": "index.html",
      "title": "R4WRDS Intermediate Course",
      "description": "Increasing your efficiency with reproducible workflows in R\n",
      "author": [],
      "contents": "\n\nWho is this course for?\nWhat makes an Intermediate R user? This course is most relevant and targeted at folks who:\nTook the Introductory R4WRDS course\nRegularly use R and want improve their efficiency and skill set\nHave a general understanding and proficiency in using {dplyr}, {ggplot2}, {sf}, and {rmarkdown}\nUnderstand (and use) general best practices for data science in R\n\nWhy R?\nR is an open-source language for statistical computing and a general purpose programming language. It is one of the primary languages used for data science, modeling, and visualization.\n\nWhat will you learn?\nIn this course, we will move more quickly, assume familiarity with basic R skills, and also assume that the participant has working experience with more complex workflows, operations, and code-bases. Each module in this course functions as a “stand-alone” lesson, and can be read linearly, or out of order according to your needs and interests. Each module doesn’t necessarily require familiarity with the previous module.\nThis course emphasizes:\nIntermediate scripting skills like iteration, functional programming, writing functions, and controlling project workflows for better reproducibility and efficiency\nApproaches to working with more complex data structures like lists and timeseries data\nThe fundamentals of building Shiny Apps\nPulling water resources data from APIs\nIntermediate mapmaking and spatial data processing\nIntegrating version control in projects with git\n\n\n\nFigure 1: Artwork by @allison_horst\n\n\n\n\nCourse Modules\nCheck/Update R/RStudio and Packages\nVersion Control with git\nProject Management and workflows\nInteractive visualization\nSimple shiny\nIteration and functional programming\nParameterized reports\nAdvanced spatial R and mapmaking: I\nAdvanced spatial R and mapmaking: II\n\n\nProject Setup and Data\nAll data used in this course is expected to live in a /data subfolder in a project directory.\nWe will be working in an R project using RStudio. If you’ve already downloaded zipped data and Rproj file from the introductory course, you’re already set and can move on to the modules.\nCreate a New Project\nIf you want to set up your own new RStudio project (highly recommend for experience!), we can create a new project file (intermediate_r4wrds.Rproj), in a few different ways. Directly from RStudio (detailed in the introductory project management module), or via the command line. We can use touch intermediate_r4wrds.Rproj (MacOS/Linux) or echo > intermediate_r4wrds.Rproj (Windows) in the root project directory.\nTo complete code exercises and follow along in the course, you will create a /data subfolder, and a /scripts subfolder to store .R scripts, which we recommend naming by module.\nYour project directory structure should look like this (note the position of the /data subfolder):\n.\n├── scripts\n│   ├── module_01.R\n│   └── module_02.R\n│   └── ...\n├── data\n│   ├── gwl.csv\n│   └── polygon.shp\n│   └── ...\n└── intermediate_r4wrds.Rproj\nDownload Data\nOnce an RStudio project has been created we can download the data in in a few ways:\nDownload to a data folder in your project from a Github repository:\n\n\n    # downloads the data.zip file to the `data` directory\n    dir.create(\"data\")\n    download.file(\"https://github.com/r4wrds/r4wrds-data-intro/raw/main/data.zip\", destfile = \"data/data.zip\")\n\n    # unzip the data:\n    unzip(zipfile = \"data/data.zip\")\n\n    # if get resulting __MACOSX folder (artifact of unzip), remove:\n    unlink(\"__MACOSX\", recursive = TRUE)\n\n\nDownloaded and unzipped from OSF\nOnce data have been downloaded and moved to a data folder, or downloaded directly into the project, we are ready to roll!\n\nWorkshop Overview\nWe will follow the SFS Code of Conduct throughout our workshop.\n\nSource content\nAll source materials for this website can be accessed at the r4wrds Github repository.\n\nAttribution\nContent in these lessons has been modified and/or adapted from Data Carpentry: R for data analysis and visualization of Ecological Data, the USGS-R training curriculum here, the NCEAS Open Science for Synthesis workshop here, Mapping in R, and the wonderful text R for data science.\n\n\nNext module:1. Updating R\nsite last updated: 2025-05-08 18:25\n\n\n\n",
      "last_modified": "2025-05-08T18:25:58-07:00"
    },
    {
      "path": "m_advanced_spatial_2.html",
      "title": "9. Advanced spatial R and mapmaking: II",
      "description": "From 1,000 point-clicks to 1 script...\n",
      "author": [],
      "contents": "\n\nContents\nOverview\nThe Packages\nRiver Data: Find Nearest\nUSGS Station\nGroundwater\nData\nUse\nfindNLDI\nExtract the NLDI Info\nSnap to the Nearest Point\nSelect Nearest by Distance\nDownload USGS Data with\nNLDI\nPlot our\nUSGS Data\n\n\nMake a\nMap with {tmap}\n\n\nLearning objectives\nLearn to extend and use {sf} with aquatic data from NHD\nsources\nMapping with {tmap}\nAdvanced spatial operations with vector data\n\nOverview\nWe can use some of the same tools and data we used previously, but we\ncan now also add a few options to download and actively use USGS/NHD\ndata. These data include river stage and flow, water quality, station\nlocations, watershed and streamline attributes, etc. Particularly for\nwater scientists, it can be immensely useful to tie in additional data,\nor use standard datasets that are continuously updated (like the USGS\ngage network).\nThe Packages\nFirst we’ll need a few packages we’ve not used yet. Please install\nthese with install.packages() if you don’t have them.\n\n\n# GENERAL PACKAGES\nlibrary(tidyverse) # data wrangling & viz\nlibrary(purrr) # iteration\nlibrary(janitor) # name cleaning\nlibrary(glue) # pasting stuff together\n\n# SPATIAL PACKAGES\nlibrary(sf) # analysis tools\nlibrary(mapview)  # interactive maps!\nmapviewOptions(fgb = FALSE) # to save interactive maps\n\nlibrary(tigris) # county & state boundaries\nlibrary(units) # for convert spatial and other units\nlibrary(dataRetrieval) # download USGS river data\nlibrary(tmap) # mapping\nlibrary(tmaptools) # mapping tools\n\n\n\nRiver Data: Find Nearest\nUSGS Station\nWe’ve demonstrated how to join data and crop data in R, but let’s use\nsome alternate options to download new data. We’ll focus on surface\nwater here, and look at how we can download and map flowlines in R. Much\nof the USGS data network can be queried and downloaded in\nR. This may include data on water quality, river discharge,\nwater temperature, spatial basins, and NHD flowlines. The\n{dataRetrieval} package is an excellent option for these\noperations.\nLet’s find a few groundwater stations to use. Here we’ll grab one\nclose to the American River and one close to the Cosumnes just for\ndemonstration purposes, but this could be any X/Y point, or set of\npoints you are interested in.\nNote, the X/Y points don’t need to fall on a river! The function will\nlook for the closest river segment and pull data based on that\npairing.\nGroundwater Data\nIteration…remember {purrr}? Let’s use it here!\n\n\n# get counties for CA as {sf}\nca_co <- counties(\"CA\", cb = TRUE, progress_bar = FALSE)\n\n\n# read the stations\ngw_files <- list.files(path = \"data/gwl/county\",\n                       full.names = TRUE, pattern = \"*.csv\")\n\n# read all files into dataframes and combine with purrr\ngw_df <- map_df(gw_files, ~read.csv(.x))\n\n# the readr package will also do this same thing by default\n# when passed a list of files with the same data types\ngw_df <- read_csv(gw_files)\n\n# now make \"spatial\" as sf objects\ngw_df <- st_as_sf(gw_df, coords=c(\"LONGITUDE\",\"LATITUDE\"), \n                  crs=4326, remove=FALSE) %>% \n  # and transform!\n  st_transform(., 4269)\n\n\n\nGet the Sacramento County shapefile.\n\n\n# get just sacramento: here we read in a shapefile:\nsac_co <- st_read(\"data/shp/sac/sac_county.shp\")\n\n\nReading layer `sac_county' from data source \n  `/Users/richpauloo/Documents/GitHub/r4wrds/intermediate/data/shp/sac/sac_county.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -13565710 ymin: 4582007 xmax: -13472670 ymax: 4683976\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# check CRS\nst_crs(sac_co)$epsg\n\n\n[1] 3857\n\n# match with other data\nsac_co <- st_transform(sac_co, st_crs(ca_co))\nst_crs(sac_co) == st_crs(ca_co) # should be TRUE!\n\n\n[1] TRUE\n\n# make a box around sacramento county\n# (a grid with an n=1) for inset\nsac_box <- st_make_grid(sac_co, n = 1)\n\n\n\nNow we can filter to our location of interest.\n\n\n# filter to only Sacramento\ngw_sac <- gw_df %>% \n  filter(COUNTY_NAME == \"Sacramento\")\n\n# use this layer to filter to only locations (stations of interest)\nsac_loi <- gw_sac %>% filter(STN_ID %in% c(\"52418\", \"5605\"))\n\nmapview(sac_co, alpha.regions=0, \n          color=\"black\", lwd=3, legend=FALSE) +\n  mapview(sac_loi, col.regions=\"purple\",layer.name=\"Locations of Interest\") \n\n\n\n\nUse findNLDI\nThe findNLDI function allows us to pass a single spatial\npoint as well as a few different parameters like search upstream or\ndownstream, and what we want to find, and then return a list of items\n(see the help page for using the function here),\nleveraging the hydro-network linked data index (NLDI)1.\nNote, we’ll need internet connectivity here for these functions to run\nsuccessfully.\nLet’s look only at mainstem flowlines from our locations of interest,\nand return the nearest NWIS sites as well as the\nNHD flowlines (streamlines). We’ll use the map() function\nto pass a list of stations along (here only 2, but this is flexible, and\nin practice we can map over a much larger number of stations).\nIf there are issues running the findNLDI function via\n{dataRetrieval}, data can be downloaded with:\nload(url(\"https://github.com/r4wrds/r4wrds/raw/main/intermediate/data/dataRetrieval_all_nldi_data.rda\")).\nFirst, we want to look up the COMID or location identifier for the\ncentroids.\n\n\nlibrary(dataRetrieval)\n\n# Need to convert our locations of interest to WGS84\nsac_loi <- sac_loi %>% \n  st_transform(4326)\n\n# now we can go get flowline data!\nus_nwis <- map(sac_loi$geometry,\n                ~findNLDI(location = .x,\n                          nav  = c(\"UM\"), \n                          find = c(\"nwis\", \"flowlines\"),\n                          distance_km = 120)\n                )\n\n\n\nGreat, now we have a list of three or more things for\neach sf object we passed to findNLDI. In this case, we\nshould have (for each location of interest we used):\norigin: this is the segment that the original LOI was\nlinked to based on the nearest distance algorithm the function used.\nNote, this is an sf LINESTRING data.frame.\nUM_nwissite: these are all the NWIS sites that were\nidentified upstream of our origin point.\nUM_flowlines: these are the Upstream Mainstem (UM)\nflowlines from our origin point.\nLet’s play with these data and make some maps.\nExtract the NLDI Info\nThere are a few options, and it depends on what your goal is. Here we\nshow a few simple ways to pull these data out or collapse them.\nRemember, we can access things in lists with our [[]]\ntoo!\n\n\n# we can split these data into separate data frame\n# and add them as objects to the .Global environment.\n# first add names based on our station IDs:\nus_nwis <- set_names(us_nwis, nm=glue(\"id_{sac_loi$STN_ID}\"))\n\n# then split into separate dataframes\n# us_nwis %>% list2env(.GlobalEnv)\n\n# Or we can combine with map_df\nus_flowlines <- map_df(us_nwis, ~rbind(.x$UM_flowlines))\nus_nwissite <- map_df(us_nwis, ~rbind(.x$UM_nwissite))\n\nmapview(sac_loi, col.region=\"purple\", legend = TRUE, \n        cex=3, layer.name=\"GW LOI\") +\n  mapview(us_nwissite, col.regions=\"orange\", \n          legend = TRUE, layer.name=\"UM NWIS Sites\") +\n  mapview(us_flowlines, color=\"steelblue\", lwd=2, \n          layer.name=\"UM Flowlines\", legend=FALSE)\n\n\n\n\nNext, let’s filter to NWIS USGS stations that have flow data\n(generally these have 8-digit identifiers instead of a longer code which\ncan be more water quality parameters), and pull streamflow data for the\nnearest station.\n\n\n# use the stringr package, part of tidyverse to trim characters\nusgs_stations <- us_nwissite %>% \n  filter(stringr::str_count(identifier) < 14)\n\n# double check?\nmapview(sac_loi, col.region=\"purple\", legend = TRUE, \n        cex=3, layer.name=\"GW LOI\") +\n  mapview(us_nwissite, col.regions=\"orange\", cex=2, \n          legend = TRUE, layer.name=\"UM NWIS Sites\") +\n  mapview(usgs_stations, col.regions=\"cyan4\", \n          legend = TRUE, layer.name=\"USGS Gages\") +\n  mapview(us_flowlines, color=\"steelblue\", lwd=2, \n          layer.name=\"UM Flowlines\", legend=FALSE)\n\n\n\n\nSnap to the Nearest Point\nThe final filter involves snapping our LOI points (n = 2) to\nthe nearest USGS stations from the stations we filtered to above. We can\nthen use these data to generate some analysis and exploratory plots.\nSnapping spatial data can be tricky, mainly because decimal precision\ncan cause problems. One solution is to add a slight buffer around points\nor lines to improve successful pairing.\nFor this example, we’ll use st_nearest_feature(), which\ngives us an index of the nearest feature (row) between two sets of\nspatial data. In this case, we have two sets of points.\n\n\n# get row index of nearest feature between points:\nusgs_nearest_index <- st_nearest_feature(sac_loi, usgs_stations)\n\n# now filter using the row index\nusgs_stations_final <- usgs_stations[usgs_nearest_index, ]\n\n# get vector of distances from each ISD station to nearest USGS station\ndist_to_loi <- st_distance(sac_loi, \n                           usgs_stations_final, \n                           by_element = TRUE)\n\n# use units package to convert units to miles or km\n(dist_to_loi_mi <- units::set_units(dist_to_loi, miles))\n\n\nUnits: [miles]\n[1] 13.696934  1.202355\n\n(dist_to_loi_km <- units::set_units(dist_to_loi, km))\n\n\nUnits: [km]\n[1] 22.043079  1.935003\n\n# bind back to final dataset:\nusgs_stations_final <- usgs_stations_final %>% \n  cbind(dist_to_loi_mi, dist_to_loi_km)\n\n# now plot!\nmapview(usgs_stations, cex = 2.75, col.regions = \"gray\",\n        layer.name = \"USGS Stations\") +\n  mapview(us_flowlines, legend = FALSE, color = \"steelblue\") + \n  mapview(usgs_stations_final, col.regions = \"yellow\",\n          layer.name = \"Nearest USGS Station to LOI\") +\n  mapview(sac_loi, col.regions=\"purple\",\n          layer.name = \"GW LOI\")\n\n\n\n\nNotice anything? How could we approach this differently so we pulled\nat least one gage per river instead of two in one river and none in the\nother?\nSelect Nearest by Distance\nIf we want to select more than a single point based on a threshold\ndistance we can use a non-overlapping join and specify a distance. For\nmany spatial operations, using a projected CRS is\nimportant because it generally provides a more accurate calculation\nsince it is based on a “flat” surface and uses a linear grid. It has the\nadditional advantage that we tend to process and understand information\nthat is grid based more easily than curvilinear (degree-based), so a\ndistance of 100 yards or 100 meters makes sense when compared with 0.001\ndegrees.\nTherefore, first we transform our data into a projected CRS, then we\ndo our join and distance calculations, then we can transform back to our\nlat/lon CRS.\n\n\nusgs_stations <- st_transform(usgs_stations, 3310)\nsac_loi <- st_transform(sac_loi, 3310)\n\n# use a search within 15km to select stations\nusgs_stations_15km <- st_join(sac_loi,\n                              usgs_stations,\n                              st_is_within_distance,\n                              dist = 15000) %>% \n  st_drop_geometry() %>%\n  filter(!is.na(X)) %>% # can't have NA's \n  st_as_sf(coords = c(\"X\",\"Y\"), crs = 4326)\n\n\nmapview(usgs_stations_15km,  col.regions = \"yellow\") +\n  mapview(sac_loi, col.regions = \"purple\") +\n  mapview(us_flowlines, legend = FALSE, color = \"steelblue\")\n\n\n\n\nWhy did we use st_drop_geometry()? Sometimes it’s\ncleaner (and faster) to operate on the data.frame without\nany of the spatial data, especially when we have many hundreds or\nthousands of complex spatial data, or we want to create a new\ngeometry.\nDownload USGS Data with NLDI\nNow we have our stations of interest, and our climate data, let’s\ndownload river flow and water temperature data with the\n{dataRetrieval} package.\n\n\n# strip out the \"USGS\" from our identifier with \"separate\"\nusgs_stations_15km <- usgs_stations_15km %>% \n  tidyr::separate(col = identifier, # the column we want to separate\n                  into = c(\"usgs\", \"usgs_id\"), # the 2 cols to create\n                  remove = FALSE) %>% # keep the original column\n  select(-usgs) # drop this column\n\n# see if there's daily discharge/wtemperature data available (\"dv\"):\ndataRetrieval::whatNWISdata(siteNumber = usgs_stations_15km$usgs_id, \n                            service = \"dv\", \n                            parameterCd = c(\"00060\", \"00010\"),\n                            statCd = \"00003\")\n\n\n    agency_cd  site_no\n163      USGS 11446500\n166      USGS 11446500\n243      USGS 11446700\n260      USGS 11446980\n                                          station_nm site_tp_cd\n163                        AMERICAN R A FAIR OAKS CA         ST\n166                        AMERICAN R A FAIR OAKS CA         ST\n243 AMERICAN R A WILLIAM B POND PARK A CARMICHAEL CA         ST\n260     AMERICAN R BL WATT AVE BRDG NR CARMICHAEL CA         ST\n    dec_lat_va dec_long_va coord_acy_cd dec_coord_datum_cd alt_va\n163   38.63546   -121.2277            F              NAD83  71.53\n166   38.63546   -121.2277            F              NAD83  71.53\n243   38.59129   -121.3327            S              NAD83  45.00\n260   38.56713   -121.3883            S              NAD83  25.00\n    alt_acy_va alt_datum_cd   huc_cd data_type_cd parm_cd stat_cd\n163       0.01       NGVD29 18020111           dv   00010   00003\n166       0.01       NGVD29 18020111           dv   00060   00003\n243       2.00       NGVD29 18020111           dv   00010   00003\n260       2.50       NGVD29 18020111           dv   00010   00003\n     ts_id loc_web_ds medium_grp_cd parm_grp_cd  srs_id access_cd\n163 234322         NA           wat        <NA> 1645597         0\n166  10977         NA           wat        <NA> 1645423         0\n243 234323         NA           wat        <NA> 1645597         0\n260 234324         NA           wat        <NA> 1645597         0\n    begin_date   end_date count_nu\n163 1971-07-20 2023-05-18     3146\n166 1904-10-01 2023-05-17    43328\n243 2016-10-01 2023-05-17     2418\n260 2016-10-01 2023-05-18     2393\n\n# Extract Streamflow for identified sites\nusgs_Q <- readNWISdv(usgs_stations_15km$usgs_id, \n                parameterCd = \"00060\", \n                startDate = \"2016-10-01\") %>% \n  renameNWISColumns()\n\n# extract water temp\nusgs_wTemp <- readNWISdv(usgs_stations_15km$usgs_id, \n                parameterCd = \"00010\", \n                startDate = \"2016-10-01\") %>% \n  renameNWISColumns()\n\n\n\n\n\n\nPlot our USGS Data\nNow we have the data, let’s plot!\n\n\n# Plot! \n(hydro <- ggplot() + \n   geom_line(data = usgs_Q, aes(x = Date, y = Flow, col = site_no),\n             size = .5) + \n   scale_color_brewer(palette = \"Set1\") +\n   facet_wrap(~site_no, scales = \"free_x\") + \n   theme_classic() + \n   labs(title=\"USGS Discharge Data\",\n        x=\"\", y=\"Discharge (cfs)\") +\n   theme(legend.position = \"none\"))\n\n\n\n# Plot temp\n(gg_temp <- ggplot() + \n    geom_path(data = usgs_wTemp, \n              aes(x = Date, y = Wtemp, col = site_no),\n              size = .5) + \n    facet_wrap(~site_no) + \n    theme_bw() + \n    labs(title=\"USGS Water Temperature Data\",\n         x=\"\", y=\"Water Temperature (C)\") +\n    scale_color_viridis_d() +\n    theme(legend.position = \"none\"))\n\n\n\n\n\nChallenge\n\nIn the plots above, we see the gaps in data are connected when using\na line plot. Ideally, we would prefer to visualize these data with gaps\n(no line) where there is no data. To do this, we can leverage handy\nfunctions from the {tidyr} package: complete()\nand fill().\n\n\nClick for Answers!\n\n\n\n# load the package\nlibrary(tidyr)\n\n# fill all unique combinations of Date in our data\nusgs_wTemp2 <- usgs_wTemp %>%\n  group_by(site_no) %>% # group by our gages first\n  complete(Date = seq.Date(min(Date), max(Date), by=\"day\")) %>% \n  # then list the cols we want to fill same value through whole dataset\n  fill(site_no, agency_cd)\n\n# now regenerate plot!\n# Plot temp\n(gg_temp2 <- ggplot() + \n    geom_path(data = usgs_wTemp2, \n              aes(x = Date, y = Wtemp, col = site_no),\n              size = .5) + \n    facet_wrap(~site_no) + \n    theme_bw() + \n    labs(title=\"USGS Water Temperature Data\",\n         x=\"\", y=\"Water Temperature (C)\") +\n    scale_color_viridis_d() +\n    theme(legend.position = \"none\"))\n\n\n\n\n\n\nMake a Map with {tmap}\nOne final component that we haven’t covered much is how to create a\npublication ready-map. We can do this using the {ggplot2}\npackage in conjunction with geom_sf(), or we can use some\nalternate packages which are built specifically to work with spatial\ndata and use a similar code structure to {ggplot2}.\nThe {tmap} and {tmaptools} are excellent\noptions to create a nice map that can be used in any report or\npublication. First, let’s load the packages we’ll use.\n\n\nlibrary(tmap)\nlibrary(tmaptools)\n\n\n\nNow we build our layers using a similar structure as\n{ggplot2}.\n\n\nfinal_tmap <-\n\n  # counties\n  tm_shape(sac_co) +\n  tm_polygons(border.col = \"gray50\", col = \"gray50\", \n              alpha = 0.1, border.alpha = 0.9, lwd = 0.5, lty = 1) +\n\n  # rivers\n  tm_shape(us_flowlines) +\n  tm_lines(col = \"steelblue\", lwd = 2) +\n  \n  # points: LOI stations\n  tm_shape(sac_loi) +\n    tm_symbols(col = \"orange3\", border.col = \"gray20\", \n               shape = 21, size = 1.5, alpha = 1) +\n    tm_add_legend('symbol',shape = 21, col='orange3', border.col='black', size=1,\n                 labels=c(' LOI')) +\n  # points usgs\n  tm_shape(usgs_stations_15km) +\n    tm_symbols(col = \"cyan4\", border.col = \"gray20\", \n               shape = 23, size = 0.5) +\n  tm_add_legend('symbol',shape = 23, col='cyan4', border.col='black', size=1,\n                 labels=c(' USGS Stations')) +\n  # layout\n    tm_layout(\n              frame = FALSE,\n              legend.outside = FALSE, attr.outside = FALSE,\n              inner.margins = 0.01, outer.margins = (0.01),\n              legend.position = c(0.2,0.8)) +\n    tm_compass(type = \"4star\", position = c(\"right\",\"bottom\")) +\n    tm_scale_bar(position = c(\"right\",\"bottom\"))\n\nfinal_tmap\n\n\n\n\nTo save this map, we use a similar function as the\nggsave(), but in this case, it’s\ntmap::tmap_save().\n\n\ntmap::tmap_save(final_tmap, \n                filename = \"images/map_of_sites.jpg\",\n                height = 11, width = 8, units = \"in\", dpi = 300)\n\n\n\n\n\nPrevious\nmodule:7. Paramaterized reports\n\nFor more info on the NLDI: https://labs.waterdata.usgs.gov/about-nldi/index.html↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_advanced_spatial.html",
      "title": "8. Advanced spatial R and mapmaking: I",
      "description": "From 1,000 point-clicks to 1 script...\n",
      "author": [],
      "contents": "\n\nContents\nOverview\nCommon Geospatial Tasks\nScript-based analyses with\n{sf}\n\n\nA\nGroundwater/Surfacewater Hydrology Example\nThe Packages\nImporting Spatial Data\nGet\nState & County Data\nIterate: Get Groundwater\nStations\n\nFilter, Select, & Spatial\nJoins\nFilter\nSpatial Crop\nSpatial Join\n\nWriting Spatial Data Out\n\nAdditional Resources\n\n\nLearning objectives\nLearn to extend and use {sf} for geospatial work\nUnderstand the power of script-based geospatial/mapping\nExpand your geospatial skills in R!\n\nOverview\nThe ability to work in one place or with one program from start to\nfinish is powerful and more efficient than splitting your workflow\nacross multiple tools. By sticking with one single framework or set of\ntools, we can reduce the mental workload necessary when switch between\nprograms, staying organized in each, and dealing with import/export\nacross multiple programs. Although different tools such as ESRI (or\nArcPy extensions) are powerful, they require a paid license and\ntypically use point-click user interfaces.\nThe advantage R has over these tools is that it is\nfreely available, easily integrates with vast statistical/modeling\ntoolboxes, has access to many spatial analysis and mapmaking tools, and\nallows us to work in a single place.\nIf we use a functional programming approach (described in the iteration module ) for spatial problems,\nR can be a very robust and powerful tool for analysis and\nspatial visualization of data! Furthermore, once analyses have been\ncompleted, we can re-use the scripts and functions for common spatial\ntasks (like making maps or exporting specific spatial files).\nCommon Geospatial Tasks\nCommon tasks in a GUI-based approach will always require the same\nnumber of point and clicks. With a script-based approach, it’s much\neasier to recycle previously written code, or to just change a variable\nand re-run the code. This efficiency is magnified immensely when it can\nbe automated or iterated over the same task through time, or multiple\ndata sets.\nFor example, some common tasks may include:\nCropping data to an area of interest for different users\nReproducing a map with updated data\nIntegrating or spatial joining of datasets\nReprojecting spatial data\nScript-based analyses with\n{sf}\nThe {sf} package truly makes working with vector-based\nspatial data easy. We can use a pipeline that includes:\nst_read(): read spatial data in (e.g., shapefiles)\nst_transform(): transform or reproject data\nst_buffer(): buffer around data\nst_union(): combine data into one layer\nst_intersection(): crop or intersect one data by\nanother\ngroup_split() & st_write() to split\ndata by a column or attribute and write out\nThere are many more options that can be added or subtracted from\nthese pieces, but at the core, we can use this very functional approach\nto provide data, make maps, conduct analysis, and so much more.\nA\nGroundwater/Surfacewater Hydrology Example\nLet’s use an example where we read in some groundwater station data,\nspatially find the closest surface water stations, download some river\ndata, and visualize!\nThe Packages\nFirst we’ll need a few packages we’ve not used yet. Please install\nthese with install.packages() if you don’t have them.\n\n\n# GENERAL PACKAGES\nlibrary(tidyverse) # data wrangling & viz\nlibrary(purrr) # iteration\nlibrary(janitor) # name cleaning\nlibrary(glue) # pasting stuff together\n\n# SPATIAL PACKAGES\nlibrary(sf) # analysis tools\nlibrary(mapview)  # interactive maps!\nmapviewOptions(fgb = FALSE) # to save interactive maps\n\nlibrary(tigris) # county & state boundaries\nlibrary(units) # for convert spatial and other units\nlibrary(dataRetrieval) # download USGS river data\nlibrary(tmap) # mapping\nlibrary(tmaptools) # mapping tools\n\n\n\nImporting Spatial Data\nWe’ll leverage the ability to pull in many different data and stitch\nthem all together through joins (spatial or common attributes). Each\ndata component may be comprised of one or more “layers”, which\nultimately we can use on a map.\nGet State & County Data\nFirst we need state and county boundaries. The {tigris}\npackage is excellent for this.\n\n\n# get {sf} CA boundary\nca <- states(cb=TRUE, progress_bar = FALSE) %>% \n  dplyr::filter(STUSPS == \"CA\")\n\n# get counties for CA as {sf}\nca_co <- counties(\"CA\", cb = TRUE, progress_bar = FALSE)\n\n\n\nLet’s also pull in a shapefile that’s just Sacramento County. We’ll\nuse this to crop/trim things down as we move forward. Note, we’ll need\nto check the coordinate reference system and projection here, and make\nsure we are matching our spatial data.\n\n\n# get just sacramento: here we read in a shapefile:\nsac_co <- st_read(\"data/shp/sac/sac_county.shp\")\n\n\nReading layer `sac_county' from data source \n  `/Users/richpauloo/Documents/GitHub/r4wrds/intermediate/data/shp/sac/sac_county.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -13565710 ymin: 4582007 xmax: -13472670 ymax: 4683976\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# check CRS\nst_crs(sac_co)$epsg\n\n\n[1] 3857\n\n# match with other data\nsac_co <- st_transform(sac_co, st_crs(ca_co))\nst_crs(sac_co) == st_crs(ca_co) # should be TRUE!\n\n\n[1] TRUE\n\n# make a box around sacramento county\n# (a grid with an n=1) for inset\nsac_box <- st_make_grid(sac_co, n = 1)\n\n\n\nAnd let’s quickly visualize these pieces! We’ll use the base\nplot() functions here.\n\n\n# make sure we have all the pieces with a quick test plot\nplot(ca$geometry, col = alpha(\"gray\", 0.5), border = \"black\", lwd=2)\nplot(ca_co$geometry, add = T, border = \"purple\", col = NA)\nplot(sac_co$geometry, add = T, border = \"cyan4\", col = \"skyblue\",alpha=0.4, lwd = 2)\nplot(sac_box, add = T, border = \"orange\", col = NA, lwd = 1.4)\n\n\n\n\nIterate: Get Groundwater\nStations\nLet’s practice our iteration skills. We’ll read in groundwater\nstations for 3 counties (El Dorado, Placer, and\nSacramento), convert to {sf} objects, plot them, and then\ncrop/select a subset of stations using spatial intersection.\nIteration…remember {purrr}? Let’s use it here!\n\n\n# read the stations\ngw_files <- list.files(path = \"data/gwl/county\",\n                       full.names = TRUE, pattern = \"*.csv\")\n\n# read all files into dataframes and combine with purrr\ngw_df <- map_df(gw_files, ~read.csv(.x))\n\n# the readr package will also do this same thing by default\n# when passed a list of files with the same data types\ngw_df <- read_csv(gw_files)\n\n# now make \"spatial\" as sf objects\ngw_df <- st_as_sf(gw_df, coords=c(\"LONGITUDE\",\"LATITUDE\"), \n                  crs=4326, remove=FALSE) %>% \n  # and transform!\n  st_transform(., st_crs(ca_co))\n\n# preview!\nmapview(gw_df, zcol=\"COUNTY_NAME\", layer.name=\"GW Stations\") +\n  mapview(sac_co, legend=FALSE)\n\n\n\n\nHmmm…looks like there are some stations up near Lake Tahoe, and then\nall the stations that are outside of the Sacramento County boundary.\nLet’s move on to do some cleaning/cropping/joining.\nFilter, Select, & Spatial\nJoins\nOne of the more common spatial operations is filtering or clipping\ndata based on a condition or another spatial layer.\nOften to complete a geospatial operation, we need to use a projected\ncoordinate reference system1 (not\nlatitude/longitude), so we can specify things in units that are easier\nto understand (kilometers or miles) instead of arc degrees, and so that\nthe calculations take place correctly. Note, here we have transformed\nour data to match up.\nFilter\nWe could certainly leverage the data.frame aspect of\n{sf} and quickly filter down to the stations of interest\nusing the COUNTY_NAME field.\n\nYou Try!\n\nUse filter() to filter our gw_df dataframe\nto only stations that occur in Sacramento County. Then make a\nmapview() map and make the color of the dots correspond\nwith the different WELL_USE categories. How many stations\nare there in each WELL_USE category?\n\n\nClick for Answers!\n\n\n\ngw_sac <- gw_df %>% \n  filter(COUNTY_NAME == \"Sacramento\")\n\ntable(gw_sac$WELL_USE)\n\n\n\n   Industrial    Irrigation   Observation         Other   Residential \n            2           144            90           101            76 \nStockwatering       Unknown \n            6            67 \n\nmapview(gw_sac, zcol=\"WELL_USE\", layer.name=\"Well Use\")\n\n\n\n\n\n\nSpatial Crop\nGreat! But what if we don’t have the exact column we want, or any\ncolumn at all? We may only have spatial data, and we want to trim/crop\nby other spatial data. Time for spatial\noperations.\nFirst, we can use base [] to crop our data. Here we take\nthe dataset we want to crop or clip (gw_sac) and crop by\nthe Sacramento county polygon (sac_co). This is a type of\nspatial join, but note, we retain the same number of columns in the\ndata.\n\n\n# spatial crop: \n# # does not bring attributes from polygons forward\ngw_sac_join1 <- gw_df[sac_co,]\n\nplot(sac_co$geometry, col = alpha(\"forestgreen\", 0.6))\nplot(gw_sac_join1$geometry, bg = alpha(\"orange\", 0.3), pch=21, add = TRUE)\ntitle(\"GW Stations (orange) \\nthat occur in Sacramento County\")\n\n\n\n\nSpatial Join\nWe can also use st_join() directly to filter for points\nthat fall within a supplied polygon(s). In our case, we want groundwater\nstations (points) that fall within our selected counties (polygons).\n\n\ngw_sac_join2 <- st_join(gw_df, sac_co, left = FALSE)\n\nmapview(gw_sac_join2, col.region=\"orange\", legend=FALSE) +\n  mapview(sac_co, alpha = 0.5, legend = FALSE)\n\n\n\n\nNote, what’s different between gw_df and\ngw_sac_join1?\nst_join joins the data spatially, but it also\nbrings the attributes along with it. Note all the\ncounty columns from gw_df now appear in the\ngw_sac_join2\nWe can also use an anti_join (the !) to find the\nstations that weren’t contained in our focal area. These operations can\nbe helpful when exploring and understanding a dataset, to identify gaps,\nhighlight specific areas, etc. st_intersects returns a\nvector of items based on whether the two spatial objects intersect\n(which can be defined differently using a multitude of spatial\nfunctions, see the sf\nhelp page).\n\n\n# anti_join: find stations that aren't contained in Sacramento County\ngw_sac_anti <- gw_df[!lengths(st_intersects(gw_df, sac_co)), ]\n\n# plot\nmapview(gw_sac_anti, \n        col.regions=\"maroon\", cex=3, \n        layer.name=\"Anti-Join Sites\") + \n  mapview(sac_co, alpha.regions=0, \n          color=\"black\", lwd=3, legend=FALSE)\n\n\n\n\nWriting Spatial Data Out\nWe may want to save these data and send to colleagues before we\nproceed with further analysis. As we’ve shown before2,\nfunctional programming allows us to split data and write it out for\nfuture use, or to share and distribute. Here we use a fairly simple\nexample, but the concept can be expanded.\nLet’s use the {purrr} package to iterate over a lists\nand write each layer to a geopackage (a self contained spatial\ndatabase). Geopackages are a great way to save vector-based spatial\ndata, they can be read by ArcGIS and spatial software, and they are\ncompact and self-contained (unlike shapefiles).\n\n\nlibrary(purrr)\nlibrary(glue)\nlibrary(janitor)\n\n# first split our gw_df data by county:\ngw_df_split <- gw_df %>% \n  split(.$COUNTY_NAME) %>% # split by cnty name \n  set_names(., make_clean_names(names(.))) # make a file friendly name\n\n# now apply function to write out points by county\nmap2(gw_df_split, # list of points by county\n     names(gw_df_split), # list of names for layers\n     ~st_write(.x, \n               dsn = \"data/county_gw_pts.gpkg\",\n               layer = glue(\"{.y}_gw_pts\"), \n               delete_layer=TRUE, # to remove layer if it exists\n               quiet = TRUE) # suppress messages\n     )\n\n\n$el_dorado\nSimple feature collection with 45 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -120.07 ymin: 38.8017 xmax: -119.944 ymax: 38.9592\nGeodetic CRS:  NAD83\n# A tibble: 45 × 15\n   STN_ID SITE_CODE      SWN   WELL_NAME LATITUDE LONGITUDE WLM_METHOD\n    <dbl> <chr>          <chr> <chr>        <dbl>     <dbl> <chr>     \n 1  10713 388017N120018… 11N1… <NA>          38.8     -120. Unknown   \n 2  47508 388030N120015… <NA>  EX-1          38.8     -120. Surveyed …\n 3  47519 388224N120021… 11N1… SUT No.1      38.8     -120. Surveyed …\n 4  47510 388395N120024… <NA>  Henderso…     38.8     -120. Surveyed …\n 5  47504 388544N120019… <NA>  DW-1          38.9     -120. Surveyed …\n 6  47512 388545N120019… <NA>  IW-1          38.9     -120. Surveyed …\n 7  47520 388545N120019… <NA>  SW-1          38.9     -120. Surveyed …\n 8  47499 388552N120017… <NA>  Apache OW     38.9     -120. Surveyed …\n 9  15199 388562N120014… 12N1… <NA>          38.9     -120. Unknown   \n10  47506 388582N120021… <NA>  ESB-2         38.9     -120. Surveyed …\n# … with 35 more rows, and 8 more variables: WLM_ACC <chr>,\n#   BASIN_CODE <chr>, BASIN_NAME <chr>, COUNTY_NAME <chr>,\n#   WELL_DEPTH <dbl>, WELL_USE <chr>, WELL_TYPE <chr>,\n#   geometry <POINT [°]>\n\n$placer\nSimple feature collection with 184 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.614 ymin: 38.7005 xmax: -120.112 ymax: 39.3079\nGeodetic CRS:  NAD83\n# A tibble: 184 × 15\n   STN_ID SITE_CODE      SWN   WELL_NAME LATITUDE LONGITUDE WLM_METHOD\n    <dbl> <chr>          <chr> <chr>        <dbl>     <dbl> <chr>     \n 1  48577 381626N121365… <NA>  SVMW Eas…     38.8     -121. Surveyed …\n 2  55217 387005N121614… <NA>  Elkhorn …     38.7     -122. Unknown   \n 3  55218 387005N121614… <NA>  Elkhorn …     38.7     -122. Unknown   \n 4  33091 387216N121247… 10N0… 10N07E18…     38.7     -121. Unknown   \n 5  52475 387222N121292… <NA>  Whyte A       38.7     -121. GPS       \n 6  52476 387222N121292… <NA>  Whyte B       38.7     -121. GPS       \n 7  13664 387269N121276… 10N0… <NA>          38.7     -121. Unknown   \n 8  13665 387285N121339… 10N0… <NA>          38.7     -121. Unknown   \n 9  13670 387290N121251… 10N0… <NA>          38.7     -121. Unknown   \n10  51280 387331N121361… <NA>  WPMW-5A       38.7     -121. Surveyed …\n# … with 174 more rows, and 8 more variables: WLM_ACC <chr>,\n#   BASIN_CODE <chr>, BASIN_NAME <chr>, COUNTY_NAME <chr>,\n#   WELL_DEPTH <dbl>, WELL_USE <chr>, WELL_TYPE <chr>,\n#   geometry <POINT [°]>\n\n$sacramento\nSimple feature collection with 494 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.695 ymin: 38.1023 xmax: -121.043 ymax: 38.7311\nGeodetic CRS:  NAD83\n# A tibble: 494 × 15\n   STN_ID SITE_CODE      SWN   WELL_NAME LATITUDE LONGITUDE WLM_METHOD\n    <dbl> <chr>          <chr> <chr>        <dbl>     <dbl> <chr>     \n 1  49420 380926N121587… 03N0… 03N03E12…     38.1     -122. GPS       \n 2  49421 380926N121587… 03N0… 03N03E12…     38.1     -122. GPS       \n 3  49419 381023N121569… 03N0… 03N03E12…     38.1     -122. GPS       \n 4  49425 381123N121510… 03N0… 03N04E16…     38.1     -122. GPS       \n 5  49426 381123N121510… 03N0… 03N04E16…     38.1     -122. GPS       \n 6  50577 381132N121695… 03N0… COSAC3        38.1     -122. Digital E…\n 7  49423 381170N121524… 03N0… 03N04E17…     38.1     -122. GPS       \n 8  49433 381200N121492… 03N0… 03N04E13…     38.1     -121. GPS       \n 9  49428 381222N121499… 03N0… 03N04E11…     38.1     -122. GPS       \n10  49416 381301N121564… 03N0… 03N04E02…     38.1     -122. GPS       \n# … with 484 more rows, and 8 more variables: WLM_ACC <chr>,\n#   BASIN_CODE <chr>, BASIN_NAME <chr>, COUNTY_NAME <chr>,\n#   WELL_DEPTH <dbl>, WELL_USE <chr>, WELL_TYPE <chr>,\n#   geometry <POINT [°]>\n\n We use map2() here, but can also use\nwalk2(), which is a “silent” map that doesn’t print any\noutput to the console.\nTo make sure this worked as intended, we can check what layers exist\nin the geopackage with the st_layers function.\n\n\n# check layers in the gpkg file\nst_layers(\"data/county_gw_pts.gpkg\")\n\n\nDriver: GPKG \nAvailable layers:\n         layer_name geometry_type features fields\n1  el_dorado_gw_pts         Point       45     14\n2     placer_gw_pts         Point      184     14\n3 sacramento_gw_pts         Point      494     14\n\nAdditional Resources\nWe’ve covered a handful of packages and functions in this module, but\nmany more exist that solve just about every spatial workflow task. All\nspatial and mapmaking operations are typically a websearch away, but we\nalso recommend the\nfollowing resources to dig deeper into the R spatial\nuniverse.\n\n\nPrevious\nmodule:7. Paramaterized reports\n\nA discussion on coordinate reference systems is a\ncomplex topic in and of itself, and for the purposes of this module, we\nsummarize it as follows: A geographic CRS is round and\nbased on angular units of degrees (lat/lng), whereas a\nprojected CRS is flat and has linear units\n(meters or feet). Many functions in {sf} that make\ncalculations on data expect a projected CRS, and can return inaccurate\nresults if an object in a geographic CRS is used. This is a fascinating\ntopic with lots written about it! For more reading see this Esri\nblog, the Data Carpentry geospatial\nlesson, and the online\nGeocomputation with R book.↩︎\nSee the iteration\nmodule for an example of iterating over a write function.↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_interactive_viz.html",
      "title": "4. Interactive Visualization",
      "description": "Having fun with interactivity in `R`\n",
      "author": [],
      "contents": "\n\nContents\nWhy interactive\nvisualizations?\nMapping with Geocoder\nCollect Data\nImport\nand Map Data\nUse\n{leaflet}\n\nUsing D3\nA Network\nwith D3\n\nPlotly\nSummary\nAdditional Resources\n\n\nLearning objectives\nHow to use interactive visualization in R\nWorkflows to go from data to visualization\nUnderstand a visualization objective\n\n\nWhy interactive\nvisualizations?\nOne of the most exciting and rewarding parts of learning to code is\nthe ability to see results! Visualization is\none of R’s strengths, and there are a seemingly constantly\ngrowing number of packages that can assist you in customizing just about\nany visualization you can think of. Importantly, this allows users to\naccess and interact with data on a level that would not otherwise be\npossible.\nBecause we can learn new things and develop new questions by\neffectively interacting with data, the ability to stitch raw data into\nsomething visual can help provide unique insight that may be much harder\nto discern with a table or some raw data.\nLet’s demonstrate a simple way we can collect data, update it\nautomagically, and visualize in a fairly streamlined manner.\nKeep in mind, this is one approach, but there are many more, with\nincreasing complexity. But for illustration, “working” examples (as in,\nit works when you try it!) are the best way to build your proficiency in\nR programming.\n\nMapping with Geocoder\nThis example will couple the power of collecting user input (favorite\nplaces to eat/drink) with interactive mapping. We’ll use a Form to\ncollect some data, and then display that via a map. This illustrates\nsome of the power (and fun!) of programming in R.\nCollect Data\nAnswer as many questions as you feel comfortable, but at minimum, the\nstreet address/name of your favorite place to eat or get a beverage (hot\nor cold).\n\nLoading…\n\nImport and Map Data\nWe’ll need these packages to work with this data.\n\n\nlibrary(tidygeocoder) # geocode our addresses\nlibrary(tidyverse)    # wrangle data\nlibrary(janitor)      # clean column names\nlibrary(glue)         # modern paste() function\nlibrary(sf)           # make spatial data\nlibrary(mapview)      # interactive maps!\nmapviewOptions(fgb = FALSE)\n\n\n\nThe entire script we need is below (don’t forget to include the\npackages above). To generate a public .csv from a Form, we\ncan use File > Publish to the Web1.\nImportantly, here we are publishing to a .csv so we can\nread the data in directly and simply using read_csv. We\nthen clean, convert it into an sf object, and map!\n\n\n# the url for the Form data \nform_data <- paste0(\"https://docs.google.com/spreadsheets/d/e/\",\n                    \"2PACX-1vSODxBm_z5Gu8a42C6ZFEa3S5iTbYV-\",\n                    \"qucCGvasGS6c0qFUAml5vSMEgbvI9PYo1HJ20Y_WY62aTAb-\",\n                    \"/pub?gid=1462593645&single=true&output=csv\")\n\n# read in url and clean\ndat <- read_csv(form_data) %>% \n  clean_names() %>% \n  rename( dining_name = 3, dining_address = 4)\n\n# geocode using Open Street Map (osm) API because it's free\ndat_geo <- dat %>%\n  geocode(dining_address, method = 'osm', lat = latitude , long = longitude)\n\n# make into sf object so we can map\ndat_geo <- dat_geo %>% \n  filter(!is.na(latitude) & !is.na(longitude)) %>% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326, remove = FALSE)\n\n# map!\nmapview(\n  dat_geo, \n  zcol = \"comfort_using_r\", \n  layer.name = \"R comfort level\", \n  cex = 6.5\n)\n\n\n\n\nUse {leaflet}\nWe used {mapview} above, but we could also use the\n{leaflet} package to make a more customizable map with some\nfancy icons and a measuring tool, to calculate how far each location is\nfrom a point of interest, or the the total area that encompasses all our\npoints.\n\n\nlibrary(leafpm)\nlibrary(leaflet)\nlibrary(leaflet.extras)\nlibrary(htmltools)\n\n# set up our map\nm <- leaflet() %>%\n  # add tiles or the \"basemaps\"\n  addTiles(group = \"OSM\") %>% # defaults to Open Street Maps\n  addProviderTiles(providers$CartoDB.Positron, group = \"Positron\") %>% \n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %>%\n  addCircleMarkers(\n    lng = -121.4944, lat = 38.5816, fillColor = \"red\", color = \"black\",\n    popup = \"Sacramento!\", group = \"Home\",\n  ) %>% \n  addCircleMarkers(\n    data = dat_geo, group = \"Food & Drink\",\n    label = ~htmlEscape(first_name),\n    popup = glue(\n      \"<b>Name:<\/b> {dat_geo$first_name}<br>\n      <b>Food_Name:<\/b> {dat_geo$dining_name}<br>\n      <b>Food_Address:<\/b> {dat_geo$dining_address}<br>\n      <b>R comfort (1-10):<\/b> {dat_geo$comfort_using_r}\"\n    )\n  )  %>% \n  addLayersControl(\n    baseGroups = c(\"Toner Lite\", \"Positron\", \"OSM\"),\n    overlayGroups = c(\"Home\", \"Food & Drink\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>% \n  addMeasure()\n\nm  # Print the map\n\n\n\n\n\nUsing D3\nA powerful visualization tool is the D3.js (javascript) library, which has some\namazing options. What’s great is there are ways to easily translate your\nR code into D3, via packages like {r2d3}.\nHere’s an example of a calendar visualization, based on the stock\nmarket open and close between 2006 and 2010.\n\n\n\nA Network with D3\nAnother example of a way to interactively engage with data is\nnetworks. Here’s a simple example of an interactive network\nvisualization:\n\n\n# Libraries\nlibrary(igraph)\nlibrary(networkD3)\n\n# create a dataset:\ndata <- tibble(\n  from = c(\n    \"Dam\",\"Dam\",\"Dam\", \"Dam\",\n    \"River\",\"River\",\"River\", \"River\",\"River\",\n    \"Canal\", \"Canal\", \n    \"Diversion\",\"Diversion\", \n    \"Reservoir\", \"Reservoir\",\"Reservoir\",\n    \"Lake\",\"Lake\",\"Lake\", \"Lake\", \n    \"Road\",\"Road\",\"Road\",\n    \"Culvert\", \"Culvert\",\n    \"Fish\", \"Fish\",\"Fish\",\n    \"Frog\",\"Frog\",\"Frog\",\n    \"MacroInvertebrates\",\"MacroInvertebrates\"\n  ),\n  to = c(\n    \"River\",\"Reservoir\",\"Canal\",\"Diversion\",\n    \"Lake\",\"Reservoir\",\"Frog\",\"Fish\",\"MacroInvertebrates\",\n    \"Diversion\", \"Reservoir\",\n    \"Dam\", \"River\",\n    \"River\",\"Dam\",\"Fish\",\n    \"Fish\",\"Dam\",\"Frog\",\"MacroInvertebrates\",\n    \"Fish\",\"Dam\", \"Canal\",\n    \"Road\", \"Dam\",\n    \"Frog\", \"River\",\"MacroInvertebrates\",\n    \"Fish\", \"River\", \"Lake\",\n    \"River\", \"Lake\"\n  )\n)\n\n# Plot\n(p <- simpleNetwork(data, height = \"600px\", width = \"600px\", \n                    fontSize = 16, fontFamily = \"serif\",\n                    nodeColour = \"darkblue\", linkColour = \"steelblue\",\n                    opacity = 0.9, zoom = FALSE, charge = -500))\n\n\n\n\nWhen we wrap code with (), as we did with the plot\nabove, it will evaluate the code and provide the output in the\nConsole, Plots, or Viewer tab\n(depending on the code). This is a handy shortcut instead of having to\nsave the object p <- my_code and then call/print the\nobject p. We use (p <- my_code).\nIf we wanted to save this interactive visualization and send it as a\nstandalone .html file, we could use the following code:\n\n\n# save the widget\nlibrary(htmlwidgets)\nsaveWidget(p, file = \"output/network_interactive.html\")\n\n\n\n\nPlotly\nAnother great tool for interacting with data is the\n{plotly} package, which if using the {ggplot2}\nframework, has a very handy ggplotly() function which you\ncan wrap around pretty much any ggplot to turn a static\nplot into an interactive one. This allows you to click, hover, zoom,\netc. inside your plot, and is a very useful tool for exploring data.\nFor example, say you were asked to find the date of a peak flow for a\ngiven year, or an outlier in a graph. We could figure this out with\ndplyr::filter() and additional queries of our data, but it\nwould also be easy to visually look and see the date of peak flow on a\nplot. This is where the power of plotly can be very\nhelpful.\nLet’s use the CalEnviroScreen\ndata from Sacramento County from a previous\nmodule. Recall, higher scores mean a higher impact of pollution to a\ncommunity.\n\n\n# load CES data for Sacramento county\nces3_sac <- readRDS(\"data/ces3_sac.rds\")\n\nmapview(ces3_sac, zcol = \"CIscoreP\")\n\n\n\n\nLet’s make a static plot that shows the association between\ngroundwater threats and the CES score. The trend is interesting, but\nwhat if we want to know what census tracts are associated with outliers\n(high or low) on this static plot?\n\n\n# plot of Groundwater threats vs. CES score\n(ces_plot <- ces3_sac %>% \n   ggplot(aes(x = gwthreatsP, y = CIscoreP, label = tract)) + \n   geom_point() +\n   geom_smooth(method = \"gam\") +\n   cowplot::theme_half_open(font_size = 12) +\n   labs(\n     title = \"CES Score vs. Groundwater Threats in Sacramento County\",\n     subtitle = \"Higher CI Score indicates higher threat/impact\",\n     x = \"Groundwater Threats (percentile)\", \n     y = \"CI Score (percentile)\"\n   )\n)\n\n\n\n\nTry wrapping the exact plot above with ggplotly(). Now\nwe can quickly interact and select data of interest. Try hovering your\npointer over a point on the plot below. Or, click and drag to select a\nspecific region of the plot to zoom into just that region. You can\ndouble click to return to the original zoom level. Visual inspection\nlike so can help us quickly explore large and complex datasets, and\nidentify the next step in our workflow.\n\n\nlibrary(plotly)\nggplotly(ces_plot)\n\n\n\n\n\nSummary\nThere are many options to make very fancy visualizations in R, but\nremember to consider the objective of your work, and how to use\nvisualization as a tool to communicate with the end user. Stories are\npowerful, and although a flashy visualization may grab someone’s\nattention, it will have more impact if it tells a story. At other times,\nthe end user of interactive data visualization is you (the analyst!), as\ninteractive visualization is an important tool for Exploratory Data\nAnalysis (see the EDA\nmodule).\nWhatever the visualization, if it communicates data in a way that\nprovides new insight, or provides a easier medium to transfer\ninformation (i.e., with an Rmarkdown document), the ability to quickly\ntake data, transform it, and visualize it is a powerful skill to\npractice.\n\nAdditional Resources\nRmarkdown\nFlexdashboard\n{r2d3}\nVisualizations\nRGraph\nGallery\nAll things Shiny\nNetwork\nvisualization\n\n\nPrevious\nmodule:3. Project Management\nNext\nmodule:5. Simple Shiny\n\nNote that these instructions are specific to Google\nDrive, where we created the Form, but most cloud-based repositories\n(Box, Dropbox, etc) have some version of allowing public read access of\nfiles (including csv files). We chose a Google Form in this example\nbecause of the integration between the Form and a readable csv.↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_iteration.html",
      "title": "6. From loops to functions",
      "description": "Multiple ways to iterate in `R`\n",
      "author": [],
      "contents": "\n\nContents\nMany paths to the same\nsummit\nfor\nloops\nlapply()\nmap()\nAdditional Resources\n\n\nLearning objectives\nUnderstand the costs and benefits of loops and functional\nprogramming\nPractice iteration with for loops\nPractice iteration with lapply() and the\n{purrr} family of functions\nUnderstand how interact with the list object type\n\n\nMany paths to the same\nsummit\nOne of the wonderful things about R is that there are\noften many ways to achieve the same result. This makes R\nexpressive, and as you practice and expand your R\nvocabulary, you’ll inevitably find multiple ways to obtain the same\nresults. Some ways may be long and meandering, and others may be more\ndirect and easy to remember. In this module we will practice two\napproaches towards iteration—the art of not repeating\nyourself—specifically, for loops1 and\nfunctions (functional programming).\nFor most parallel problems (i.e., the current operation does not\ndepend on the one that came before), functional programming offers\nsignificant advantages to for loops. However, both loops\nand functions are powerful ways to automate processes and workflows. We\nwill review how and when to use each, and their pros and cons.\nThe core objective of this lesson is to practice harnessing the power\nof iteration in your workflows, and to demonstrate ways to iterate with\nfunctional programming. If you can define a function to work with one\nobject in R, functional programming allows you to scale\nthat function to any number of objects (assuming you have enough\ncomputing resources). Together, functional programming and iteration\nallow automation at scale, and put R a cut above GUI-based\ndata workflows that require “clicking though” an analysis.\nProficiency in functional programming means you can\nturn an entire workflow into a function, or individual steps for a\nworkflow can each be discrete functions, making it easy to reproduce or\nrevise your work.\nProficiency in iteration means you can apply\ndeveloped functions many times with little effort, which allows you to\nwork with small or large datasets with ease.\nProficiency in automation means you can remove\nyourself from running the code entirely so that it happens all on its\nown—we will demonstrate approaches to automation in the next\nmodule.\nIn this module, let’s imagine you’re a data manager, and need to\nprovide data to a group of users. We’ll practice iteration on reading\nand writing data to illustrate a transition from for loops\nto functional programming.\n\nfor loops\n\n“Don’t repeat yourself. It’s not only repetitive, it’s redundant, and\npeople have heard it before.” -Lemony Snicket\n\nThe for loop is ubiquitous across programming languages\nand a fundamental concept that allows us to obey a core concept in\nprogramming: don’t repeat yourself. The for loop is a good\nplace to begin a discussion of iteration because it makes iteration very\nexplicit—you can see exactly what is taking place in each iteration, or\nloop.\nA common problem we might face is reading multiple data frames into\nR. In the /data/gwl folder, we have station\ndata for El Dorado, Placer, and Sacramento counties. We can read these\nin one by one by copy and pasting code, but we’re repeating ourselves.\nThis may not matter for only 3 counties, but if we were to use all 58\ncounties with groundwater level data, this would be a non-scaleable\napproach prone to human error.\n\n\nlibrary(tidyverse)\n\neldorado <- read_csv(\"data/gwl/county/El Dorado.csv\")\nplacer   <- read_csv(\"data/gwl/county/Placer.csv\")\nsac      <- read_csv(\"data/gwl/county/Sacramento.csv\")\n\n\n\nWe can replace this code with a for loop and read all of these items\ninto a list2.\n\n\n# list all files we want to read in\nfiles_in <- fs::dir_ls(\"data/gwl/county\")\n\n# initialize a list of defined length\nl <- vector(\"list\", length = length(files_in))\n\n# loop over all files and read them into each element of the list\nfor(i in seq_along(l)){\n  l[[i]] <- read_csv(files_in[i])\n}\n\n\n\nAfter the loop finished, if you run i in the console,\nyou’ll notice that it has a value of 3. That’s because i is\nupdated as the loop evaluates, and 3 is the last iteration of\ni in the loop. This can be useful when debugging a loop\nbecause the last index that the loop was evaluated may identify the\nlocation of a problem that breaks the loop.\nWhat happened above is that we evaluated the loop first starting with\n1 in the place of i, and went to length(l),\nwhich is 3, each time placing the integer wherever there is an\ni in the above loop. Although our index is i,\nwe can use any other unquoted character string, like j,\nk, or even index — its only purpose is to hold\nthe index that we iterate through.\nAfter the loop evaluates, we can access each element of the list with\ndouble bracket [[ notation, subsetting either by an integer\nindex, or a name if one exists. This list doesn’t have names, but we\ncould set them by assigning a vector of names to\nnames(l).\n\n\n# access first list element - El Dorado county dataframe\nl[[1]] \n\n\n\nfor loops happen sequentially, and require us to think\nof code in terms of objects that we iterate through, index by index.\nThis can result in both slower and duplicated (verbose) code. In\naddition to writing efficient code, a core rule of good programming is\nto not repeat oneself.\nLet’s imagine now that you needed to write each of these data into a\nseparate file, separated by the unique ID of each station (e.g., the\nSITE_CODE). Your entire loop would look like this:\n\n\n# initialize a list of defined length\nl <- vector(\"list\", length = length(files_in))\n\n# loop over all files and read them into each element of the list\nfor(i in seq_along(l)){\n  l[[i]] <- read_csv(files_in[i])\n}\n\n# combine all list elements into a single dataframe\n# then split into another list by SITE_CODE.\nldf <- bind_rows(l)\nldf <- split(ldf, ldf$SITE_CODE)\n\n# loop over each list element and write a csv file\nfs::dir_create(\"data/gwl/site_code\")\n# here we make a list of files names ( names(ldf) is a vector )\nfiles_out <- glue::glue(\"data/gwl/site_code/{names(ldf)}.csv\")\n\nfor(i in seq_along(ldf)){\n  write_csv(ldf[[i]], files_out[i])\n}\n\n\n\ndplyr::bind_rows(l) row binds a list of\ndata.frames similar to base R’s\ndo.call(rbind.data.frame, l), but is more strict, and will\nthrow an error if column types don’t match.\n\n\n\nThis is a lot of code to accomplish a relatively standard iterative\nworkflow of reading in a directory of files, combining them, and writing\nthem out. Functional programming can simplify loop-based workflows, run\nfaster, and encourage you to think conceptually about the\ntransformations at play, without worrying about tracking a changing\nindex. Moreover, loops in R are usually unnecessary unless\nthe result of the ith index depends on the i-1\n(previous) index.\nlapply()\nBase R gives us a toolkit for functional programming via\nthe apply family of functions, specifically\nlapply() and mapply(). The “l” in\nlapply() stands for “list”, and can be read as\n“list apply”. The apply functions are designed to iterate\nover lists, matrices, rows, or columns, and are very flexible. For\nexample, we can simplify the for loops above as:\n\n\n# read\nl <- lapply(files_in, read_csv)\n\n# bind and split by SITE_CODE\nldf <- bind_rows(l)\nldf <- split(ldf, ldf$SITE_CODE)\n\n# write out: requires function (write_csv) \n# with 2 args (x = ldf & y = files_out)\nmapply(function(x, y) write_csv(x, y), ldf, files_out) \n\n\n\nWe can make this clearer by extracting the anonymous\nfunction3 above, assigning it to an\nidentifier, and calling that in mapply():\n\n\nmy_function <- function(x, y){ \n  write_csv(x, y)\n}\n\nmapply(my_function, ldf, files_out) \n\n\n\nNotice that we don’t need to initialize a list to store output, or\nkeep track of indices. The emphasis is on the transformation taking\nplace, not index bookkeeping, and setting up looping patterns. The\nresult is the same, and we have a much clearer way to approach the\nproblem.\n\nmap()\nThe map family of functions in the {purrr}\npackage improves on base R’s apply functions\nwith simplified syntax, type-specific output that makes it harder to\naccidentally create errors, and convenience functions for common\noperations. When we combine map() with pipes\n(%>%) we can greatly simplify our code.\nFirst, to mimic what we did with lapply() above:\n\n\n# read\nl <- map(files_in, ~read_csv(.x))\n\n# bind and split by SITE_CODE\nldf <- bind_rows(l)\nldf <- group_split(ldf, SITE_CODE)\n\n# write\nwalk2(ldf, files_out, ~write_csv(.x, .y)) \n\n\n\nThe .x signifies each of the individual elements of\npassed into the function, and can be thought of as a placeholder for the\ninput. We begin read_csv() with a ~ to\nindicate the beginning of a function.\nWe can further simplify our code with map_df() to\nautomatically row bind the list elements into one dataframe. Moreover,\nwe can pipe these statements together to avoid creating the intermediate\nldf object.\n\n\n# read and bind, split, and write\nmap_df(files_in, ~read_csv(.x)) %>% \n  group_split(SITE_CODE) %>% \n  walk2(files_out, ~write_csv(.x, .y))\n\n\n\nLet’s break down what we did above.\nWe mapped the function read_csv() over the vector of\nfile paths, files_in. Although this function is simple, in\npractice we can map a large and complex function in the same way. The\n_df in map_df() means we pass the list\notherwise returned by map() into bind_rows(),\nand thus return one combined dataframe of all the csv files we read\ninstead of a list of dataframes.\nWe used group_split() to split the combined dataframe\nby SITE_CODE which returns a list of dataframes ordered by\nthe unique values of the grouping variable. This is identical to base\nR’s split() except it doesn’t return a named\nlist.\nWe walk2()ed over the list of dataframes (one for each\nSITE_CODE), and the files_out vector from\nabove, and wrote a csv for each pair of objects (.x =\ndataframe and .y = output file path).\n\nExtra information\n\nWhat would happen if we used map2() instead of\nwalk2() in the code above?\n\nClick for the Answer!\n\nwalk2() is a special case of walk() which\ntakes 2 vector inputs instead of 1. The first and second vector inputs\nare called in the function with .x and .y like\nso: walk2(input_1, input_2, ~function(.x, .y)). We use\nwalk() instead of map() whenever we want the\nside effect of the function, like writing a file. We could also use\nmap() here, but it would unnecessarily print each of the\n723 dataframes.\nYou may now we wondering if there is a map3(),\nmap4() and so on. To map over more than 2 inputs at once,\ncheck out the “parallel” map purrr::pmap() function, which\nis similar to base R’s mapply().\n\n\n\nYou may notice that we used an intermediate object\n(files_out) from above. We could re-write our chain without\nthis object by creating it within the function call:\n\n\n# read, bind, and write\nmap_df(files_in, ~read_csv(.x)) %>% \n  group_split(SITE_CODE) %>% \n  walk(\n    ~write_csv(\n      .x, \n      glue::glue(\"data/gwl/site_code/{.x$SITE_CODE[1]}.csv\")\n    )\n  )\n\n\n\nThe ~, .x, and .y syntax may\nseem confusing at first, but with some practice it will become easier,\nand it provides a consistent syntax to express complex ideas about your\ncode. More more importantly, the emphasis is on keeping track of\nfunctions rather than creating and managing the scaffolding of\nfor loops.\nTaking things one step further, imagine you were:\nonly interested in WELL_USE types equal to\n“Observation”\nyou needed to convert the WELL_DEPTH from feet to\nmeters\nyou needed to output the data as three shapefiles, one for each\ncounty\nWe could capture these transform steps in a function, store it in our\n/functions folder as\ndescribed in the project management module, and in this way, clean\nup our workspace so we can keep track of the functions applied on our\ndata and keep our scripts short and readable.\nStore this in a file\n/scripts/functions/f_import_clean.R:\n\n\n# import dataframe, filter to observation wells, convert well \n# depth feet to meters, project to epgs 3310, & export the data\nf_import_clean_export <- function(file_in, file_out){\n  read_csv(file_in) %>% \n    filter(WELL_USE == \"Observation\") %>% \n    mutate(well_depth_m = WELL_DEPTH * 0.3048) %>% \n    sf::st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4269) %>% \n    sf::st_transform(3310) %>% \n    sf::st_write(file_out, delete_layer = TRUE)\n}\n\n\n\nNow we can walk() inputs over this function with ease in\nour main script without keeping track of loops and indices, or even the\nfunction internals, which are neatly stored in their own file.\n\n\nsource(\"scripts/functions/f_import_clean_export.R\")\n\n# create a directory to store results\nfs::dir_create(\"results\")\n\n# vectors with function args: input (.x) & output (.y) files\nfiles_in  <- fs::dir_ls(\"data/gwl/county\")\nfiles_out <- here(\"results\", str_replace_all(basename(files_in), \".csv\", \".shp\"))\n\nwalk2(files_in, files_out, ~f_import_clean_export(.x, .y))\n\n\n\nNow we check the /results directory to verify it is\npopulated with the shapefiles we just wrote.\n\nAdditional Resources\nIteration is a core skill that will allow you to scale your workflows\nfrom small to large while maintaining reproducibility. Combined with a\nproficiency in functional programming, you will more easily develop and\nstore functions, declutter your workspace to focus on important\ntransformations, streamline tracking down and fixing bugs, and keep\ntrack of data pipelines. Your ability to perform and re-perform\narbitrarily complex workflows will exponentially increase.\nWe recommend the following places to start to learn more about\nfunctional programming in R:\nR for Data Science book section on Programming\nA\ntalk about functional programming by Hadley Wickham\nAdvanced R book chapter on Functional programming\n\nLesson adapted from R for Data\nScience.\n\n\nPrevious\nmodule:5. Simple Shiny\nNext\nmodule:7. Paramaterized Reports\n\nFor loops are an example of imperative\nprogramming, which differs from functional\nprogramming in that it emphasizes the steps to take to change\nthe state of a computer rather than composing and applying functions.↩︎\nFor a review of R’s list data\nstructure, see the data\nstructures module and Hadley Wickham’s excellent “R for Data\nScience” chapter on vectors.↩︎\nAn anonymous function is a function that is not assigned\nto an identifier. In other words, it is created and used but doesn’t\nexist in the Gobal Environment as a function you can call by name. The\nbenefit of using anonymous functions is that they allow you to quickly\nwrite single-use functions without storing and managing them. For\nexample, in the expression\nlapply(1:5, function(x) print(x)), the anonymous function\nis function(x) print(x). To make it a regular, named\nfunction, we assign it to a value, like\nprint_value <- function(x) print(x). Then it can be\ncalled by name like so: lapply(1:5, print_value).↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_parameterized_reports.html",
      "title": "7. Parameterized reports",
      "description": "How to automate routine reporting\n",
      "author": [],
      "contents": "\n\nContents\nparams\nSet up a report with\nparams\nIterate over a report\nIn-line\nR\nAdditional Resources\n\n\nLearning objectives\nExtend functional programming skills to automate .Rmd\nreports\nUnderstand the available types of reports provided by\n{rmarkdown}\n\n\n\n“Your success in life will be determined largely by your ability to\nspeak, your ability to write, and the quality of your ideas, in that\norder.” — Patrick\nWinston (1943-2019)\n\nData science is the art of combining domain knowledge, statistics,\nmath, programming, and visualization to find order and meaning in\ndisorganized information. Communicating the results of your analyses, or\nability to make data “speak”, is of utmost importance. The modern day\nopen source package ecosystem is full of powerful ways to give voice to\nour analyses.\nAnalyses typically lead to figures, tables, and interpretation of\nthese information. The {rmarkdown}\npackage provides R users with a standardized approach for\nturning R analyses into reports, documents, presentations,\ndashboards, and websites. In this module, we assume familiarity with\n{rmarkdown}, and extend the previous modules on iteration,\nfunctional programming, and reproducible workflows to demonstrate how to\niterate over reports.\n\nReview the introduction to\n{rmarkdown} if needed.\nAccording to R\nMarkdown: The Definitive Guide, some example use cases for creating\na parameterized report include:\nShowing results for a specific geographic location.\nRunning a report that covers a specific time period.\nRunning a single analysis multiple times for different\nassumptions.\nControlling the behavior of knitr (e.g., specify if you want the\ncode to be displayed or not).\nIn this module, we will focus on the first case, and build\nparameterized reports for a set of geographic locations. Throughout this\ncourse, we’ve been working with groundwater elevation data across\nCalifornia counties. Let’s imagine that we want to generate a report on\ngroundwater level trends for a set of counties.\nAlthough the RStudio Interactive Development Environment (IDE)\nencourages knitting RMarkdown documents by clicking a button, we can\nalso knit documents via: rmarkdown::render(). Iterating\nover render() is the key to scaling parameterized reports.\nTo iterate over R Markdown reports, we must first understand how to use\nparams.\n\n\n\n\nparams\nA parameterized .Rmd file takes a set of params (short\nfor “parameters”) in the YAML header, which are bound into a named list\ncalled params and accessed with code from within the .Rmd\nfile with params$<paramater-name>. For example,\nconsider the example YAML:\ntitle: \"My awesome paramaterized report\"\noutput: html_document\nparams:\n  start_date: 2021-01-01\n  watershed: Yuba\n  data: gwl_yuba.csv\nIn the code, we could then access the value \"2021-01-01\"\nwith params$start_date. Similarly,\nparams$watershed will equal \"Yuba\" and\nparams$data will equal \"gwl_yuba.csv\".\n\nSet up a report with\nparams\nLet’s apply params to our task and generate an\nhtml_document for a set of counties. To illustrate, we will\nuse a simplified, pre-processed dataset of 3 counties (Sacramento, Yolo,\nand San Joaquin counties). If you’re motivated to do so, you can use the\nentire groundwater level dataset of > 2 million records to scale the\nprocess to all counties. Read in the data and take a look at the\nfields.\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\n# groundwater level data for Sacramento, Yolo, San Joaquin counties\ngwl <- read_rds(\"data/sac_yolo_sj.rds\")\n\ngwl %>% \n  group_by(SITE_CODE) %>% \n  slice(1) %>% \n  plot()\n\n\n\n\nWe will iterate over the COUNTY_NAME to create three\nreports, one for each county. Copy and paste the following code into a\nnew file reports/gwl_report.Rmd\n\n---\ntitle: \"`r paste(params$county, 'Groundwater Levels')`\"\noutput: html_document\nparams:\n  county: \"placeholder\"\n---\n\n<br>\n\n```{r, echo = FALSE, message = FALSE, error = FALSE, warning = FALSE}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\nlibrary(mapview)\nlibrary(maps)\nlibrary(DT)\nmapviewOptions(fgb = FALSE)\n\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, out.width = \"100%\")\n\n# filter all groundwater level data (already loaded into memory) by \n# the supplied county\nd <- filter(gwl, COUNTY_NAME == params$county)\n\n# extract the county spatial file\ncounty_sf <- st_as_sf(map(\"county\", plot = FALSE, fill = TRUE)) %>% \n  filter(ID == paste0(\"california,\", tolower(params$county)))\n\n```\n\n\nThis report shows groundwater levels in `r params$county` county.\n\nDates range from `r min(d$MSMT_DATE, na.rm = TRUE)` to `r max(d$MSMT_DATE, na.rm = TRUE)`.\n\nData source: [DWR Periodic Groundwater Level Database](https://data.cnra.ca.gov/dataset/periodic-groundwater-level-measurements).\n\n<br>\n\n## Distribution of measurements over time\n\n50% of measured values occur on or after `r median(d$MSMT_DATE, na.rm = TRUE)`.\n\n```{r hist, echo = FALSE}\nd %>% \n  ggplot() +\n  geom_histogram(aes(MSMT_DATE)) +\n  theme_minimal() +\n  labs(title = \"\", x = \"\", y = \"Count\")\n```\n\n\n<br>\n\n## Monitoring sites\n\n```{r map, echo = FALSE}\n# mapview of county outline\ncounty_mv <- mapview(\n  county_sf, layer.name = paste(params$county, \"county\"), \n  lwd = 2, color = \"red\", alpha.regions = 0\n)\n\n# mapview of monitoring points\npoints_mv <- d %>% \n  group_by(SITE_CODE) %>% \n  slice(1) %>% \n  select(-MSMT_DATE) %>% # remove msmt date b/c its irrelevant\n  mapview(layer.name = \"Monitoring stations\")\n\ncounty_mv + points_mv\n```\n\n<br>\n\n## All groundwater levels\n\n```{r plot, echo = FALSE}\n# interactive hydrograph\np <- ggplot(d, aes(MSMT_DATE, WSE, color = SITE_CODE)) + \n  geom_line(alpha = 0.5) +\n  guides(color = FALSE)\nplotly::ggplotly(p)\n```\n\n<br> \n\n```{r dt, echo = FALSE}\n# data table of median groundwater level per site, per year\nd %>% \n  select(-c(\"COUNTY_NAME\", \"WELL_DEPTH\")) %>%\n  st_drop_geometry() %>% \n  mutate(YEAR = lubridate::year(MSMT_DATE)) %>% \n  group_by(SITE_CODE, YEAR) %>% \n  summarise(WSE_MEDIAN = median(WSE, na.rm = TRUE)) %>%\n  ungroup() %>% \n  DT::datatable(\n    extensions = 'Buttons', options = list(\n      dom = 'Bfrtip',\n      buttons = \n        list('copy', 'print', list(\n          extend  = 'collection',\n          buttons = c('csv', 'excel', 'pdf'),\n          text    = 'Download'\n        ))\n    )\n  )\n```\n\n\n***\n\nReport generated on `r Sys.Date()`.\n\n\nPause and think\nTake a moment to read the .Rmd file above and see what\nit does. Notice where params$county is located in the\ndocument. Particularly, in the first code chunk it’s used to filter the\ngroundwater level data (assumed to be in memory so we only load it once\nrather than every time we run this script) down to the county\nparameter.\nd <- filter(gwl, COUNTY_NAME == params$county)\nNext, how might you write an .Rmd file like the one\nabove and test that everything looks the way you want it to before\ncalling it done? In other words, would you start by writing\nparams$county in all places it needs to be or start with\none county, make sure everything works, and then substitute in\nparams$county?\n\n\nIterate over a report\nFinally, we create a vector of counties we want to write reports for\nand iterate over them. We also need to specify the output location of\neach file. Since we are writing html_documents, the file\nextension is .html. Using walk2() from our\nfunctional programming toolkit, we can pass in the counties vector and\nthe output file paths into rmarkdown::render() and silently\nwrite the files.\n\n\n# unique counties to write reports for\ncounties  <- unique(gwl$COUNTY_NAME)\n\n# output file names\nfiles_out <- tolower(counties) %>% \n  str_replace_all(\" \", \"_\") %>% \n  paste0(., \".html\")\n\n# silently (walk) over the county names and file names, \n# creating a report for each combination\nwalk2(\n  counties, \n  files_out,\n  ~rmarkdown::render(\n    input       = \"reports/gwl_report.Rmd\", \n    output_file = here(\"reports\", .y),\n    params      = list(county = .x)\n  )\n)\n\n\n\nOpen and explore each of the files that were written.\n\n\nPause and think\nIf we wanted to automate reports like this and have them published\nonline or emailed to our team every morning at 7AM, what tools would we\nneed?\nHint: see the automation module\nsection on task schedulers.\n\n\nIn-line R\nWithin an .Rmd we can insert R code in-line\nusing the following syntax:\n\n\n`r <function>`\n\n\n\nSo for instance we can write a string like:\n\nThe mean of 1 and 3 is `r mean(c(1,3))`.\n\nAnd when the document knits, we get: The mean of 1 and 3 is 2.\nConsider this as an approach to add specific output about each site\nin the text narrative.\n\nAdditional Resources\nAlthough we only demonstrated one type of output report in this\nmodule, the html_document, there are many other output\nformats that you can parameterize and iterate over, including Word\ndocuments, PDFs,\nflexdashboards,\nand presentations.\n\nView some of the available R Markdown output\nformats here.\nTo dig deeper, see the official RMarkdown\nguide for paramaterized reports.\n\n\nPrevious\nmodule:6. Iteration\nNext\nmodule:8. Advanced spatial\n\n\n\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_project_management.html",
      "title": "3. Project Management & Data Organization",
      "description": "Approaches towards organization and efficiency.\n",
      "author": [],
      "contents": "\n\nContents\nFirst principles\n.RProfile\nGlobal (user-level) .Rprofile\nLocal (project-level) .Rprofile\nUsing .Rprofile\n\n.Renviron\nStrategies to organize projects/code\nAbstracting Functions from Code\n\n{renv}\n\n\nLearning objectives\nImplement best practices for reproducible data science\nCreate and use an RStudio project (.RProj)\nUnderstand how and when to modify .RProfile and .Renviron files\nApply strategies to organize functions and scripts\nUnderstand package environments and how to manage them with {renv}\n\n\nFirst principles\n\n\n\nFigure 1: What to avoid building (Source: https://xkcd.com/2054/).\n\n\n\nThe first step in any data science project is to set up and maintain a clean, predictable development environment. As you accumulate raw data, write code, and generate results, things can get messy if you don’t stick to good programming naming and organization habits. In this module we’ll cover how to keep your projects organized and consistent, which will make your projects more reproducible, keep your workflows efficient, firm up your code to stand the test of time, and give your code structure so it’s easy to maintain when things break or you need to revisit it down the line.\nA development environment is the set of tools you use to process data and the toolshed. R and packages are your tools, and an RProject is your toolshed.\n\nREVIEW\n\nAlthough this is an intermediate level course, we will revisit introductory material on “Project Management” because no matter your skill level in R, strategic project management remains fundamental. Subsequent modules in this course assume familiarity .Rprojects, naming conventions, and general best practices.\n\n\n\nAfter reviewing core introductory topics, we will discuss .RProfile and .Renviron files and when to use them. To review core best practices we recommend particular familiarity with the following concepts:\nFilenaming\nRelative filepaths and the {here} package\nBest practices for project organization\n\n.RProfile\nIf you have user- or project-level code that needs to be run every time you start up R, customizing your .RProfile can streamline this operation.\nThe .RProfile file is an actual (hidden) file that is automatically sourced (run) as R code when you open an R session. A .RProfile file can live in the project root directory, or the user’s home directory, although only one .RProfile can be loaded per R session or RProject. If a project-level .Rprofile exists, it supersedes the user-level .Rprofile.\nYou can also “show hidden files” (instructions for Mac and PC) and edit .RProfile in a text editor. You can also do so in R with file.edit(\"~/.RProfile\").\nGlobal (user-level) .Rprofile\nThe easiest and most consistent way to edit your .RProfile file across operating systems is with the {usethis} package. Run usethis::edit_r_profile() to open your user-level (or global) .RProfile. This is the default .Rprofile that will be used for any and all projects unless you have a local (project-level) .Rprofile.\n\n\nusethis::edit_r_profile()\n\n\nLocal (project-level) .Rprofile\nWe can create or edit a local or project-level .RProfile with the scope argument inside the edit_r_profile function. Remember, a project-level .RProfile will supersede the global .Rprofile.\n\n\nusethis::edit_r_profile(scope = \"project\")\n\n\n\nUsing .Rprofile\nTo illustrate how .RProfile works, let’s do something cool and useless. We’ll write a short program that greets us with a random inspirational quote, and then we’ll put in .RProfile so it runs whenever we start up R.\nThe {cowsay} package is a fun way to print text animal art.\n\n\ncowsay::say(what = \"hello world!\", by = \"cow\")\n\n\n ----- \nhello world! \n ------ \n    \\   ^__^ \n     \\  (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||\n\nLet’s randomize the animal displayed and make the message it says one of the motivational quotes found at this Github repo, copy and paste the code into our .RProfile, and restart R.\n\n\nlibrary(cowsay) # animals!\nlibrary(glue)   # pasting things together\n\n# get vector of all animals\nanimals <- names(cowsay::animals)\n\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n\n# get dataframe of inspirational quotes\nquotes  <- readr::read_csv(glue(\"https://gist.githubusercontent.com/{repo}/{csv}\"))  \n\n# make full quote\nquotes$full_quote  <- glue(\"{quotes$Quote} - {quotes$Author}\")\n\n# now use it!\ncowsay::say(sample(quotes$full_quote, 1), by = sample(animals, 1))\n\n\n ----- \nA good rest is half the work. - NA \n ------ \n    \\   \n     \\\n                   ____\n                _.' :  `._\n            .-.'`.  ;   .'`.-.\n   __      / : ___\\ ;  /___ ; \\      __\n  ,'_ \"\"--.:__;\".-.\";: :\".-.\":__;.--\"\" _`,\n  :' `.t\"\"--.. '<@.`;_  ',@>` ..--\"\"j.' `;\n       `:-.._J '-.-'L__ `-- ' L_..-;'\n          \"-.__ ;  .-\"  \"-.  : __.-\"\n             L ' /.------.\\ ' J\n             \"-.   \"--\"   .-\"\n             __.l\"-:_JL_;-\";.__\n         .-j/'.;  ;\"\"\"\"  / .'\\\"-.\n         .' /:`. \"-.:     .-\" .';  `.\n      .-\"  / ;  \"-. \"-..-\" .-\"  :    \"-.\n  .+\"-.  : :      \"-.__.-\"      ;-._   \\\n  ; \\  `.; ;                    : : \"+. ;\n  :  ;   ; ;                    : ;  : \\:\n  ;  :   ; :                    ;:   ;  :\n  : \\  ;  :  ;                  : ;  /  ::\n  ;  ; :   ; :                  ;   :   ;:\n  :  :  ;  :  ;                : :  ;  : ;\n  ;\\    :   ; :                ; ;     ; ;\n  : `.\"-;   :  ;              :  ;    /  ;\n ;    -:   ; :              ;  : .-\"   :\n  :\\     \\  :  ;            : \\.-\"      :\n  ;`.    \\  ; :            ;.'_..--  / ;\n  :  \"-.  \"-:  ;          :/.\"      .'  :\n   \\         \\ :          ;/  __        :\n    \\       .-`.\\        /t-\"\"  \":-+.   :\n     `.  .-\"    `l    __/ /`. :  ; ; \\  ;\n       \\   .-\" .-\"-.-\"  .' .'j \\  /   ;/\n        \\ / .-\"   /.     .'.' ;_:'    ;\n  :-\"\"-.`./-.'     /    `.___.'\n               \\ `t  ._  /  bug\n                \"-.t-._:'\n  \n\nrm(animals, quotes) # remove the objects we just created\n\n\n\n.Renviron\nSometimes you need to store sensitive information, like API Keys, Database passwords, data storage paths, or general variables used across all scripts. We don’t want to accidentally share these information, accidentally push them to Github, or copy and paste them over and over again from script to script. We also might want to build a codebase that relies on a few variables that another user can set in their own system in a way that works for them. Environmental variables are the way to address all of these concerns.\nEnvironmental variables are objects that store character strings. They are accessible from within R upon startup. To view all environmental variables, use Sys.getenv() or Sys.info(). You can also pull out one environmental variable at a time by passing in its name, for instance:\n\n\nSys.info()[[\"user\"]]\n\n[1] \"rapeek\"\n\nYou can set your own environmental variables which are stored in another hidden file called .Renviron (this is the Python analog of .env). Keep in mind, .Renviron files typically contain lists of environmental variables that look similar to R code but it is actually not running R code…so don’t put R code in your .Renviron file! If we need to run R code when starting up R, we use .RProfile.\nTo illustrate the use of .Renviron, we run usethis::edit_r_environ(), add the environmental variable ANIMAL = \"cat\", save, and restart R.\n\n\nusethis::edit_r_environ()\n\n\nWe can access our environmental variable as follows (remember you need to restart R for changes to take effect, try Session > Restart R):\n\n\nSys.getenv(\"ANIMAL\")\n\n\nWe can use our environmental variable, for instance, in a function.\n\n\ninspire_me <- function(animal){\n\n  # get pieces to make link\n  repo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\n  csv  <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n  \n  # silently read dataframe\n  suppressMessages(\n    quotes  <- readr::read_csv(\n      glue::glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")\n    )  \n  )\n  \n  # paste together the full quote\n  quotes$full_quote  <- paste0(quotes$Quote, \" -\", quotes$Author)\n  \n  # make a user-specified animal say the quote\n  cowsay::say(sample(quotes$full_quote, 1), by = animal)\n\n}\n\n# have the environmental variable say a quote\ninspire_me(Sys.getenv(\"ANIMAL\"))\n\n\nAlthough it may not appear powerful in this trivial example, when a project grows substantially large and complex, or when managing multiple sensitive passwords and access tokens, environmental variables are a standard approach that are widely used.\nImportant note, both these files .Renviron and .Rprofile need to end with a blank newline at the end. If this isn’t present, the last line of the file is ignored, and there isn’t a message or error associated with this. The usethis functions typically take care of this for you, but be aware of it just in case!\n\nPause and think\n\nIn the example function above, we might notice that reading in a url from a csv every time we run inspire_me() is a lot of unnecessary overhead. Where else might we be able to read that csv in automatically when R starts up, so that it’s available for our inspire_me() function, and that we only need to read it once?\n\n\nClick for Answers!\n\nWe can move read step of the csv into a project-level RProfile, so it’s available to the project where we need this csv, but not to any general R session we may open outside of the project.\n.RProfile\n\n\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv  <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\n\n# silently read dataframe\nsuppressMessages(\n  quotes  <- readr::read_csv(\n    glue::glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")\n  )  \n)\n\n# paste together the full quote\nquotes$full_quote  <- paste0(quotes$Quote, \" -\", quotes$Author)\n\n\nModified function\n\n\ninspire_me <- function(animal){\n\n  # make a user-specified animal say the quote\n  cowsay::say(sample(quotes$full_quote, 1), by = animal)\n\n}\n\n\n\n\n\nStrategies to organize projects/code\nBest practices for writing code across languages typically recommend package imports and function definitions at the top of a script, followed by code. For example, a script may look like this:\n\n\n# import packages\nlibrary(tidyvere)\nlibrary(fs)\n\n# define functions\nmy_first_function <- function(){\n  print(\"hello\")\n}\n\nmy_second_function <- function(){\n  print(\"world\")\n}\n\n# run scripts/functions\nmy_first_function()\nmy_second_function()\n\n\nThese approaches work well when scripts are relatively simple, but as a project grows large and complex, it’s best practice to move functions into another script or set of scripts, and break up your workflow into discrete steps.\nFor instance, although the inspire_me() function above is relatively simple, we can pretend that the read, transform, and print steps carried out in the function were themselves long functions in part of a much more complex, real-world workflow. Imagine we created a script called functions.R that contained the following code. Don’t worry if you haven’t seen purrr::walk() before. We’ll cover this in a later module on iteration, and all you need to know about it now is that it “walks” over each input and applies a function. In this case, we apply the require() function to a vector of package names to load them.\nNotice that all functions start with f_. This prefix makes it easy to read in a script and takes advantage of auto-complete.\n\n\n# list packages in a vector and load them all\npkgs <- c(\"readr\", \"cowsay\")\npurrr::walk(pkgs, require, character.only = TRUE)\n\n# read quotes from a url\nf_read_data <- function(url){\n  suppressMessages(\n    quotes  <- read_csv(url)  \n  )\n  return(quotes)\n}\n\n# paste the quote to the author\nf_preprocess_data <- function(d){\n  d$full_quote  <- paste0(d$Quote, \" -\", d$Author)\n  return(d)\n}\n\n# print a random animal and a random quote\nf_inspire_me <- function(d){\n  animals <- names(animals)\n  say(sample(d$full_quote, 1), by = sample(animals, 1))\n}\n\n\nWe can call this script using source() to load or import these functions into our environment where they are available for use, just as we load a library.\n\n\nsource(\"scripts/functions.R\")\n\n\nAbstracting Functions from Code\nHowever, this is hardly a satisfying solution because in a real project, our pretend functions above may grow quite large, and we will likely add more and more functions. Eventually, a single script may hold them all, and something like functions.R may become many hundreds of lines long, making it difficult to sift through, debug, or add new lines of code. A better organizational approach which makes things easier to maintain over time is to move all our functions to a directory /functions, and store them all as separate files named after their function name:\nA good rule of thumb to follow is to try and keep scripts less than 150 lines in length. When scripts approach this length, they become difficult to keep track of, and there were likely missed opportunities to refactor the script into separate functions and modules.\nSave as /scripts/functions/f_read_data.R\n\n\n# read quotes from a url\nf_read_data <- function(url){\n  suppressMessages(\n    quotes  <- read_csv(url)\n  )\n  return(quotes)\n}\n\n\nSave as /scripts/functions/f_preprocess_data.R\n\n\n# paste the quote to the author\nf_preprocess_data <- function(d){\n  d$full_quote  <- paste0(d$Quote, \" -\", d$Author)\n  return(d)\n}\n\n\nSave as /scripts/functions/f_inspire_me.R\n\n\n# print a random animal and a random quote\nf_inspire_me <- function(d){\n  animals <- names(animals)\n  say(sample(d$full_quote, 1), by = sample(animals, 1))\n}\n\n\nThe functions folder in the root project directory should now look like this:\n\n\n\nNow in our /scripts directory, we create a script, 01_control.R to source our functions and use them. Be sure to restart R to clear your environment before sourcing this control script so we know we are working from a clean slate.\nSave as /scripts/01_control.R and run.\n\n\n# packages needed for this script\npkgs <- c(\"readr\", \"cowsay\", \"tidyverse\", \"glue\")\nwalk(pkgs, require, character.only = TRUE)\n\n# silently source all functions using the purrr::walk function\nfns <- fs::dir_ls(\"scripts/functions\")\nwalk(fns, ~source(.x))\n\n# define the url where quotes are located\n# get pieces to make link\nrepo <- \"JakubPetriska/060958fd744ca34f099e947cd080b540\"\ncsv <- \"raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\nurl <- glue(\"https://gist.githubusercontent.com/{repo}/{csv}\")  \n\n# use all of our functions\nf_read_data(url) %>% \n  f_preprocess_data() %>% \n  f_inspire_me()\n\n\n\n\n ----- \nYou can't stop the waves, but you can \nlearn to surf. -Jon Kabat-Zinn \n ------ \n    \\   \n     \\\n                   ____\n                _.' :  `._\n            .-.'`.  ;   .'`.-.\n   __      / : ___\\ ;  /___ ; \\      __\n  ,'_ \"\"--.:__;\".-.\";: :\".-.\":__;.--\"\" _`,\n  :' `.t\"\"--.. '<@.`;_  ',@>` ..--\"\"j.' `;\n       `:-.._J '-.-'L__ `-- ' L_..-;'\n          \"-.__ ;  .-\"  \"-.  : __.-\"\n             L ' /.------.\\ ' J\n             \"-.   \"--\"   .-\"\n             __.l\"-:_JL_;-\";.__\n         .-j/'.;  ;\"\"\"\"  / .'\\\"-.\n         .' /:`. \"-.:     .-\" .';  `.\n      .-\"  / ;  \"-. \"-..-\" .-\"  :    \"-.\n  .+\"-.  : :      \"-.__.-\"      ;-._   \\\n  ; \\  `.; ;                    : : \"+. ;\n  :  ;   ; ;                    : ;  : \\:\n  ;  :   ; :                    ;:   ;  :\n  : \\  ;  :  ;                  : ;  /  ::\n  ;  ; :   ; :                  ;   :   ;:\n  :  :  ;  :  ;                : :  ;  : ;\n  ;\\    :   ; :                ; ;     ; ;\n  : `.\"-;   :  ;              :  ;    /  ;\n ;    -:   ; :              ;  : .-\"   :\n  :\\     \\  :  ;            : \\.-\"      :\n  ;`.    \\  ; :            ;.'_..--  / ;\n  :  \"-.  \"-:  ;          :/.\"      .'  :\n   \\         \\ :          ;/  __        :\n    \\       .-`.\\        /t-\"\"  \":-+.   :\n     `.  .-\"    `l    __/ /`. :  ; ; \\  ;\n       \\   .-\" .-\"-.-\"  .' .'j \\  /   ;/\n        \\ / .-\"   /.     .'.' ;_:'    ;\n  :-\"\"-.`./-.'     /    `.___.'\n               \\ `t  ._  /  bug\n                \"-.t-._:'\n  \n\nsource() is the key to chaining together many scripts. In the example above, we were able to abstract functions into a separate folder which makes keeping track of them much easier than if they cluttered our control script.\nIt’s also a good rule of thumb to keep each line of code less than 75 characters long to ease readability.\n\nLearn more\n\nSeparating all functions into standalone scripts is not a revolutionary idea – in fact, this is precisely how R packages are written! For example, see the {dplyr} github repo’s /R folder which contains all dplyr functions in one directory. When you call library(dplyr) you’re essentially sourcing all of these functions into your environment.\n\n\nIf project management and reproducible data pipelines are interesting to you, check out the {targets} R package. A similar framework for Shiny Apps exists called {golem}, which also includes {usethis}-like commands that streamline common chores in Shiny App development.\n\n\n\n{renv}\nWe use RProjects because we expect that whoever else opens your code on their machine is likely to have a different project root path, and using an RProject ensures your code is portable between different computers with different root project paths (e.g., ~/Documents/Github/myproject versus C:\\Users\\louis\\Documents\\myproject).\nDevelopment environments are similar. When we work in R – or any programming language for that matter – we use a snapshot of package versions based on when we downloaded and installed them [e.g. with install.packages()]. You can check the version of the installed packages loaded into your current environment with sessionInfo().\n\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] glue_1.7.0   cowsay_0.9.0 purrr_1.0.2  knitr_1.45  \n\nloaded via a namespace (and not attached):\n [1] bit_4.0.5         jsonlite_1.8.8    compiler_4.3.2   \n [4] highr_0.10        crayon_1.5.2      tidyselect_1.2.0 \n [7] parallel_4.3.2    rmsfact_0.0.3     jquerylib_0.1.4  \n[10] png_0.1-8         yaml_2.3.8        fastmap_1.1.1    \n[13] readr_2.1.5       R6_2.5.1          curl_5.2.0       \n[16] tibble_3.2.1      distill_1.6       bslib_0.6.1      \n[19] pillar_1.9.0      tzdb_0.4.0        rlang_1.1.3      \n[22] utf8_1.2.4        cachem_1.0.8      xfun_0.41        \n[25] fs_1.6.3          sass_0.4.8        bit64_4.0.5      \n[28] memoise_2.0.1     cli_3.6.2         withr_3.0.0      \n[31] magrittr_2.0.3    digest_0.6.34     vroom_1.6.5      \n[34] rstudioapi_0.15.0 fortunes_1.5-4    hms_1.1.3        \n[37] lifecycle_1.0.4   vctrs_0.6.5       downlit_0.4.3    \n[40] evaluate_0.23     fansi_1.0.6       rmarkdown_2.25   \n[43] tools_4.3.2       usethis_2.2.2     pkgconfig_2.0.3  \n[46] htmltools_0.5.7  \n\nThe version number is the string of numbers listed after a package name and underscore.\nSimilarly, you can use installed.packages() to view information on all of your installed packages.\nWhen packages change between versions, changes are typically designed to fix bugs or improve performance, but sometimes, they can break code. Thus, collaborative work on a project may be challenged by people working on the same code but with different versions of packages.\nEven the version of R itself changes, although base R changes very slowly and the R Core Team tries to make new versions of R backwards-compatible to not break scripts written before potentially breaking changes.\nThe solution to this problem is for everyone to use the same versions of packages (and R), which is to say that collaborators should use the same development environment. This is a common concept across programming languages.\n{renv} manages your package environment and makes it easy to share it with others by creating and curating a “lock” file (renv.lock) in the root project directory. When starting a project, create the file with renv::init(), install packages as you go along, and update the lockfile with renv::snapshot(). When a collaborator opens your project (for example, after cloning it from Github), all they need to do is open the .RProj file and {renv} will automatically set up the development environment captured in the lock file.\nThe renv.lock file is a JSON file with information on the version of R and the versions of all packages used by the project.\nIf you find yourself needing to share important analyses, perhaps that run on a production server, you should look into {renv}. For most day-to-day data science that you don’t plan on sharing or working collaboratively on, it may be unnecessary.\n\n\nPrevious module: 2. Git Next module: 4. Interactive Visualization\n\n\n\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_reproducible_workflows.html",
      "title": "Reproducible workflows and automation",
      "description": "How to make computers do your bidding  \n",
      "author": [],
      "contents": "\n\nContents\nReproducible workflows and automation\nCase Study: groundwater level trend analysis\nOur Task\nImport Data\nMap/Visualize the Data\nModel Trends in Data\nNested Lists\n\nHow to write helpful functions\n1. Download\n2. Read\n3. Clean\n4. Model\n5. Visualize\n6. Write\n\n\nTask scheduling\nLogging\nWhen to not automate\nCode as instructions\nAdditional resources\n\n\nLearning objectives\nPractice functional programming with iterative workflows to power reproducible analyses\nUnderstand how to use nested lists within dataframes\nPractice conditional execution\nDiscuss task schedulers and strategies to fully automate workflows\n\n\nReproducible workflows and automation\nIn this module, we extend lessons in functional programming from the previous module on iteration, as well as best practices discussed in the project management module to approach a case study. Special attention is paid to automation.\nAs you expand your ability to write R code, automation becomes possible. “Automation” can mean different things, but generally refers to the process of replacing time that you would spend on a workflow with a computer program, or an R script in the context of this course. The beauty of automation is that you can create workflows that would be nearly impossible to complete in a human lifetime, and run them thousands of times, all while you’re out on a coffee break. In some circles, this is sometimes jokingly referred to as, “making computers do your bidding.” Automation happens whenever you write a program that saves you time, freeing you to do other things.\nThe very act of writing an R script (or program) to perform analyses is automation, because you have automated the need to point and click through various tasks and type instructions at particular times, such as in a traditional point-and-click workflow.\nHowever, any R program you write still needs a human to run it, and perhaps you have not one, but a chain of 5 scripts that are evaluated sequentially to import, clean, model, visualize, and output data. Someone needs to run the program. Another step up the automation ladder is to wrap these 5 scripts in a single program that calls them all in sequence, like a master switch. Instead of running 5 scripts, now we only need to run 1.\nThere is still further automation potential. We can write another program to call the “master switch” at specified times, fully remove ourselves from the pipeline, and allow the program will run all on its own (although it may require maintenance from time to time).\n\nAny time we have a fully automated program that runs on its own, logging should be implemented. Logging allows us to monitor an automated process, and diagnose and quickly fix anything that went wrong.\n\n\n\nFigure 1: Automation is like watering a row of plants. Each program is like a bucket that waters one plant at a time. Wrapping the programs into a single “master switch” control script is like installing drip irrigation so that all plants are watered at once. In order to fully automate, we can set a task scheduler, which is like adding a timer onto our drip irrigation system.\n\n\n\nWe should always strive for at least a degree of automation represented by the left-most figure, with a set of programs for each step in our workflow, and functions abstracted out of these programs1 to improve readability and project organization.\nWhether or not to write wrapper programs or configure task schedulers depends on the needs of the workflow. Guidance on when and when to not automate will be discussed in this module.\nTo demonstrate varying degrees of automation in a real-life example and extend our practice of functional programming, we will focus on a case study using the DWR’s Periodic Groundwater Level Database.\n\nCase Study: groundwater level trend analysis\nThe California Department of Water Resources publishes a routine report on groundwater level conditions that shows groundwater level trends on a point-by-point basis. Short-term (i.e., 1, 3, and 5 year) and long-term trends (i.e., 10 and 20 years) are displayed in the report.\n\n\n\n\nFigure 2: The “California Groundwater Conditions Update - Spring 2020”, DWR (2020) demonstrates the rate and direction of groundwater level change over different time periods, shown here is the 20000-2020 trend.\n\n\n\nOur Task\nLet’s imagine our task is to assess change in groundwater level across all stations and plot it on a map, just as we see in the report. In the sections that follow we will calculate groundwater level change trends at representative monitoring points in the Periodic groundwater level database we have been using in this course, and demonstrate best practices in creating a reproducible, automatable pipeline.\nAlong the way, we will also cover key concepts including:\nconditional execution\nfunctional programming with nested lists\ntips for writing helpful functions\ntask scheduling\nlogging\nImport Data\nTo begin, let’s import data and join it. If your internet connection is slow, use: d <- read_csv(here(\"data\", \"gwl_sac.csv\")) to skip the step below and read in a copy of the data from a github repository. If you want to see how to download this data, follow along with the code below2:\n\n\nlibrary(tidyverse)\nlibrary(here)\n\n# urls for data\nbase_url <- \n  \"https://data.cnra.ca.gov/dataset/dd9b15f5-6d08-4d8c-bace-37dc761a9c08/resource/\"\nurls <- paste0(\n  base_url, \n  c(\"af157380-fb42-4abf-b72a-6f9f98868077/download/stations.csv\",\n    \"bfa9f262-24a1-45bd-8dc8-138bc8107266/download/measurements.csv\",\n    \"f1deaa6d-2cb5-4052-a73f-08a69f26b750/download/perforations.csv\")\n)\n\n# create directory and define paths to save downloaded files\ndir.create(here(\"data\", \"pgwl\"))\nfiles_out <- here(\"data\", \"pgwl\", basename(urls))\n\n# download files\nwalk2(urls, files_out, ~download.file(.x, .y))\n\n# read and combine - use datatable for speed and memory management\nd <- map(files_out, ~data.table::fread(.x)) %>% \n  reduce(left_join, by = \"SITE_CODE\")\n\n# d contains ~2.3 million observations: filter to Sacramento County\nd <- filter(d, COUNTY_NAME == \"Sacramento\")\n\n\n\n\nWe use copied URLs here, but for large, standardized websites you can use {rvest} and SelectorGadget to automate scraping URLs and other data. We also used purrr::reduce() to recursively left_join() a list of dataframes. This is a powerful concept, because the alternative is to manually left_join() each list element. Read up on ?reduce to learn more.\n\n\n\n\nMap/Visualize the Data\nNow that we have our Sacramento data in R either by downloading it with the code above or by reading it in directly with read_csv, let’s transform it to an sf object and plot just to sanity check that we’re indeed looking at Sacramento County groundwater levels.\n\n\nlibrary(sf)\nlibrary(mapview)\nmapviewOptions(fgb = FALSE) # setting to help render map\n\n# select only columns we will use in this analysis\nd <- d %>% \n  select(SITE_CODE, MSMT_DATE, WSE, LONGITUDE, LATITUDE, WELL_DEPTH)\n\n# convert to sf object\nd <- st_as_sf(d, coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4269) \n\n# take the first observation per SITE_CODE since they all have \n# the same location, and plot\ngroup_by(d, SITE_CODE) %>% \n  slice(1) %>% \n  ungroup() %>% \n  mapview(popup = NULL, layer.name = \"Groundwater stations\")\n\n\n\n\nModel Trends in Data\nRecall that we want to assess change in groundwater level across all stations and plot it on a map. In other words, assume we have the linear model describing a set of groundwater elevations (\\(\\hat{Y}_i\\)), as a function of time (\\(X_i\\)), proportional to some unknown y-intercept (\\(\\beta_0\\)) and coefficient (\\(\\beta_1\\)) for a set of observations at times \\(i = 1, ..., n\\):\n\\[\n\\hat{Y}_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i\n\\]\nTime alone is actually a poor predictor of groundwater level (and shouldn’t be interpreted in a strict statistical sense), but for our purposes, the linear model above is useful because the \\(\\beta_1\\) coefficient can be interpreted as the average groundwater level trend at a monitoring site over some time frame. If \\(\\beta_1 > 0\\), the average groundwater level is increasing, and if \\(\\beta_1 < 0\\), it is decreasing. The magnitude of \\(\\beta_1\\) tells us by how much the groundwater level is increasing or decreasing and can be plotted per monitoring site, and compared to other monitoring sites to understand the spatial distribution of groundwater level change in a region. In this example, our region of interest is Sacramento County, but in the spirit of automation, we will create a function that allows us to apply this workflow to any county, or even the entire state of California.\nTo apply a linear model to each monitoring site, we use the lm() function in base R. If we apply the function to our entire dataset, we end up with a linear model of all groundwater levels in Sacramento county:\n\n\nggplot(d) +\n  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +\n  geom_smooth(aes(MSMT_DATE, WSE), method = \"lm\", se = FALSE)\n\n\n\n\nLet’s filter out that one outlier (groundwater levels don’t just spike by hundreds of feet so rapidly like that), filter the date range of the data to the last 20 years, and replot. We can see a slight negative groundwater level trend over the basin, but this spatial average may be skewed by monitoring stations with more data than others and increased data density in more recent years.\n\n\n# recent data without the obviously erroneous outlier\nd_recent <- filter(d, WSE >-200 & MSMT_DATE >= lubridate::ymd(\"2000-01-01\"))\n\nggplot(d_recent) +\n  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5) +\n  geom_smooth(aes(MSMT_DATE, WSE), method = \"lm\", se = FALSE)\n\n\n\n\nHowever, this isn’t quite what we need. What we need is a linear model for each and every one of the individual stations in this dataset. {ggplot2} is great for visualizing what we are looking for. For example, here are a few of the monitoring sites, each with a unique linear model, which can be viewed with facet_wrap().\n\n\nd_recent %>% \n  filter(SITE_CODE %in% unique(d$SITE_CODE)[1:7]) %>% \n  ggplot() +\n  geom_line(aes(MSMT_DATE, WSE)) +\n  geom_smooth(aes(MSMT_DATE, WSE), method = \"lm\", se = FALSE) +\n  facet_wrap(~SITE_CODE)\n\n\n\n\nNow, it’s time to calculate those numbers with lm(). We will use {broom} to tidy up model results in a clean dataframe. Note that because the class of our MSMT_DATE column is \"POSIXct\", the units returned for our coefficient \\(\\beta_1\\) will be in \\(ft/sec\\) rather than the more intuitive \\(ft/yr\\), thus we define a function to handle the conversion from \\(ft/sec\\) to \\(ft/yr\\). For the entire dataset the average annual change in groundwater level over the 20 year timeframe is:\n\n\n# convert from seconds to years\nsec_to_yr <- function(sec){\n  yr <- sec * 31557600\n  return(yr)\n}\n\n# create the linear model and view output\nm <- lm(WSE ~ MSMT_DATE, data = d_recent) \n\nm\n\n\n\nCall:\nlm(formula = WSE ~ MSMT_DATE, data = d_recent)\n\nCoefficients:\n(Intercept)    MSMT_DATE  \n  4.614e+01   -4.462e-08  \n\n# tidy the model output with broom and convert ft/sec to ft/yr\nbroom::tidy(m) %>% \n  slice(2) %>% \n  mutate(estimate_ft_yr = sec_to_yr(estimate))\n\n\n# A tibble: 1 × 6\n  term           estimate     std.error statistic   p.value estimate_ft_yr\n  <chr>             <dbl>         <dbl>     <dbl>     <dbl>          <dbl>\n1 MSMT_DATE -0.0000000446 0.00000000192     -23.3 1.75e-117          -1.41\n\nFrom above, we see that the average annual decline is -1.41 \\(ft/yr\\).\nNested Lists\nWe could turn this linear model call into a function and loop over it with a for loop, saving each model result in a list element. We could also map this function over a list of dataframes created by split(d, d$SITE_CODE) (or the tidyverse equivalent, group_split(d, SITE_CODE)), but there’s a better way with functional programming.\nWe will use nested lists, specifically with the tidyr::nest() function, which allows us to convert a dataframe grouped by some variable (in our case it will be SITE_CODE) into a dataframe with a column that “nests” the data for each unique grouping variable. For instance:\n\n\nl <- d_recent %>% \n  group_by(SITE_CODE) %>% \n  nest() \nl\n\n\n# A tibble: 302 × 2\n# Groups:   SITE_CODE [302]\n   SITE_CODE          data          \n   <chr>              <list>        \n 1 384931N1212618W001 <sf [7 × 4]>  \n 2 384742N1213146W001 <sf [7 × 4]>  \n 3 384798N1212614W001 <sf [38 × 4]> \n 4 384532N1212856W001 <sf [18 × 4]> \n 5 384425N1213031W001 <sf [38 × 4]> \n 6 384260N1212853W001 <sf [19 × 4]> \n 7 384251N1213346W001 <sf [7 × 4]>  \n 8 384526N1211695W001 <sf [112 × 4]>\n 9 384511N1212360W001 <sf [19 × 4]> \n10 384252N1211997W001 <sf [9 × 4]>  \n# … with 292 more rows\n\nThe resulting dataframe has a number of rows equal to the length of unique values in the SITE_CODE column, and a column called data with nested dataframes, one for each SITE_CODE (grouping variable). Notice that in the data column above, each nested dataframe has 4 columns, but a differing number of rows (groundwater level observations). This makes sense because each monitoring station has the same number of fields, but a different number of samples. Let’s inspect the first dataframe:\n\n\nl$data[[1]]\n\n\nSimple feature collection with 7 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.262 ymin: 38.4931 xmax: -121.262 ymax: 38.4931\nGeodetic CRS:  NAD83\n# A tibble: 7 × 4\n  MSMT_DATE             WSE WELL_DEPTH           geometry\n  <dttm>              <dbl>      <dbl>        <POINT [°]>\n1 2004-03-01 00:00:00  -8.6        780 (-121.262 38.4931)\n2 2003-10-01 00:00:00  -3.3        780 (-121.262 38.4931)\n3 2003-03-15 00:00:00  -1.1        780 (-121.262 38.4931)\n4 2002-10-01 00:00:00 -10.5        780 (-121.262 38.4931)\n5 2001-10-01 00:00:00 -13          780 (-121.262 38.4931)\n6 2001-03-15 00:00:00   1.9        780 (-121.262 38.4931)\n7 2000-03-15 00:00:00  -0.2        780 (-121.262 38.4931)\n\nWith nested data, we we can mutate new columns on the dataframe based onto the nested data column by mapping over each dataframe. For instance, we can add a linear model:\n\n\nd_recent %>% \n  group_by(SITE_CODE) %>% \n  # nest the data per SITE_CODE grouping variable\n  nest() %>% \n  mutate(\n    # map a linear model onto the data\n    model = map(data, ~lm(WSE ~ MSMT_DATE, data = .x))\n  )\n\n\n# A tibble: 302 × 3\n# Groups:   SITE_CODE [302]\n   SITE_CODE          data           model \n   <chr>              <list>         <list>\n 1 384931N1212618W001 <sf [7 × 4]>   <lm>  \n 2 384742N1213146W001 <sf [7 × 4]>   <lm>  \n 3 384798N1212614W001 <sf [38 × 4]>  <lm>  \n 4 384532N1212856W001 <sf [18 × 4]>  <lm>  \n 5 384425N1213031W001 <sf [38 × 4]>  <lm>  \n 6 384260N1212853W001 <sf [19 × 4]>  <lm>  \n 7 384251N1213346W001 <sf [7 × 4]>   <lm>  \n 8 384526N1211695W001 <sf [112 × 4]> <lm>  \n 9 384511N1212360W001 <sf [19 × 4]>  <lm>  \n10 384252N1211997W001 <sf [9 × 4]>   <lm>  \n# … with 292 more rows\n\nNow we have a linear model output object per SITE_CODE, but let’s not stop there. Let’s also mutate new columns with the intercept (\\(\\beta_1\\)), the direction and magnitude of the intercept, convert units to \\(ft/yr\\), and unnest() the data.\n\n\nd_recent <- d_recent %>% \n  group_by(SITE_CODE) %>% \n  # nest the data per SITE_CODE grouping variable\n  nest() %>% \n  mutate(\n    # map a linear model across data, extract slope magnitude & direction\n    model     = map(data, ~lm(WSE ~ MSMT_DATE, data = .x)),\n    b1        = map(model, ~coefficients(.x)[[\"MSMT_DATE\"]]),\n    b1        = map(b1, ~sec_to_yr(.x)),\n    direction = map(b1, ~ifelse(.x >= 0, \"increase\", \"decline\")),\n    # duration of the data in units of years so we know how the period\n    # over which the linear model is estimated\n    length_yr = map(data, ~as.numeric(diff(range(.x$MSMT_DATE))) / 365)\n  ) %>% \n  # unnest all columns so we can access values\n  unnest(c(data, b1:length_yr)) %>% \n  select(-model)\n\n\n\n\n\n\nTake a moment to View(d_recent) and inspect the new columns.\nNow we can visualize trends for each station like before. Note that not all records are a full 20 years long!\n\n\nd_recent %>% \n  filter(SITE_CODE %in% unique(d$SITE_CODE)[1:7]) %>% \n  ggplot() +\n  geom_line(aes(MSMT_DATE, WSE)) +\n  geom_smooth(aes(MSMT_DATE, WSE, color = direction), method = \"lm\", se = FALSE) +\n  facet_wrap(~SITE_CODE) +\n  labs(color = \"Trend\")\n\n\n\n\nDon’t forget that these trends have a spatial component. Let’s now only visualize sites with a duration_yr of 5 years or more.\n\n\n# slice the first observation per SITE_ID (for location) and \n# re-convert to sf which is lost during nest and unnest\nd_map <- d_recent %>% \n  group_by(SITE_CODE) %>% \n  slice(1) %>% \n  ungroup() %>% \n  st_as_sf() %>% \n  filter(length_yr >= 5)\n\n# create a bin for trend magnitude that matches DWR bins (Fig 2)\nd_map$bin <- cut(\n  d_map$b1, \n  breaks = c(-1000, -2.5, 0, 2.5, 1000), \n  labels = c(\"Increase > 2.5 ft/yr\", \"Increase 0 - 2.5 ft/yr\", \n             \"Decrease 0 - 2.5f ft/yr\", \"Decrease > 2.5 ft/yr\")\n)\n  \n# sacramento county polygon\nsac <- st_read(here(\"data\", \"shp\", \"sac\", \"sac_county.shp\")) %>% \n  st_transform(st_crs(d_map))\n\n\nReading layer `sac_county' from data source \n  `/Users/richpauloo/Documents/GitHub/r4wrds/intermediate/data/shp/sac/sac_county.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -13565710 ymin: 4582007 xmax: -13472670 ymax: 4683976\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# plot direction\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = d_map, aes(fill = direction), pch = 21, size = 2) +\n  theme_void() +\n  labs(fill = \"Groundwater trend\")\n\n\n\n# plot magnitude bins\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = d_map, aes(color = bin), size = 2, alpha = 0.8) +\n  rcartocolor::scale_color_carto_d(\"Groundwater trend\", palette = \"TealRose\") +\n  theme_void()\n\n\n\n\nThe map above suggest there are only 3 locations in the basin that show an absolute change of more than 2.5 \\(ft/yr\\). Most of the longer term monitoring locations have an annual absolute rate of change within 0-2.5 \\(ft/yr\\). A quick comparison with Figure 2 suggests that our calculated groundwater level trends are similar in magnitude.\nHow to write helpful functions\nEverything we’ve done until this point is valuable, but is it reproducible? In the sense that we can re-run these scripts on new data, yes, but we can apply project management best practices and refactor our code into functions and programs (scripts) that are easy to re-run (Figure 1, left-most situation). We can go one step further and wrap everything into a control script (Figure 1, center situation), and in the next section, we’ll discuss how to fully automate (Figure 1, right-most situation).\nIn the spirit of “don’t repeat yourself” (DRY), it’s often said:\n\nIf you copy paste a workflow more than 3 times, write a function.\n\nRecall that in this case study, we want to reevaluate groundwater level trends twice per year, once in spring and once in fall. We decide to save time in the future by abstracting these tasks into a set of functions and a central control script. We also know that others will need to use this same workflow, so we strive to turn our analysis into a reproducible tool for others and include helpful messaging, errors, and conditional execution (i.e., if statements) in the code.\n1. Download\nLet’s begin with the download step. Save this as scripts/functions/f_wcr_download.R. Notice that we add a dir_out argument that allows a user to specify a directory to save files to. If this path doesn’t exist, we create it. Also, we allow users to specify the urls, so if case they change in the future, this function need not change.\n\n\n# download wcr data\nf_wcr_download <- function(dir_out, urls){\n\n  # create directory and define paths to save downloaded files\n  if(! dir.exists(dir_out)){ \n    dir.create(dir_out) \n  }\n  \n  # output file paths\n  files_out <- file.path(dir_out, basename(urls))\n  \n  # download files\n  walk2(urls, files_out, ~download.file(.x, .y))\n  \n}\n\n\n\n2. Read\nNext, the import function should be saved as scripts/functions/f_wcr_import.R. Let’s design it so that the user provides a county and a vector of input files. The function returns a helpful message in case they don’t provide a valid county. Along the way, we also provide helpful messages with cat() statements that print messages to the terminal as the function moves along. Messages like this are can be considered rudimentary form of logging which provides a user information on the status of a function, and may help diagnose and fix errors that may arise by showing the last place where a successful cat() message printed.\n\n🐈 cat() stands for “concatenate and print”. It takes any number of objects, concatenates them with a space in-between, and prints them to the console. In the function below, we add \"/n\" (newlines) to each cat() call so that the next cat() message begins on a new line in the console.\n\n\n# import downloaded wcr data\nf_wcr_import <- function(county, files_in){\n  # valid counties from entire dataset, i.e.,\n  # dput(sort(unique(d$COUNTY_NAME)))\n  counties_valid <-\n    c(\"Alameda\", \"Alpine\", \"Amador\", \"Butte\", \"Calaveras\", \"Colusa\",\n      \"Contra Costa\", \"Del Norte\", \"El Dorado\", \"Fresno\", \"Glenn\",\n      \"Humboldt\", \"Imperial\", \"Inyo\", \"Kern\", \"Kings\", \"Klamath, OR\",\n      \"Lake\", \"Lassen\", \"Los Angeles\", \"Madera\", \"Marin\", \"Mariposa\",\n      \"Mendocino\", \"Merced\", \"Modoc\", \"Mono\", \"Monterey\", \"Napa\", \"Nevada\",\n      \"Orange\", \"Placer\", \"Plumas\", \"Riverside\", \"Sacramento\", \"San Benito\",\n      \"San Bernardino\", \"San Diego\", \"San Francisco\", \"San Joaquin\",\n      \"San Luis Obispo\", \"San Mateo\", \"Santa Barbara\", \"Santa Clara\",\n      \"Santa Cruz\", \"Shasta\", \"Sierra\", \"Siskiyou\", \"Solano\", \"Sonoma\",\n      \"Stanislaus\", \"Sutter\", \"Tehama\", \"Tulare\", \"Tuolumne\", \"Ventura\",\n      \"Yolo\", \"Yuba\", \"ALL\")\n\n  # ensure input county is valid before reading in data\n  if(! county %in% counties_valid) {\n    stop(\n      glue(\"County must be one of: {paste(counties_valid, collapse = ', ')}\"),\n      call. = FALSE\n    )\n  }\n\n  cat(\"Valid county provided, now proceeding to read data...\", \"\\n\")\n\n  # read and combine - use datatable for speed and memory management\n  d <- map(files_in, ~data.table::fread(.x)) %>%\n    reduce(left_join, by = \"SITE_CODE\")\n\n  cat(\"Files read and combined...\", \"\\n\")\n\n  # if a county is supplied (not requesting ALL the data), filter to it\n  if(county != \"ALL\"){\n    cat(\"Filtering to\", county, \"county...\", \"\\n\")\n    d <- filter(d, COUNTY_NAME == county)\n  }\n\n  # select only relevant columns & convert to sf object\n  cat(\"Selecting relevant columns...\", \"\\n\")\n  d <- d %>%\n    select(SITE_CODE, MSMT_DATE, WSE, LONGITUDE, \n           LATITUDE, WELL_DEPTH, COUNTY_NAME) %>%\n    filter(!is.na(LONGITUDE) & !is.na(LATITUDE)) %>%\n    st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4269)\n\n  cat(\"Data import complete.\", \"\\n\")\n  return(d)\n}\n\n\n\nNotice the use of conditional execution (if() statements that check if certain criteria apply before proceeding), coupled with stop() functions that terminate the function call. It doesn’t make sense to read in an entire dataframe and filter by a county name character sting that’s not present in the data, so we check for that condition before proceeding and provide a helpful error message in case the user enters an incorrect county. We added a valid county called “ALL” which if passed to the function, doesn’t filter the data to a county, and instead returns the entire dataset. All throughout, we added helpful cat() messages so that as this function is chugging along, messages are printed to the console to let us know what’s happening under the hood. This also helps with debugging and logging (as we will discuss later).\nTo verify our function works as expected, let’s trigger an error message by asking it to return groundwater levels from a county that doesn’t exist in the vector of valid counties.\n\n\nf_wcr_import(county = \"Tatooine\")\n\n\n\nError: County must be one of: Alameda, Alpine, Amador, Butte, Calaveras, Colusa,\nContra Costa, Del Norte, El Dorado, Fresno, Glenn, Humboldt, Imperial, Inyo, \nKern, Kings, Klamath, OR, Lake, Lassen, Los Angeles, Madera, Marin, Mariposa, \nMendocino, Merced, Modoc, Mono, Monterey, Napa, Nevada, Orange, Placer, Plumas, \nRiverside, Sacramento, San Benito, San Bernardino, San Diego, San Francisco, \nSan Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz,\nShasta, Sierra, Siskiyou, Solano, Sonoma, Stanislaus, Sutter, Tehama, Tulare, \nTuolumne, Ventura, Yolo, Yuba, ALL\n3. Clean\nNext, we need to clean our data. You can imagine a much more extensive cleaning process for another dataset, but in our simple example, all we want to do is provide the user with the option to set a start and end time to filter the data by. If a start or end date is not provided, we have the function calculate the min and max date and use those.\nSave the following code as scripts/functions/f_wcr_clean.R:\n\n\n# clean wcr data\nf_wcr_clean <- function(df, start_date = NULL, end_date = NULL){\n\n  cat(\"Cleaning data\", \"\\n\")\n\n  # if start and end date are NULL, use the min and max date\n  if(is.null(start_date)){\n    start_date <- min(df$MSMT_DATE, na.rm = TRUE)\n    cat(\"Start date NULL, using min date:\", as.character(start_date), \"\\n\")\n  }\n  if(is.null(end_date)){\n    end_date <- max(df$MSMT_DATE, na.rm = TRUE)\n    cat(\"End date NULL, using max date:\", as.character(end_date), \"\\n\")\n  }\n\n  # filter to date range and clean\n  df <- filter(df, MSMT_DATE >= start_date & MSMT_DATE <= end_date)\n\n  cat(\n    \"Data filtered between\",\n    as.character(start_date), \"and\",\n    as.character(end_date),   \"\\n \"\n  )\n\n  return(df)\n}\n\n\n\nNotice that about half of this function is focused not on the transformation, but rather, setting the min and max dates if they’re not provided.\nchecking that the correct object classes are passed into the function, and sensibly imputing argument values if they’re not provided by the user (i.e., if not provided as arguments, start and end dates are assigned as the min and max dates).\nThe actual filter() statement only takes 1 line of code. The amount of time you should spend fine-tuning functions like so depends on how much you anticipate others using them, and how much you want to constrain their use and messaging. Not every project requires this level of detail.\n\nExtra practice: using stop()\n\nA useful function when writing functions is stop(), which stops a function from running, and prints an error message. When paired with if(), you can build functions that test for conditions where you want the function to error.\nFor example, in the function above f_wcr_clean(), you may notice that the function depends on a valid date object passed in as the second and third arguments, start_date and end_date.\nHow would you use if() and stop() to verify if the supplied start_date and end_date were indeed valid date objects?\n\n\nClick for the Answer!\n\nThe answer is to check the class of the supplied arguments. If they are not within an expected class of c(\"Date\", \"POSIXct\", \"POSIXt\"), then we can throw an error. Look up the stop() documentation (?stop) to see what the .call argument does.\n\n\n# clean wcr data\nf_wcr_clean <- function(df, start_date = NULL, end_date = NULL){\n\n  # valid date classes\n  valid_class <- c(\"Date\", \"POSIXct\", \"POSIXt\")\n\n  # verify correct date class\n  if(! is.null(start_date) & ! class(start_date)[1] %in% valid_class ){\n    stop(\n      glue(\n        \"Invalid `start_date` class, use: {paste(valid_class, collapse=', ')}\",\n      ),\n      call. = FALSE\n    )\n  }\n  if(! is.null(end_date) & ! class(end_date)[1] %in% valid_class ){\n    stop(\n      glue(\n        \"Invalid `end_date` class, use: {paste(valid_class, collapse=', ')}\",\n      ),\n      call. = FALSE\n    )\n  }\n\n  cat(\"Cleaning data\", \"\\n\")\n\n  # if start and end date are NULL, use the min and max date\n  if(is.null(start_date)){\n    start_date <- min(df$MSMT_DATE, na.rm = TRUE)\n    cat(\"Start date NULL, using min date:\", as.character(start_date), \"\\n\")\n  }\n  if(is.null(end_date)){\n    end_date <- max(df$MSMT_DATE, na.rm = TRUE)\n    cat(\"End date NULL, using max date:\", as.character(end_date), \"\\n\")\n  }\n\n  # filter to date range and clean\n  df <- filter(df, MSMT_DATE >= start_date & MSMT_DATE <= end_date)\n\n  cat(\n    \"Data filtered between\",\n    as.character(start_date), \"and\",\n    as.character(end_date),   \"\\n \"\n  )\n\n  return(df)\n}\n\n\n\n\n\n\n4. Model\nMoving on, after cleaning, it’s time to model. Save another function scripts/functions/f_wcr_model.R that calculates the linear model at each SITE_CODE:\n\n\n# build linear model for each unique SITE ID and\n# add important columns (direction), beta_1 coefficient\nsec_to_yr <- function(sec){\n  yr <- sec * 31557600\n  return(yr)\n}\n\nf_wcr_model <- function(df){\n  result <- df %>%\n    group_by(SITE_CODE) %>%\n    # nest the data per SITE_CODE grouping variable\n    nest() %>%\n    mutate(\n      # map a linear model across data, extract slope magnitude & direction\n      model     = map(data, ~lm(WSE ~ MSMT_DATE, data = .x)),\n      b1        = map(model, ~coefficients(.x)[[\"MSMT_DATE\"]]),\n      b1        = map(b1, ~sec_to_yr(.x)),\n      direction = map(b1, ~ifelse(.x >= 0, \"increase\", \"decline\")),\n      # duration of the data in units of years so we know how the period\n      # over which the linear model is estimated\n      length_yr = map(data, ~as.numeric(diff(range(.x$MSMT_DATE))) / 365)\n    ) %>%\n    # unnest all columns so we can access values\n    unnest(c(data, b1:length_yr)) %>%\n    select(-model)\n\n  return(result)\n}\n\n\n\n5. Visualize\nNext, we need to visualize and save these data. Add another function scripts/functions/f_wcr_visualize_map.R which creates the reproduced DWR map above.\n\n\n# visualize results \nf_wcr_visualize_map <- function(df, yr_min){\n\n  # slice the first observation per SITE_ID (for location) and\n  # re-convert to sf which is lost during nest and unnest\n  d_map <- df %>%\n    group_by(SITE_CODE) %>%\n    slice(1) %>%\n    ungroup() %>%\n    st_as_sf() %>%\n    filter(length_yr >= yr_min) %>%\n    # create a bin for trend magnitude that matches DWR bins (Fig 2)\n    mutate(\n      bin = cut(\n        b1,\n        breaks = c(-1000, -2.5, 0, 2.5, 1000),\n        labels = c(\"Increase > 2.5 ft/yr\", \"Increase 0 - 2.5 ft/yr\",\n                   \"Decrease 0 - 2.5f ft/yr\", \"Decrease > 2.5 ft/yr\")\n      )\n    )\n\n  # sacramento county polygon\n  sac <- st_read(here(\"data\", \"shp\", \"sac\", \"sac_county.shp\")) %>%\n    st_transform(st_crs(d_map))\n\n  # plot magnitude bins\n  p <- ggplot() +\n    geom_sf(data = sac) +\n    geom_sf(data = d_map, aes(fill = bin), pch = 21, size = 3, alpha = 0.9) +\n    rcartocolor::scale_fill_carto_d(\"\", palette = \"TealRose\") +\n    theme_void()\n\n  return(p)\n}\n\n\n\nAdd another function scripts/functions/f_wcr_visualize_timeseries.R which writes a plot of the groundwater level over time, and shows its location on a map.\n\n\n# visualize timeseries of groundwater level with a map\nf_wcr_visualize_timeseries <- function(df, dir_out){\n  dir.create(dir_out)\n\n  # color of trend - up or down\n  col_trend <- ifelse(df$direction[1] == \"decline\", \"red\", \"blue\")\n\n  # create the hydrograph\n  p_hydrograph <- df %>%\n    ggplot(aes(MSMT_DATE, WSE)) +\n    geom_line() +\n    geom_smooth(\n      color = col_trend, method = \"lm\", se = FALSE,\n      lwd = 2, linetype = \"dashed\"\n      ) +\n    guides(color = FALSE) +\n    labs(title = glue::glue(\"{df$SITE_CODE[1]}, {round(df$b1[1], 2)} ft/yr\")) +\n    theme_minimal()\n\n  # create the map\n  # make measurements data spatial if not already\n  df_sf <- st_as_sf(df)\n\n  # sacramento county polygon\n  sac <- st_read(here(\"data\", \"shp\", \"sac\", \"sac_county.shp\")) %>%\n    st_transform(st_crs(df_sf))\n\n  # map\n  p_map <- ggplot() +\n    geom_sf(data = sac) +\n    theme_void() +\n    geom_sf(data = df_sf[1, ], color = col_trend, size = 4, alpha = 0.9)\n\n  # combine plots: requires {pathwork}\n  p_combined <- p_map + p_hydrograph\n\n  # save\n  ggsave(\n    file.path(dir_out, glue::glue(\"{df$SITE_CODE[1]}.png\")),\n    p_combined,\n    height = 4, width = 12\n  )\n\n}\n\n\n\n6. Write\nAnd finally, we want to save these data. We can write another function for that, but we can also achieve that pretty simply within the control script using functional programming, so we’ll do that.\nNow that we have functions defined, let’s define one control module to download, import and clean, model, visualize, and write. Save this as a new file scripts/00_control.R.\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\nlibrary(pathwork)\n\n# load functions\nfs:dir_ls(here(\"functions\"), regexp = \"f_wcr\") %>%\n  walk(~source(.x))\n\n# urls for data\nbase_url <-\n  \"https://data.cnra.ca.gov/dataset/dd9b15f5-6d08-4d8c-bace-37dc761a9c08/resource/\"\nurls <- paste0(\n  base_url,\n  c(\"af157380-fb42-4abf-b72a-6f9f98868077/download/stations.csv\",\n    \"bfa9f262-24a1-45bd-8dc8-138bc8107266/download/measurements.csv\",\n    \"f1deaa6d-2cb5-4052-a73f-08a69f26b750/download/perforations.csv\")\n)\n\n# data path to download files and read them\ndata_path <- here(\"data\", \"pgwl\")\n\n# ------------------------------------------------------------------------\n# download data\nf_wcr_download(data_path, urls = urls)\n\n# ------------------------------------------------------------------------\n# import, clean, and model\nd <-\n  # import downloaded data\n  f_wcr_import(\n    county   = \"Sacramento\",\n    files_in = fs::dir_ls(data_path)\n  ) %>%\n  # minor cleaning of one unreasonable value\n  filter(WSE >-200) %>%\n  # clean data\n  f_wcr_clean(start_date = lubridate::ymd(\"2000-01-01\")) %>%\n  # fit a linear model for each SITE_CODE group, extract beta_1\n  f_wcr_model()\n\n# ------------------------------------------------------------------------\n# visualize big picture map\n# create output directory if it doesn't already exist\npath_plot <- here(\"results\", \"plot\")\ndir.create(path_plot)\n\np <- f_wcr_visualize_map(d, yr_min = 5)\nggsave(file.path(path_plot, \"00_ALL.png\"), p, height = 6, width = 6)\n\n# write all timeseries plots. use map instead of walk because we want\n# to capture output to inspect\nsafely_viz <- safely(f_wcr_visualize_timeseries)\nerrs <- group_split(d, SITE_CODE) %>% map(~safely_viz(.x, path_plot))\n\n# ------------------------------------------------------------------------\n# write\n# create output directory if it doesn't already exist\npath_csv <- here(\"results\", \"csv\")\ndir.create(path_csv)\n\n# write all results as one big csv and use default ordering to sort\nwrite_csv(d, file.path(path_csv, \"00_ALL.csv\"))\n\n# silently write a separate csv for each group\nd %>%\n  split(.$SITE_CODE) %>%\n  walk(~write_csv(.x, file.path(path_csv, glue::glue(\"{.x$SITE_CODE[1]}.csv\"))))\n\n\n\nNotice the use of purrr::safely() when calling the f_wcr_visualize_timeseries() function. Sometimes, when mapping over many inputs (for instance 420 SITE_CODEs), we may encounter an error in one of the elements that stops the function from completing. safely() always succeeds, and we can go back and inspect errors because the function returns a list of results and errors (if they occur).\n\nAnother related error-handling verb is possibly() which works like safely, but allows us to specify a default value to insert for each error.\nIn this pipeline, we actually see some errors when we have a SITE_CODE with only one measurement. We can’t create a geom_smooth() ggplot layer with only one point, so it throws an error. But assuming we didn’t know that, we could figure it out by inspecting the output of the safely() function:\n\n\nerrs %>% \n  transpose() %>% \n  simplify_all() %>% \n  unlist()\n\n\n\n$error.message\n[1] \"'gpar' element 'fontsize' must not be length 0\"\n\n$error.call\ncheck.length(gparname)\nWe can browse errs to find the indices of these locations, or find them with code:\n\n\ni <- which(map_lgl(simplify_all(errs), ~!is.null(.x)) == TRUE)\ni\n\n\n\n[1]  54 119\nIf we inspect these indices, we can verify that they only have one measurement each. By using safely(), we were able to not let these 2 elements stall the entire pipeline of 420 elements.\n\n\ngroup_split(d, SITE_CODE)[i]\n\n\n\n[[1]]\n# A tibble: 1 x 8\n  SITE_CODE          MSMT_DATE             WSE WELL_DEPTH\n  <chr>              <dttm>              <dbl>      <int>\n1 383434N1214367W001 2000-04-24 00:00:00 -41.4        160\n# … with 4 more variables: geometry <POINT [°]>, b1 <dbl>,\n#   direction <chr>, length_yr <dbl>\n\n[[2]]\n# A tibble: 1 x 8\n  SITE_CODE          MSMT_DATE             WSE WELL_DEPTH\n  <chr>              <dttm>              <dbl>      <int>\n1 385312N1215006W001 2001-05-29 00:00:00  1.13         75\n# … with 4 more variables: geometry <POINT [°]>, b1 <dbl>,\n#   direction <chr>, length_yr <dbl>\nTo conclude, we abstracted our tasks to a set of functions, each contained in multiple, short, modular scripts rather than one massive and unwieldy script. This helps with maintenance, portability, and reproducibility. We used functional programming to replace for loops, and demonstrated how use safely() for error handling, as errors are a part of life when working with large lists, making requests from APIs or fileservers, and scaling processes to a large number of inputs.\n\n\nPause and think\nImagine you wanted to scale this analysis beyond Sacramento county to Placer County. How would you approach this?\nHow could you scale to all counties in California?\nWhat would you need to scale to all counties in the United States?\n\nClick for Answers!\n\nAs with all things in coding, there are many ways to accomplish this, but some ways are more efficient than others.\nNotice that f_wcr_import() has an argument county. To scale the analysis to Placer county, we would simply change the county argument to \"Placer\". However, you may notice that f_wcr_visualize_map() and f_wcr_visualize_timeseries() both depend on a Sacramento county polygon, so you can swap that out with a Placer county polygon and be off to the races.\nHowever, if you wanted to scale this to the state, you’d need to assemble polygons for all counties in the state. That sounds time consuming. However, as with most things, it’s good practice to google what feels like a common problem, because chances are you’re not the first person with this problem! In fact, there’s a great way to grab county data from directly within R using the {maps} package.\n\n\nlibrary(maps)\ncounties <- st_as_sf(map(\"county\", plot = FALSE, fill = TRUE))\ncounties <- subset(counties, grepl(\"california\", counties$ID))\nplot(counties)\n\n\n\n\nWith all California counties in hand, we can re-write the function to filter this polygon to the county of interest.\nGoing one step further, if we had a dataset of groundwater levels across the entire US, {maps} has county polygons for each county, so it wouldn’t be difficult to extend our code to a nationwide analysis.\n\n\nTask scheduling\nLet’s imagine that these groundwater levels were not the DWR periodic groundwater level database, but rather a set of continuous monitoring sensors on telemetry that updated daily, and you needed to kick this pipeline off every day at 10am. Surely you wouldn’t manually run 00_control.R every morning. You’d use a task scheduler.\nTask schedulers vary by system. In general, they run a program at defined intervals. We won’t set one up in this course, but it’s good to know about them:\nHow to set up a task scheduler on Windows\nHow to set up cron, an open source task scheduler for Linux\n\nAlso check out the {taskscheduleR} and {cronR} packages for Windows and Linux, which have RStudio add-ins.\nLogging\nWhenever code is running without your supervision on a task scheduler, things may break so it’s a good idea to print information whenever the job kicks off so you can review this information in case something doesn’t go as planned. We’ve been decorating our code with cat() statements that print updates to the console, and this is helpful, but we can be much more sophisticated and start logging. Logging is cornerstone in software engineering, but less so in data science. Logging is important for larger jobs that run often (e.g., anything on a task scheduler) with potential to break, or that may be subject to inputs that sufficiently change and that may breaks code.\nA full discussion of logging is out of scope for this lesson, but you can learn more about logging by browsing these packages:\n{lgr}\n{logger}\n\nWhen to not automate\nAs a data scientist, you’ll often perform once-over ad-hoc analyses. You may or may not need to return to these analyses, or components of the analyses. Automation takes time, and just because we have the tools to automate doesn’t mean that you always should. Whether or not you should automate depends on how often you anticipate the workflow needs to be re-evaluated. The more the workflow needs to be run, the more time we can justify spending on firming up the reproducibility and automation potential of program. When it’s not immediately apparent that automation is necessary (e.g., “a daily report of streamflow automatically scraped from 1,000 sensors that we set up on a task scheduler”) a general rule of thumb for whether or not to automate may boil down to the following consideration:\n\nWill the time saved in the future by automation exceed the time spent automating the process now? If so, automate.\n\n\n\n\n\nFigure 3: Figure credit: xkcd, 1205\n\n\n\nDon’t underestimate the time it can take to automate.\n\n\n\n\nFigure 4: Figure credit: xkcd, 1319\n\n\n\n\nCode as instructions\nWe’ve all done a point-and-click analysis before that becomes unwieldy, large, and “too big to fail.” In other words, we cross our fingers and hope we’ll never have to repeat the analysis because either:\nit took so long to complete that it would be unimaginable to spend that same amount of time repeating it\ncertain parts of the workflow were subjective, so repeating it would yield a different result\nrepeating one part of the workflow would invalidate downstream components and derivative products of the workflow, creating even more work\nWriting code is all about ensuring reproducibility and automation so we never find ourselves in this position. Writing efficient code ensures that the code is easy to interact with, debug, and modify.\n\ncode = instructions\n\nNo matter the language the code is in, whether it’s R, Python, Julia or something else, at its most fundamental level, code is a set of instructions to execute. In the data science world, coding allows automation of routine point-and-click tasks, and a good program (e.g., .R script) will replace the need to click through an analysis. Moreover, as data volume (size) and frequency (timing) increase, many point-and-click software tools are stretched to their limits and become slow or non-responsive. Code usually runs without the overhead of graphics processors, can be used again and again, and can be automated to run on its own.\nAs you grow in your capability with R, you will naturally find yourself able to interact with other languages and systems because at the end of the day, you’re in the business of writing instructions for computers to carry out, and these instructions usually involve similar tasks.\n\nAdditional resources\nConvert your R code into command line tools: RStudio is a great interactive development environment (IDE) for writing R code, but it’s important to remember that R is essentially a program on your computer that you can access and interact with via the command line using the utility Rscript. If you need to scale your work to a cluster, or your work expands to involve other programmers who may be less familiar with R, you may need to package your R code as a command line tool, which is a familiar format for data engineers, data scientists, and developers coming from other backgrounds. A command line tool is essentially a program that can be called from the command line, and passed arguments like so:\n\nRscript my_script.R arg1 arg2 arg3\n\nLearn more about turning your R code into a command line tool in this Software Carpentry lesson.\n\n\nPrevious module:Iteration Next module:Parameterized reports\n\nAs discussed in the project management module, abstracting functions from code is a best practice of defining functions and storing them in separate scripts to keep your workspace easier to read and de-bug.↩︎\nNote that over time, the URLS in this code may change, and the data will certainly grow in volume. Thus, when comparing the output of the code to the data provided with this course, there may be differences because this code always grabs the most up-to-date data, which will probably contain more records than what we provide.↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_simple_shiny.html",
      "title": "5. Simple Shiny",
      "description": "How to create interactive dashboards with R\n",
      "author": [],
      "contents": "\n\nContents\nWhat is a Shiny App\nand Why is it useful?\nCase Study:\nSacramento county groundwater elevation\n\nBasic Shiny App Structure\nCreating a Shiny App\nui\n(frontend)\nControl\nwidgets\nOutputs\n\nserver (backend)\nReactivity\nRender\nobjects\n\nPutting it all together\nShare and deploy a Shiny\nApp\nExtending\nShiny\nAdditional Resources\n\n\n\nLearning objectives\nUnderstand what a Shiny App is and why you might want or need to\nbuild one\nUnderstand the basic structure of a Shiny App and common\nhiccups\nDiscuss approaches to extend your Shiny skills\n\n\nWhat is a Shiny App\nand Why is it useful?\nAccording to the Mastering Shiny\nbook:\n\nShiny is a framework for creating web applications using\nR code. It is designed primarily with data scientists in\nmind, and to that end, you can create pretty complicated Shiny apps with\nno knowledge of HTML, CSS, or JavaScript.\n\nShiny Apps are made using the {shiny}\npackage.\nBecause they extend R-based analyses, interactive Shiny\nApps have as many diverse niches and uses as the the R\ncommunity itself, and it’s likely that you have come across a Shiny App\nin the wild before.\nShiny Apps are highly customizable, and allow R users to\nwrap existing code and data with an additional layer of interactivity\nusing R code from the {shiny} package to\nbetter visualize, analyze, export, and more. This allows any user (even\nnon-R users!) to interact with the data, providing a\npowerful way of exploring and understanding the data within the Shiny\nApp.\nNow that we know what a Shiny App is, a fundamental question that we\nneed to answer is “when and why should I build a Shiny App?” Generally,\nyou should create a Shiny App when you want to make results and/or data\navailable to others, or when the dataset is complex enough to warrant a\nShiny App to ease exploration. To provide a specific example, let’s\nexamine a case study.\nCase Study:\nSacramento county groundwater elevation\nImagine you’re analyzing groundwater levels across California1 and want to assess groundwater\nelevation trends over time in Sacramento County. You may begin with an\nEDA to filter the data to Sacramento County, clean the data, and make\nsome exploratory plots.\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf)\n\n# read groundwater level data pre-filtered to Sacramento county\ngwl <- read_csv(\"data/gwl/gwl_sac.csv\") %>% \n  st_as_sf(coords = c(\"LONGITUDE\",\"LATITUDE\"), crs = 4269)\n\n# read sacramento county shapefile and reproject\nsac <- st_read(\"data/shp/sac/sac_county.shp\", quiet=TRUE) %>% \n  st_transform(4269)\n\n# plot the groundwater levels at each monitoring site \ngwl %>% \n  ggplot() +\n  geom_line(aes(MSMT_DATE, WSE, group = SITE_CODE), alpha = 0.5)\n\n\n\n# slice the first station per group of groundwater level observations\n# (see comment aside)\ngwl_min <- gwl %>% \n  group_by(SITE_CODE) %>% \n  slice(1) %>% \n  ungroup() \n\n# visualize sites on a map\nggplot() +\n  geom_sf(data = sac) +\n  geom_sf(data = gwl_min, alpha = 0.5, color = \"blue\") +\n  theme_void()\n\n\n\n\nNotice that nrow(gwl) is greater than\nnrow(gwl_min), but\nlength(unique(gwl$SITE_CODE)) equals\nlength(unique(gwl_min$SITE_CODE)). We don’t need to plot\nall of the redundant gwl locations on the map, so we\ncreated a minimal tibble with only the unique\nSITE_CODEs, and hence, geometry.\nDuring the analysis, you realize you want to easily look at data by\nmonitoring site, so you make a function that streamlines this.\n\n\n# function that takes a site code and generates a plot\nf_make_hydrograph <- function(data, site_code){\n  p <- data %>% \n    filter(SITE_CODE == site_code) %>% \n    ggplot(aes(MSMT_DATE, WSE)) +\n    geom_line(alpha = 0.5) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(title = glue::glue(\"Groundwater level (ft AMSL) at: {site_code}\"))\n  return(p)\n}\n\n# make a hydrograph for the first SITE_CODE\nf_make_hydrograph(gwl, gwl$SITE_CODE[1])\n\n\n\n\nYour supervisor requests figures from certain stations, and you find\nyourself re-running this function to generate them. You also have\nteammates that require subsets of these data per monitoring site, with\nspecific columns in the output. You add a function that writes data to\ncsv files to handle this task, but realize that this project is ongoing,\nand people will keep coming to you with requests for alternate\ncombinations of plots or data. Also, the groundwater level data is\nsufficiently large and complex that you want to be able to view them all\nin one place.\n\nOne solution to automate the “data visualization exploration” and\n“data sharing” processes is to create a Shiny App.\n\nIn the sections that follow, we will create a Shiny App that allows\nusers to easily select and visualize groundwater data at different\nmonitoring stations, and export data for a selected site.\n\nBasic Shiny App Structure\nAt the bare minimum, a Shiny App is an .R file or set of\n.R files with three components: a ui,\nserver and runApp(). The runApp()\nfunction takes the ui and the server objects\nand runs the Shiny web application.\nBy design, Shiny Apps separate front and back end\ncomponents of the web application. The ui stands for “user\ninterface” and defines front-facing components that the user sees and\ninteracts with, like plots, tables, sliders, buttons, and so on. The\nserver holds the back-end logic that accepts input from the\nuser, and uses these inputs to define what data transformations should\noccur and what is passed back to the frontend for the user to view and\ninteract with.\nIn order to demonstrate a simple Shiny App, we will use a single file\ncalled app.R defines a ui and\nserver. Afterward, we will also discuss approaches to\nmodularize an app into separate files - this may be necessary if an app\nbecomes sufficiently complex.\nCreating a Shiny App\nThe most simple way to create an app within RStudio is to click\nFile > New File > Shiny Web App….\n\n\n\nBy default, RStudio will create a new folder with a name you provide\nin your project directorY. Enter gwl_shiny as the\n“Application name” and click Create.\n\n\n\nYou should now have a folder called gwl_shiny in the\nproject directory, which you can verify in the File Viewer Pane. Inside\nthat folder should be a single file, app.R.\n\n\n\nA default Shiny App is contained in this file. We can run all of the\ncode to view the app on our local machine, or click the Run\nApp icon in the top right corner of the code editor.\n\n\n\nScroll through app.R and notice that there is a\nui object, a server object which is a function\nof input and output, and a\nrunApp() function which takes the ui and\nserver objects as input.\n\nui (frontend)\nThe ui defines what the user sees when they interact\nwith the Shiny App. Generally speaking, the ui shows:\ncontrol widgets (the interface) which allow the user to\nsend commands to the server\noutput from the server\nControl widgets\nConsider the following example App, which demonstrates a few of the\ncontrol widget2 inputs available for Shiny Apps.\n\n\nlibrary(shiny)\n\nui <- fluidPage(\n    # sidebar with generic example inputs\n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\n              \"i_slider\",     # control widget ID     \n              \"Slider Input\", # label above the widget\n              min = 1,\n              max = 50,\n              value = 30\n            ),\n            fileInput(\n              \"i_file\",       # control widget ID     \n              \"File Input\"    # label above the widget\n            ),\n            dateInput(\n              \"i_date\",       # control widget ID     \n              \"Date Input\"    # label above the widget\n            ),\n            textInput(\n              \"i_text\",       # control widget ID     \n              \"Text Input\"    # label above the widget\n            ),\n            selectInput(\n              \"i_select\",     # control widget ID     \n              \"Select Input\", # label above the widget\n              choices = c(\"A\",\"B\",\"C\")\n            ),\n            radioButtons( \n              \"i_radio\",      # control widget ID  \n              \"Radio Buttons\",# label above the widget\n              choices = c(\"A\",\"B\")\n            )\n        ),\n        mainPanel(\"Look at all those inputs!\")\n    )\n)\n\n# blank server logic - the App does nothing with \n# control widget inputs and creates no output!\nserver <- function(input, output) {}\n\n# run the Shiny app\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\nThese control widgets may be intuitive to you, as we’ve all\ninteracted with web-based tools and apps that use widgets like these.\nView a more comprehensive list of control widgets here.\nChances are that you won’t use all of the control widget inputs\navailable in any given Shiny App you build. The choice of inputs you use\nwill depend on what information you want to present to the user, and how\ninformation they select with those widgets should modify data in the\nserver.\nOutputs\nA ui with only control widgets (i.e., the App example\nabove) is like having a keyboard without a monitor. You can change\ninputs by typing on the keyboard, but can’t see anything happen without\na monitor! In this analogy, the monitor is a set of outputs.\nYour Shiny App users will spend a lot of time interacting with\noutputs, and most of our time spent designing and creating a\nShiny App will go into creating outputs. To view default Shiny outputs,\ntype shiny::output and see what comes up in tab complete.\nFor example, plotOutput() is used to render plots,\ntextOutput() is used to render text, and so on.\n\n\n\nThere are other types of output not included in the\n{shiny} package for objects created by other packages, for\nexample leaflet::leafletOutput() and\nDT::dataTableOutput() create leaflet and\nDT objects.\nEach output in the ui takes as a first argument the\n“output id” as a character string, for example,\nplotOutput(\"my_plot\") means a plotOutput()\nwith an “output id” of \"my_plot\".\n\n\nFix the App\n\nEvery output in the ui should have a corresponding\nobject rendered in the server. Thus, if we define a Shiny\nApp with plotOutput(\"my_plot\") in the\nui, we need to assign a rendered plot to the object\noutput$my_plot in the server. For\npractice, fix the following Shiny App. You’ll know you were successful\nif you see a plot when running the App.\n\n\n\nlibrary(shiny)\n\n# user interface\nui <- fluidPage( plotOutput(\"plot\") )\n\n# server logic that draws a plot\nserver <- function(input, output) {\n  output$my_plot <- renderPlot({ plot(1:10) })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\nClick for the Answer!\n\nThis very boring Shiny App illustrates a very critical concept. It\nhas no control widget inputs, but it does have a matching output id and\noutput object in the ui and server\nrespectively.\nThe answer, therefore, is frustratingly simple:\nplotOutput(\"plot\") should be\nplotOutput(\"my_plot\").\n\n\nlibrary(shiny)\n\n# user interface\nui <- fluidPage( plotOutput(\"my_plot\") )\n\n# server logic that draws a plot\nserver <- function(input, output) {\n  output$my_plot <- renderPlot({ plot(1:10) })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\nserver (backend)\nIf the ui control widgets are like the keyboard\nreceiving inputs, then the server is the computer brain of\nthe Shiny App that takes inputs, translates the information from the\ninputs (code that does something with the data input), and creates\noutputs, and finally sends these back to the ui where a\nuser can interact or see the result.\nReactivity\nReactivity3 is a driving concept in Shiny App\nprogramming. The fundamental idea is that what happens in a Shiny App\ndepends only on values that change, and parts of the App are\ninterconnected such that when inputs change, these inputs trigger a\nre-evaluation of code that regenerates outputs, until the inputs change\nagain, and so on.\nRender objects\nIn the ui we saw that each output ID created by the\nserver had a unique ID name and was wrapped in an “output”\nfunction. For example,\nplotOutput(\"my_plot\")\nhad an object in the server called\noutput$my_plot\nthat was assigned the result of a\nrenderPlot({ }) function, and inside that\nfunction was the code for our plot. Each “render” function\nhas a corresponding “output” function (e.g.,\nrenderText() and textOutput() go together). To\nview some of the default “render” functions, type\nshiny::render and view the options available in the tab\ncomplete popup.\n\n\n\n\nPutting it all together\nLet’s take what we’ve covered and put it all together to address the\nCase\nStudy introduced above. We will build a simple Shiny App with a\nplotOutput() and DT::dataTableOutput() that\nshows the hydrograph and data for a user-specified monitoring point.\nWe’ll also include data download buttons within the data table and\ndemonstrate how to make the plot interactive with {plotly}.\n\n\nlibrary(shiny)\nlibrary(shinythemes)\nlibrary(tidyverse)\n\n# load sac county groundwater data and sac county polygon\ngwl <- read_csv(\"data/gwl/gwl_sac_shiny.csv\")\n\n# ------------------------------------------------------------------------\n# user interface\nui <- fluidPage(\n\n    # change default theme to \"united\"\n    theme = shinytheme(\"united\"),\n  \n    # application title\n    titlePanel(\"Sacramento County Groundwater Level Data\"),\n\n    # sidebar with a dropdown input for site_code\n    sidebarLayout(\n        sidebarPanel(\n            selectInput(\"site_code\",\n                        \"Select a site code:\",\n                        choices = unique(gwl$SITE_CODE))\n        ),\n\n        # tabs with hydrograph and data table\n        mainPanel(\n            tabsetPanel(\n                tabPanel(\"Hydrograph\", plotly::plotlyOutput(\"gwl_plot\")),\n                tabPanel(\"Data\", DT::dataTableOutput(\"gwl_data\"))\n            )\n        )\n    )\n)\n\n# ------------------------------------------------------------------------\n# define server logic to plot a hydrograph and create data table\nserver <- function(input, output) {\n\n    # --------------------------------------------------\n    # create hydrograph\n    output$gwl_plot <- plotly::renderPlotly({\n\n        # draw the ggplot based on the \"site_code\" user input\n        p <- filter(gwl, SITE_CODE == input$site_code) %>%\n            ggplot(aes(MSMT_DATE, WSE)) +\n            geom_line(alpha = 0.5) +\n            geom_smooth(method = \"lm\", se = FALSE) +\n            labs(title = input$site_code,\n                 x = \"\", y = \"Groundwater level (ft AMSL)\")\n\n        # render the plotly object\n        plotly::ggplotly(p)\n    })\n\n    # --------------------------------------------------\n    # create data table\n    output$gwl_data <- DT::renderDataTable({\n\n        # draw the plot based on the \"site_code\" user input\n        DT::datatable(\n            filter(gwl, SITE_CODE == input$site_code),\n            extensions = 'Buttons',\n            options =\n                list(dom = 'Bfrtip',\n                     buttons = c('copy', 'csv', 'excel', 'pdf', 'print'))\n        )\n    })\n}\n\n# ------------------------------------------------------------------------\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\nThe sidebarLayout() and tabsetPanel()\nfunctions demonstrated in this module are just a few of the many layout\noptions to configure the placement of control widgets and outputs in\nyour Shiny Apps.\nShiny App design and customization options are vast. We demonstrated\nthe “united” theme here, which is one of many {shinythemes}.\nYou can also add custom CSS\nin-line, or in a styles.css script in the Shiny App\ndirectory.\nFinally, although we demonstrate a simple Shiny App, more functional\nand complex Apps may benefit from modularizing particular App\ncomponents. Learn more about\nbuilding Shiny modules here.\n\nLet’s break down what we just did.\nBefore anything else, we loaded the packages we needed4 and our groundwater level data\ngwl.\nNext, in the ui, we added a custom theme from the\n{shinythemes} package to change the default appearance of\nour App. We used a selectInput() control widget with a ID\n\"site_code\" and choices equal to the unique values of\nSITE_CODEs in the gwl dataframe. Then, in the\nmainPanel() we added a tabsetPanel() with two\ntabPanel() tabs: one with a\nplotly::plotlyOutput() plot, and another with a\nDT::dataTableOutput() output.\nFinally, in the server, we rendered plotly and\ndataTable objects and saved them to the output object under\nthe output IDs \"gwl_plot\" and \"gwl_data\".\nThese objects are displayed in the ui.\nAnd the resulting Shiny App is:\n\n\n\n\n\nShare and deploy a Shiny App\nUp until now, we’ve been using our own computer to run our Shiny App\nexample. How do we share a Shiny App with others?\nThe fastest and simplest way to share your Shiny app with another\nR user is to share the app.R file and data\ndependencies. They can open the file and run the app on their computer\nas we’ve been doing in this module. However, we may want to share our\napp with a wider, non-technical audience over the internet. When we move\na Shiny App from our computer onto a web server so that it can be shared\nover the internet, we deploy the app.\nThere are a range of services that allow us to deploy Shiny Apps that\nrange from simple to complex, and costly to inexpensive.\nA relatively painless way to deploy is via shinyapps.io, run by RStudio. A\nfree tier is available, and provides a limited number of “live hours”\nper month for users to interact with your App. Paid tiers afford more\n“live hours” per month, a faster web server, and features like password\nprotection.\n{shiny} is open source, and in that spirit, the open\nsource Shiny\nServer can be installed on any local or cloud server (e.g., AWS,\nGoogle Cloud, Microsoft Azure). If you don’t have a background in cloud\ncomputing, your System Administrator (Sys Admin) can help you move your\nApp onto a cloud or company server5. Although this option\nrequires more manual setup than shinyapps.io, it offers a non-commercial\nalternative, and perhaps more control depending on your needs.\n\nExtending Shiny\nCongratulations! You’re now familiar with the basics of\n{shiny}. This module is just the tip of the iceberg. There\nare so many ways to extend {shiny}, and the opportunities\nfor customization of Shiny Apps are vast. We recommend the following\nways to increase your knowledge in this domain:\nfind open source Shiny Apps that you admire, clone their source code\nto your machine, run the App, and change things so you can explore how\nthe App works. Borrow bits of code in new Apps you create\nfind a side project that’s a good candidate for a Shiny App and\nbuild one - it doesn’t need to be work-related; in fact, it might be\nmore fun to work on something tangential that you find interesting\nexplore the free online books and guided video tutorials in the\nlinks below\nAdditional Resources\nBelow are a few freely available books, presentations, and locations\nto find Apps online:\nMastering\nShiny: Free online book that serves as an authoritative guide on how\nto build, maintain, deploy, and customize R Shiny Apps. Material goes\nwell beyond what is presented in this module.\nGuided Shiny Video\ntutorials: Series of guided video tutorials with live-code demos and\nslides that explain the fundamentals of R Shiny Apps and how to use\nthem.\nRStudio Shiny\nGallery: Examples of Shiny Apps highlighting different ways Shiny\nhas been used, with links to source code\n\n\nPrevious\nmodule:4. Interactive Visualization\nNext\nmodule:6. Iteration\n\nFor instance, imagine you are using the California\nDepartment of Water Resources’ Periodic\nGroundwater Level Database used throughout this course.↩︎\nA control widget is a tool that allows users to send\ninformation from the frontend ui to the backend\nserver. Different widgets allow different types of\ninformation to pass between the ui and server.\nNotice that the first argument to each control widget is the unique ID\nor name of that widget. IDs must be unique, and are accessed in the\nserver in a named list called input. For example, the\nsliderInput(\"bins\") has the unique ID\n\"i_slider\" and the user-input value to this widget can be\naccessed from within the server with input$i_slider.↩︎\nA comprehensive treatment of reactive programming in\nShiny is beyond the scope of this module, but we recommend reading this\narticle, and this book\nsection to learn more about reactive graphs when you decide to start\nbuilding more complex Shiny Apps.↩︎\nAlthough we don’t cover it in this module, you can also\nsplit your App into 3 files: ui.R (for the\nui), server.R (for the server),\nand global.R (loads all App dependencies). When an App\nbecomes big and complex, splitting it up into smaller parts can make it\neasier to develop.↩︎\nThe Shiny Server\nAdministrator’s Guide](https://docs.rstudio.com/shiny-server/)) provides\ndetailed documentation geared towards System Administrators that will\nhelp in deploying a Shiny App.↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_updating_r.html",
      "title": "1. Updating R and R packages",
      "description": "How to check versions and update your installation\n",
      "author": [],
      "contents": "\n\nContents\nChecking Versions and\nUpdating\nCheck your\n  \nVersion\nCheck/Update\n \n\nUpdating\nR Packages\nBest Practices for\nUpdating\n\n\nLearning objectives\nUnderstand R versions and how to check what you\nhave\nLearn how to update your R/RStudio installation\nLearn how to update R packages\n\nChecking Versions and\nUpdating\nAs packages and R continue to improve, new versions\nof R are released1. In\nR, major versions are released\ninfrequently (i.e., 3.0.0 was released in\nApril 2013 and 4.0.0 was released in April\n2020), but minor versions are released more regularly\n(4.0.4 was released in February 2021 and\n4.0.5 was released in March 2021).\nAt minimum it’s advisable to maintain and update to the most recent\nmajor version of R, as these often contain important\nsecurity/programming changes. Depending on your workflow and package\nneeds, it’s good practice to keep the most recent minor version as well,\nthough it’s not uncommon to maintain multiple minor versions of\nR if your analyses or workflows depend on specific versions\nof a package.\nCheck your\n  \nVersion\nAn easy and quick way to check what version of R you\nhave and the most recent available version is first to open\nR, type R.version, hit enter, and see what you\nget:\n\n\nR.version\n\n\n               _                           \nplatform       aarch64-apple-darwin20      \narch           aarch64                     \nos             darwin20                    \nsystem         aarch64, darwin20           \nstatus                                     \nmajor          4                           \nminor          1.2                         \nyear           2021                        \nmonth          11                          \nday            01                          \nsvn rev        81115                       \nlanguage       R                           \nversion.string R version 4.1.2 (2021-11-01)\nnickname       Bird Hippie                 \n\nThis command will return information about the R version\nyour system is using, as well as some information specific to your\noperating system (os).\nNext, visit the R\nCRAN website to see what the most recent version is. If you haven’t\nupdated recently, go ahead and grab the most recent R\nversion for your system and install\nit.\nWhen you update a major version of R, your personal\nlibrary of packages will no longer work, and you will need to reinstall\nall of your packages. There are a few tools you can use to do this like\n{installr}\nfor Windows machines, and {updateR}.\nThis work to stay up-to-date happens fairly infrequently.\nCheck/Update\n \nWe can check our version of RStudio by going to the toolbar at the\ntop of your RStudio window and clicking Help >\nCheck for Updates. If you have the most recent\nversion, there will be a message box letting you know as much. If not,\nRStudio will direct you to their webpage to download the most recent\nversion.\nWhile it isn’t always necessary to update to the most recent version\nof R, there’s no reason not to always use\nthe most recent stable version of RStudio. It will generally include the\nbest features and up-to-date fixes.\nUpdating R Packages\nAs we use R more regularly for different tasks, it’s\ncommon to accumulate many R packages in our R\nlibrary. Every package is maintained and updated on a different schedule\nthan R and RStudio, and\nso as new functions and features are written, or bugs are fixed,\npackages will be updated intermittently. Some package maintainers do\nthis regularly, others may do it sporadically. Either way, we will\ntypically need to update packages semi-regularly.\nFiles associated with installed R packages are located\nat .libPaths(). View installed packages with\nlist.files(.libPaths()).\nThere are several methods of updating your R packages. If updating\nvia RStudio, go to the toolbar at the top and select\nTools >\nCheck for Package Updates.... Depending on how\nmany packages you’ve installed, and how recently you updated things, a\nwindow will appear saying either All packages up-to-date! or\nsomething that looks more like this:\nAs a rule of thumb, whenever updating your R version, it’s best to\nupdate/install your R packages too!\n\n\n\nFigure 1: Package update window\n\n\n\nWe can choose to Select All and update everything at once,\nor selectively update things.\nAfter we click Install Updates we may typically also\nsee a message like this:\n\n\n\nFigure 2: Restart R\n\n\n\nYou can choose to cancel, but it’s fine to click Yes\nand proceed. Sometimes we will also get something in the\nConsole that will ask the following:\n\nDo you want to install from sources the packages which need\ncompilation? (Yes/no/cancel)\n\nSome packages require this, so generally it’s okay to type\nYes and hit enter to proceed. At this point we can wait and\nlet R/RStudio update things for us. Depending on how many\npackages are involved, this can take a few minutes. Importantly, when\nit’s all said and done, make sure there weren’t errors or issues with\npackages that didn’t get installed. We can use\nTools >\nCheck for Package Updates... again and see what\npackages remain, or if we get a message saying all packages are\nup-to-date.\nBest Practices for Updating\nWe recommend the following approach to updating R\npackages (check out the great rstats.wtf chapter on this\ntopic and more):\nUpdate packages frequently (weekly)\nUpdate R quarterly (or at least\ncheck)\nFor complete reproducibility for a project check out the\n{renv}\npackage\nDon’t copy packages over between R versions–start clean\nand fresh\nUpdating packages can be irksome, but it’s typically for the best. We\nadvise updating your packages frequently if you regularly use\nR (e.g;, a weekly checks and updates to packages). If you\nwant to be sure you have the exact package version for future use and\nreproducibility, the {renv} may be a great solution for an\nanalysis or report, but may not be necessary if your aren’t using or\nworking on a specific project/analysis. {renv} keeps a\nrecord of your R packages within an RProject down to the exact version\nnumber when you originally loaded these packages, and allows you to\nupdate this record as your project evolves over time.\nRead more about {renv} in the project management module.\nOverall, treat R and R packages as\nsomething that will be highly functional with minimal but regular\nmaintenance, like a vehicle that needs new tires or an oil change once\nin a while. This will keep things running smoothly over the\nlong-term.\n\n\n\nFigure 3: Success over time! (Artwork by @allison_horst)\n\n\n\n\n\nPrevious\nmodule: Introduction\nNext\nmodule: 2. Git\n\nSee a list of previous versions here.↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    },
    {
      "path": "m_version_control.html",
      "title": "2. Version Control with `git`",
      "description": "A time machine you can share...\n",
      "author": [],
      "contents": "\n\nContents\nThe case for version control\nUsing version control\n\nLearning git\nthrough a project\nSetup\ngit locally\nCreate a\nRepository\nCollaborative Github\nSettings\nBranches\n& Conflicts\nCreate a\nbranch\nDownload Data: add,\ncommit, push\nPull\nRequests\nfetch\nand pull\n\nClosing the\nloop\nAdditional Git Details\nResources & Tutorials\n\n\nLearning objectives\nUnderstand and use basic git commands for version\ncontrol\nDevelop a mindset and approach to set up and manage a version\ncontrolled project\nIdentify best practices for using git\ncollaboratively\n\nThis lesson assumes you have:\nSet up a Github Account\nInstalled git on your computer locally\nThe case for version control\nAs water data scientists, we will work independently and with others\non documents, data, and code that change over time. Version\ncontrol is a framework and process to keep track of changes\nmade to a document, data, or code with the added bonus that you\ncan revert to any previous change or point in time and\nyou can work in parallel with multiple people on the same\nmaterial. In this module, we will practice version control with\ngit, which is perhaps the most common version control\nsystem available today.\nThe important difference between using a tool like git\nand Github versus cloud-based services (e.g., Dropbox, Google Docs)\nwhich are in a way, version control systems, is the ability to\ncontrol exactly what changes and/or files get\n“versioned”. These versions are snapshots of your work with unique\nidentifiers along with a short commit message, which allows\nus to restore these changes at any point in time. Thus, git\noffers much finer control over specific changes by specific users; this\nmakes version control a very powerful tool (as well as a computer\nlifesaver when things go awry!).\nWhat is the difference between git and Github?\ngit is a version control program that\nis installed on your computer with associated commands we use to\ninteract with version controlled files.\nGithub is a website that interfaces with\ngit and allows us to store/access/share our files as\nrepositories1.\nThere is a rich and complex set of tools and functionality with\ngit and Github, but in this module, we’ll focus on the\nbasics and link to more material.\nUsing version control\nSome folks may just use version control as another way to backup\ntheir work2, but version control really shines\nwhen working on a collaborative project. For example, many are familiar\nwith the situation below:\nFigure from http://www.phdcomics.com/comics/archive/phd101212s.gif\nThe comic above shows a single thesis document, but really the\nsituation isn’t any different than a big data analysis project. A common\nseries of steps that are generally taken by multiple people, may\ninclude:\nDownload/gather data\nClean/transform data\nAnalyze and visualize data\nProduce a front facing product or report\nMany of these tasks (or subtasks) may overlap or depends on one\nanother. Complex collaborative projects require forethought in how to\nset things up so everyone can seamlessly stitch their contribution into\nthe overall fabric of a project without holding up the process for other\nteam members.\nWith version control, if changes you make break things, it’s easy to\nrevert or get back to an earlier working version using the commit\ntimeline, because we have access to the history of our changes.\n\nPause and Discuss!\n\nIf you had 10 people to work on a big project with steps outlined\nabove, how would you approach it? How would you break out these pieces\nor subtasks? How would you track progress?\n\n\n\nThis module will cover version control with\ngit, including the basic commands, but\nit’s important to remember:\n\nUnderstanding and learning how to approach, setup, and\nimplement collaborative projects with version control are just as\nimportant as learning git commands\n\nThus, successful version control is not just about effectively using\ntools like git, but also developing skills to organize\nprojects, collaborate with others, and implement version control so it’s\nuseful (not just another way to back things up…see below).\nLearn to leverage the power of git\nfor things other than just a backup! (Figure from https://xkcd.com/1597/)(ref:xkcdGit) Learn to leverage the power of git for things other\nthan just a backup! (Figure from https://xkcd.com/1597/).\n\n\n\n\nLearning git\nthrough a project\nSometimes the best way to learn is to walk through a real-world\nexample. Let’s walk through a simplified example of a common set of\ntasks a team may face using R and learning git\nalong the way.\nLet’s imagine we have a team,\nJo and\nMo. Each has a unique\nskill set, and each knows how to use R and\ngit. The team is tasked with downloading and visualizing\nflow data for a specific river gage to provide status report updates on\na monthly basis, which are used for various regulatory actions (i.e.,\nhow much water is available for diversion, how flows relate to period of\nrecord averages, etc). In this case, the cloud is Github where\nour main\nrepository lives. More on this in a moment. First we need to make sure\neverything is setup and installed before we begin our git\nproject!\n\n\n\nFigure 1: An example team and workflow.\n\n\n\nSetup git locally\nWe highly recommend Jenny Bryan & Jim Hester’s happygitwithr website because it\ncovers everything (and more) that you may encounter when using\ngit with R. In particular, please take a moment to\ncheck/update your git installation and configure Github.\nThe following links will help with this:\nInstall\nGit\nIntroduce\nyourself to Git\nThe {usethis}\npackage is an excellent option to help you get things setup within\nR, like this:\n\n\nlibrary(usethis)\n\n# use your actual git user name and email!\nuse_git_config(user.name = \"yoda\", user.email = \"yoda2021@example.org\")\n\n\n\nRemember, we only need to do this once!3\nCreate a Repository\nThere are several ways to create repositories on Github and with\nRStudio4. We recommend the Github\nfirst option as it’s easiest to intialize and manage.\nTo begin, go to Github.com and login. From there, let’s create a new\nrepository!\n\nChallenge\n\nPlease take a moment and create a new repository on Github following\nthe happygitwithr.com\ninstructions.\nName the repository water-is-life\nCheck the Add a README file box at the bottom\nMake this a public repository\n\nClick for Answers!\n\nHopefully you see something like this before you click the green\nCreate Repository button.\n\n\n\nAnd then afterwards, a screen like this:\n\n\n\n\nThe last piece to this is getting this repository which is currently\nin the “cloud” to a local copy on your computer. This is called\ncloning. To clone the repository to your\ncomputer, we need to do the following:\nCopy the repository address. Make sure you’ve clicked\nHTTPS and the link in the box starts with\nhttps://. Copy that link to your clipboard.\n\n\n\nNow open RStudio and navigate to File > New Project >\nVersion Control > Git\nIn the Repository URL box, paste the link\nin, and hit Tab. The Project\ndirectory Name should automagically fill with the name of\nthe repository. Go ahead and put this where you want it to live\nlocally5 and hit Create\nProject. Open in new session is optional, but allows\nyou to keep existing projects open.\n\n\n\nGreat! We should now have an RStudio project that is version\ncontrolled with git, and there is a local copy we can work\nwith on our computer.\nCollaborative Github\nSettings\nSince we are working on a collaborative project, we need to make sure\nour collaborators have access to make changes to our repository. So, if\nJo created the repository,\nthey would need to add fellow collaborators to the repository. Let’s do\nthat now:\nGo to Github.com and login\nNavigate to your repository water-is-life\nClick on the Settings tab in the upper\nright corner\nYou can view and clone (copy) any public Github repository. However,\nyou can only commit changes if you are the owner, or have been added as\na collaborator (more on this later).\n\n\n\nFigure 2: Click on the Settings tab in the upper right\ncorner to add collaborators.\n\n\n\nClick on Manage Access on the side bar\non the left (you may need to enter your password or authentication\nagain)\n\n\n\nFigure 3: Then click on Manage Access.\n\n\n\nClick on Invite Collaborator and enter\nthe username or email you want to add!\n\n\n\nFigure 4: Invite collaborators by email or username!\n\n\n\nBranches & Conflicts\nOften when teaching git, we start with the basic\ncommands. However, it’s important to consider one of the more powerful\nparts of using git is the ability to use\nbranches. Branches, or branching, in git,\nis a way for multiple folks to work on the same repository\nindependently, and a standardized way to integrate those changes back\ninto the repository. Every repository typically starts with one single\nmain branch. This is like the trunk of the\ntree, or the main train track. We can create additional branches off of\nthis track, and if we want to merge them back in to make them available\nfor our collaborators, we use something called a pull\nrequest. This is essentially a way to double check if there\nwill be any conflicts between the work in the branch and the work in the\nmain branch6,\nand it also provides a way to document and review any changes. If a pull\nrequest does not create merge conflicts (don’t worry about this now, we\nwill cover it later) and if a collaborator on the repository\napproves the pull request, then the branch is “pulled” (or\nmerged) into the main branch, and then\ntypically deleted (since the branch has served its purpose, and the\nchanges in it are now reflected in the main branch).\nEven if you are the only person working in a repo, it’s a good habit\nto use branches because if you realize the branch/work you are doing is\na dead end, you can always delete a branch and return to the\nmain branch to pick things up where you\nlast left off.\n\n\n\nFigure 5: Example of multiple branches in a project\nsplitting off the main branch\n\n\n\nCreate a branch\nIn our example we want to make sure we keep our\nmain branch\nclean and up-to-date with changes we are happy with…all other work will\ngo through a branch, pull request,\nreview, and merge process\nto minimize conflicts or duplication of work.\nJo is a wizard at\ngrabbing the data we want to work with and saving it in an organized\nway. She’s cloned the repository to her computer, and the first thing\nshe wants to do is create a fresh branch to work off of, so her changes\ncan be reviewed before being merged into the\nmain\nbranch.\nWe can create a branch in RStudio in one of two ways:\nTools > Terminal > New Terminal and look for the\nTerminal tab in RStudio. Click on it, and at the prompt, type: \ngit checkout -b BRANCH_NAME.\nUsing the Git tab in RStudio and\nclicking on the New Branch button, type in\nthe branch name, make sure the Remote is set to origin and\nclick Create!\nLet’s create a new branch called jo_download_data.\nIdeally, each team member will do the same, and each of these branches\ncomes from the\nmain branch,\nwhich is the main trunk or “clean” version of the project.\n\n\n\n(ref:startingBranch) Jo\ncreates a new branch from\nmain to write\na data download script in R.\n\n\n\nFigure 6: (ref:startingBranch)\n\n\n\nDownload Data: add,\ncommit, push\nJo’s task is to download the data. Because this is a task that will\nneed to happen regularly, she writes a function to download the specific\nriver discharge data to an organized set of folders. She runs the\nfunction to get the most recent data, and saves it locally on her\nversion of the repository!\nAs she works, the general process she follows is:\nadd: Do some work, add some files,\nscripts, etc. To version control these changes, we need to stage or\nadd that work so it can be versioned by\ngit. We do this with git add <my_file>\nor check the Staged buttons in the RStudio\ngit panel.\ncommit: Add a commit\nmessage that is succinct but descriptive (i.e.,\nadded water data files). These are the messages you’ll be\nable to go back to if you want to travel back in time and see a\ndifferent version of something…so be kind to your future self and add\nsomething helpful.\npush: Finally, when we’re ready to\nsend locally commited changes up the cloud, we push this up\nto the Github remote cloud repository!\nRemember, you can commit as frequently as you like, and push at the\nend of the day, or push every time you commit. The timestamp identifier\nis added with a commit message, not with the push.\n\nYou Try!\n\nTake the following function and:\nMake a new folder in your project called code, save as\na script called 01_download_data.R\nstage it (either through the Git tab, or via\ngit add),\ncommit (add a message!),\npush to the jo_download_data branch\n\n\nlibrary(fs)\nlibrary(dataRetrieval)\nlibrary(readr)\nlibrary(glue)\n\n# make a function to download data, defaults to current date for end\n\n# function: dates as \"YYYY-MM-DD\"\nget_daily_flow <- function(gage_no){\n\n  # create folder to save data\n  fs::dir_create(\"data_raw\")\n\n  # set parameters to download data\n  siteNo <- gage_no # The USGS gage number\n  pCode <- \"00060\" # 00060 is discharge parameter code\n\n  # get NWIS daily data: CURRENT YEAR\n  dat <- readNWISdv(siteNumbers = siteNo,\n                    parameterCd = pCode)\n  # add water year\n  dat <- addWaterYear(dat)\n  # rename the columns\n  dat <- renameNWISColumns(dat)\n\n  # save out\n  write_csv(dat,\n            file =\n              glue(\"data_raw/nfa_updated_{Sys.Date()}.csv\"))\n}\n\n# RUN with:\n#siteNo <- \"11427000\"# NF American River\n# get_daily_flow(siteNo)\n\n\n\n\nAnswers!\n\nOnce we’ve made our script and saved it in the code\nfolder, we should see something like this in our\nGit tab… notice the change when the box is\nchecked.\n\n\n\nNext we want to commit and add a\nmessage. Note when we first click the Commit button, we’ll\nsee this screen:\n\n\n\nWhen we enter a commit message and click\nCommit, we’ll end up with this, which tells us our commit\nworked…but it’s still only local! Note we’ll have a message\nsaying our branch is\n\nahead of origin/jo_download_data by 1 commit.\n\nThat’s ok, it just means we still need to\npush our changes up to the remote\nbranch.\n\n\n\nThe final step is to push our changes\nup to the cloud. Click Push and you should\nget a message back like this. This means things worked…a final check\nwould be go to Github and make sure the file is online!\n\n\n\n\nThink of every commit you make as a train station. The more commits\nyou make (and the more descriptive they are) the easier it will be to\nget around…particularly because your train can travel back in\ntime!\nPull Requests\nThe next step in this version control odyssey is to get\nJo’s work from the\njo_download_data branch into the\nmain branch.\nHere we use a Pull Request. This is a way to\nreview the changes, check for any conflicts (if for instance, folks were\nworking on the same file), and then Merge these changes\ninto the main\nbranch. Then we can delete the jo_download_data branch,\ncreate another one to work on the next task, and so on.\nHere’s what we might see on Github if we visited our\nwater-is-life repository after we pushed our changes up.\nFirst, we will hopefully see an option to make a pull request because\nGithub recognizes there are changes from another branch that aren’t in\nthe main branch. We want to click the Compare and\npull request button.\n\n\n\nNext we have an option to add some additional descriptions, comments,\nabout what this pull request (PR) is doing, and why. We can tag a\nreviewer (collaborator on the Github repo), and add labels, milestones,\netc. These are all helpful for keeping track of what’s done and what’s\nnot.\n\n\n\nAfter we click the green Create pull request button,\nwe should see something that looks like the following.\nImportant! We ideally will see a green checkmark with a\nmessage “This branch has no conflicts with the base\nbranch, which means, our main\nbranch can easily merge this new work in!\nmerge conflicts are not something to be worried about,\nthey happen, and there are a number of ways to resolve them. A good\nresource and walk through using Github\nis here, or with command\nline here.\n\n\n\nExiting vim: if you end up in a text\neditor and aren’t sure how to exit, you may be in vim. It\nis the default editor for many systems and programs. To exit without\nsaving changes, hit Esc, then type\n:q! and hit\nEnter. To save changes, hit\nEsc and then type\n:wq and hit\nEnter.\n\nGo ahead and click Merge pull request, and wait\nuntil you see a screen shortly after that says the Pull request\nwas successfully merged and closed!\n\n\n\n\nfetch and pull\nNow that Jo has\ncompleted the first part of the team task, we want to move to the second\ntask, which is cleaning the data. Thankfully\nMo is great at data\nvisualization, and has a script that will clean up the code and\nvisualize it for us. But first,\nMo needs to\npull the changes that\nJo just merged into the\nmain\nrepository. There are two approaches to this. One is to use a\ngit fetch, the other is a git pull. The main\ndifference between the two:\nfetch: The safe version, because it\ndownloads any remote content from the repository, but does not\nupdate your local repository state. It just keeps a copy of the\nremote content, leaving any current work intact. To fully integrate the\nnew content, we need to follow a fetch by a\nmerge.\npull: This downloads the remote\ncontent and then immediately merges the content with your\nlocal state, but if you have pending work, this will create something\ncalled a merge conflict, but not to worry, these can be\nfixed!7.\nMo needs to\npull changes in to his branch from\nmain and\nproceed with the task. Here we assume\nJo has already completed\nwork on her branch to download, clean, and transform the data.\n\n\n\n\nYou Try!\n\nTake the following code from\nMo and:\npull changes into the repository so everything is\nup-to-date!\ncreate a unique branch: mo_cleanviz\nsave this\ncode to a new script at\ncode/02_clean_visualize.R\nstage it (either through the Git tab, or via\ngit add),\ncommit (add a message!),\npush to\nMo’s branch\nFinish by adding a Pull Request and merging back to\nmain!\n\nAnswers!\n\nNow we have a commit history that might look something\nlike this.\n\n\n\nWhat is one explanation for commit 4? We also have a\nproject that is very easy to re-run and update, and it can be updated or\nintegrated into a parameterized Rmarkdown\nreport which could be shared and updated easily.\n\n\n\n\n\nClosing the loop\nWe’ve reviewed how to create branches, add and revise files, and then\ncommit and push these changes to a shared repository for collaborative\nprojects. Importantly, it’s best practice to always\npull/fetch at the beginning of each work\nsession so you have the most-up-to-date changes. And if something needs\nto be fixed, you can always pull a specific branch and make a change and\npull request for that branch before pull requesting and merging back to\nthe main.\nFor example, if Jo\nneeded to make a quick fix to some data analysis, she could pull\nMo’s branch, make a fix\n(commit 4), and merge that back into the same branch\nbefore he finalizes the analysis and merges back to the\nmain\nbranch.\n\n\n\nThis is a simplified workflow, but it is very powerful. There are\nadditional options within this workflow, like reverting to previous\nversions, pruning branches or git history, and more, but\nthe core steps to successfully using version control rely on the\nmaterial covered above, and practice!\n\nAdditional Git Details\nAlthough there are many more details important to learning version\ncontrol, many come with more practice and expertise. However, a few\nimportant tidbits:\nWhen working on Github, there are options for private\nand public repositories. private repos are\nonly visible to you and any collaborators you’ve added to your repo.\nCertain accounts permit unlimited private repos.\n.gitignore: We can access or create this file with the\nusethis::edit_git_ignore() function. Any file extension or\nspecific file we include in our .gitignore file will mean\ngit ignores it. This is helpful for hidden files or\ntemporary files that may change a lot (i.e., *.html) but\naren’t necessary to version contol. You can even ignore entire\ndirectories or large files ( >= 100MB). Github is designed to version\ncontrol scripts, not large amounts of data.\nAdditionally you can use the\nusethis::git_vaccinate() function to\nensure there are no issues or secure files that may get accidentally\nadded to your version control repository.\nLarge files…Github is great, but less ideal for large files. Github\nwill complain with any file over about ~50MB, and anything greater than\n100MB requires git-lfs (large file\nsizes), but it is still unwieldy. See this\narticle for suggestions and options.\n\nResources & Tutorials\nhttps://swcarpentry.github.io/git-novice/\nhttps://learngitbranching.js.org\nR\nfor excel with github\n“Excuse me do you have a\nmoment to talk version control?” by Jenny Bryan\nGithub for\nproject management\n\n\nPrevious\nmodule: 1. Updating R\nNext\nmodule: 3. Project Management\n\nGithub is not the only site that integrates with\ngit to provide cloud-based version control. Other notable\nexamples include Gitlab and Bitbucket. In this module, we will\nfocus on Github, although the underlying git commands you\nwill learn are extensible to other git-based\nrepositories.↩︎\nGit xkcd↩︎\nIf you ever need to edit your global/git profile, you\ncan use a situation report, usethis::git_sitrep() to see\nwhat settings exist and how to diagnose problems.↩︎\nsee\nthis excellent overview of the various ways to create/link an RStudio\nproject with a Github repository↩︎\nIt’s a good idea to store all your github repositories\nin one place (e.g., ~/Documents/Github). We recommend to\navoid nesting git/RStudio projects inside other\ngit/RStudio projects–this can create confusion and make it\ndifficult to properly version control different projects. Each project\nshould have its own repository (e.g.,\n~/Documents/Github/project_01,\n~/Documents/Github/project_02).↩︎\nIn more complex workflows, we can pull branches into\nother branches before pulling them into the main branch. In\nthe module, we will use more simplified examples, but just remember that\nthere are many other ways to branch and merge.↩︎\nhttps://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/resolving-a-merge-conflict-on-github↩︎\n",
      "last_modified": "2025-05-08T17:20:18-07:00"
    }
  ],
  "collections": []
}
